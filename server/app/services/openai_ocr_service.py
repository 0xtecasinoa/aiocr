"""
OpenAI OCR Service for high-accuracy text extraction using GPT-4 Vision API.
Provides near 100% accuracy for Japanese and English text extraction.
"""

import asyncio
import logging
import base64
import json
from typing import Dict, Any, Optional, List
from pathlib import Path
import re
from PIL import Image
import io
import pandas as pd
from openai import AsyncOpenAI
from app.core.config import Settings

logger = logging.getLogger(__name__)
settings = Settings()


class OpenAIOCRService:
    """High-accuracy OCR service using OpenAI GPT-4 Vision API."""
    
    def __init__(self):
        self.client = None
        self.model = settings.OPENAI_MODEL
        self.supported_formats = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.pdf', '.xls', '.xlsx']
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize OpenAI client with proper error handling."""
        if not settings.OPENAI_API_KEY or settings.OPENAI_API_KEY == "your-openai-api-key-here":
            print("\n" + "="*80)
            print("‚ö†Ô∏è  OPENAI API KEY NOT CONFIGURED")
            print("="*80)
            print("To use OCR functionality, you need to:")
            print("1. Get an OpenAI API key from: https://platform.openai.com/api-keys")
            print("2. Create a .env file in the server directory")
            print("3. Add: OPENAI_API_KEY=your-actual-api-key-here")
            print("4. Restart the server")
            print("="*80 + "\n")
            logger.warning("OPENAI_API_KEY not set. OpenAI OCR service will not be available.")
            return
        
        try:
            self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            print(f"‚úÖ OpenAI client initialized successfully with model: {settings.OPENAI_MODEL}")
            logger.info("OpenAI client initialized successfully")
        except Exception as e:
            print(f"‚ùå Failed to initialize OpenAI client: {str(e)}")
            logger.error(f"Failed to initialize OpenAI client: {str(e)}")
            self.client = None
        
    def _encode_image_to_base64(self, image_path: str) -> str:
        """Encode image to base64 for OpenAI API."""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    
    def _optimize_image_for_ocr(self, image_path: str) -> str:
        """Optimize image for better OCR results, especially for barcode images."""
        try:
            from PIL import ImageEnhance, ImageFilter
            
            # Open and process image
            with Image.open(image_path) as img:
                # Convert to RGB if needed
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Resize if too large (OpenAI has size limits)
                max_size = 2048
                if max(img.size) > max_size:
                    img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                
                # Enhance image for better OCR, especially for barcodes
                # Increase contrast to make text and barcodes more readable
                enhancer = ImageEnhance.Contrast(img)
                img = enhancer.enhance(1.3)  # Increase contrast by 30%
                
                # Increase sharpness for better text recognition
                enhancer = ImageEnhance.Sharpness(img)
                img = enhancer.enhance(1.2)  # Increase sharpness by 20%
                
                # Apply slight unsharp mask for barcode clarity
                img = img.filter(ImageFilter.UnsharpMask(radius=1, percent=120, threshold=2))
                
                # Save optimized image
                optimized_path = str(Path(image_path).with_suffix('.optimized.jpg'))
                img.save(optimized_path, 'JPEG', quality=98, optimize=True)  # Higher quality for barcodes
                
                print(f"üñºÔ∏è Image optimized for barcode OCR: {optimized_path}")
                return optimized_path
                
        except Exception as e:
            logger.warning(f"Image optimization failed: {str(e)}, using original")
            return image_path
    
    async def extract_text_from_image(
        self,
        image_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from image using OpenAI GPT-4 Vision API.
        
        Args:
            image_path: Path to the image file
            language: Language hint (jpn+eng for Japanese and English)
            confidence_threshold: Not used for OpenAI (always high confidence)
        
        Returns:
            Dict containing extracted text and metadata
        """
        if not self.client:
            raise Exception("OpenAI client is not initialized. Please check OPENAI_API_KEY configuration.")
        
        try:
            start_time = asyncio.get_event_loop().time()
            
            # Optimize image for better results
            optimized_path = self._optimize_image_for_ocr(image_path)
            
            # Encode image to base64
            base64_image = self._encode_image_to_base64(optimized_path)
            
            # Determine language context
            language_context = ""
            if "jpn" in language.lower():
                language_context = "This image contains Japanese text (hiragana, katakana, kanji). "
            if "eng" in language.lower():
                language_context += "This image contains English text. "
            
            # Create comprehensive OCR prompt for maximum accuracy and multiple product detection
            ocr_prompt = f"""
            {language_context}
            
            Please perform high-accuracy OCR on this product catalog image. This image likely contains MULTIPLE DIFFERENT PRODUCTS.
            
            CRITICAL MULTI-PRODUCT DETECTION:
            This appears to be a product catalog page with multiple distinct products. Each product typically has:
            - Different character designs/images („Éî„Ç´„ÉÅ„É•„Ç¶, „Ç§„Éº„Éñ„Ç§, etc.)
            - Separate product codes (ST-03CB, ST-04CB, ST-05CB, etc.)
            - Individual JAN codes (13-digit barcodes)
            - Distinct prices and specifications
            
            BARCODE AND JAN CODE EXTRACTION PRIORITY:
            1. **BARCODES**: Look carefully for BARCODE IMAGES - black and white striped patterns
            2. **JAN NUMBERS UNDER BARCODES**: Extract the numbers displayed below barcode stripes
            3. **JAN FORMAT**: 13-digit numbers like 4970381806026, often starting with 4970381
            4. **BARCODE LABELS**: Look for text like "ÂçòÂìÅJAN„Ç≥„Éº„Éâ" or "JAN„Ç≥„Éº„Éâ" near barcodes
            5. **READ BARCODE NUMBERS**: Even if text is small, focus on extracting the complete 13-digit number
            
            INSTRUCTIONS:
            1. Extract ALL visible text with 100% accuracy
            2. **PRIORITIZE BARCODE READING**: Look for striped barcode patterns and read the numbers underneath
            3. IDENTIFY each separate product section/area in the image
            4. For each product, extract ALL related information
            5. AVOID repeating shared information (like company name, general descriptions)
            6. Focus on product-specific information: names, codes, prices, sizes, dates
            7. Preserve exact spacing and formatting for product data
            8. For Japanese text, preserve kanji, hiragana, and katakana exactly
            9. Extract numbers, prices, codes with exact formatting
            10. If you see the same product name with different codes, treat as separate products
            
            SPECIAL FOCUS - Look for these product details FOR EACH PRODUCT:
            - **JAN„Ç≥„Éº„Éâ (JAN codes)** - 13-digit numbers starting with 4 (MOST IMPORTANT - often shown as barcodes)
            - ÂïÜÂìÅÂêç (Product names) - usually contains character names like „Éî„Ç´„ÉÅ„É•„Ç¶, „Ç§„Éº„Éñ„Ç§, etc.
            - ÂïÜÂìÅ„Ç≥„Éº„Éâ (Product codes) - ST-03CB, ST-04CB, ST-05CB, EN-XXXX patterns
            - Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º (Prices) - amounts with ÂÜÜ or ¬•
            - Áô∫Â£≤‰∫àÂÆöÊó• (Release dates) - dates like 2024Âπ¥12Êúà
            - „Çµ„Ç§„Ç∫ÊÉÖÂ†± (Size info) - dimensions with mm, cm
            - ÂÖ•Êï∞ (Quantities) - numerical amounts
            - „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç (Character names) - specific Pokemon or anime character names
            
            RESPONSE FORMAT - Return valid JSON only:
            {{
                "raw_text": "Clean extracted text without repetition, preserving structure",
                "confidence_score": 95.0,
                "language_detected": "japanese"
            }}
            
            CRITICAL: 
            - Return ONLY valid JSON
            - Do NOT repeat the same text multiple times
            - Extract each piece of information only once
            - Focus on unique product data, not repeated headers/footers
            - **PRIORITIZE BARCODE NUMBERS** - even if they appear small or under striped patterns
            """
            
            print(f"ü§ñ OPENAI OCR: Processing image with {self.model}")
            
            # Call OpenAI Vision API
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": ocr_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"  # High detail for better OCR accuracy
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4000,
                temperature=0.1  # Low temperature for consistent, accurate results
            )
            
            # Parse response
            response_text = response.choices[0].message.content
            
            print(f"üîç DEBUG: OpenAI Raw Response:")
            print(f"Response length: {len(response_text)}")
            print(f"First 500 chars: {response_text[:500]}")
            print(f"Last 500 chars: {response_text[-500:]}")
            
            try:
                # Try to parse as JSON first
                if response_text.strip().startswith('{'):
                    print("üîç DEBUG: Parsing as direct JSON")
                    result = json.loads(response_text)
                else:
                    # If not JSON, extract JSON from markdown code blocks
                    json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                    if json_match:
                        print("üîç DEBUG: Found JSON in markdown code block")
                        result = json.loads(json_match.group(1))
                    else:
                        # Try to find JSON anywhere in the response
                        json_match = re.search(r'(\{[^{}]*"raw_text"[^{}]*\})', response_text, re.DOTALL)
                        if json_match:
                            print("üîç DEBUG: Found JSON pattern in response")
                            result = json.loads(json_match.group(1))
                        else:
                            print("‚ö†Ô∏è  DEBUG: No JSON found, using fallback")
                        # Fallback: treat entire response as raw text
                        result = {
                            "raw_text": response_text,
                            "confidence_score": 90.0,
                            "language_detected": "unknown",
                            "product_info": {},
                            "word_confidences": {},
                            "processing_metadata": {"method": "openai_gpt4_vision", "model": self.model}
                        }
                            
                print(f"üîç DEBUG: Parsed result keys: {list(result.keys())}")
                if 'product_info' in result:
                    print(f"üîç DEBUG: Product info: {result['product_info']}")
                        
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è  DEBUG: JSON parsing failed: {e}")
                logger.warning(f"Failed to parse OpenAI response as JSON: {e}")
                # Fallback to treating response as raw text
                result = {
                    "raw_text": response_text,
                    "confidence_score": 90.0,
                    "language_detected": "unknown", 
                    "product_info": {},
                    "word_confidences": {},
                    "processing_metadata": {"method": "openai_gpt4_vision", "model": self.model}
                }
            
            # Clean up optimized image if it was created
            if optimized_path != image_path:
                try:
                    Path(optimized_path).unlink()
                except:
                    pass
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            result["processing_time_ms"] = processing_time_ms
            
            # Ensure required fields exist
            result.setdefault("raw_text", "")
            result.setdefault("confidence_score", 95.0)  # OpenAI typically has high confidence
            result.setdefault("language_detected", "unknown")
            result.setdefault("word_confidences", {})
            result.setdefault("bounding_boxes", [])
            result.setdefault("text_blocks", [])
            
            # Parse structured data from raw text - support multiple products
            raw_text = result.get("raw_text", "")
            
            # Detect if this is a multi-product document
            multiple_products = self._detect_multiple_products(raw_text)
            
            if multiple_products:
                print(f"üîç DETECTED MULTIPLE PRODUCTS: {len(multiple_products)} products found")
                # Return the first product as the main structured data, but include all products in _products_list
                structured_data = {
                    "product_name": multiple_products[0].get('product_name'),
                    "sku": multiple_products[0].get('sku'),
                    "jan_code": multiple_products[0].get('jan_code'),
                    "price": multiple_products[0].get('price'),
                    "release_date": multiple_products[0].get('release_date'),
                    "category": multiple_products[0].get('category'),
                    "brand": multiple_products[0].get('brand'),
                    "manufacturer": multiple_products[0].get('manufacturer'),
                    "product_index": multiple_products[0].get('product_index', 1),
                    "section_text": multiple_products[0].get('section_text', ''),
                    "total_products_detected": len(multiple_products),
                    "has_multiple_products": True
                }
                # Store complete product list for processor
                structured_data["_products_list"] = [
                    {
                        "product_name": p.get('product_name'),
                        "sku": p.get('sku'),
                        "jan_code": p.get('jan_code'),
                        "price": p.get('price'),
                        "release_date": p.get('release_date'),
                        "category": p.get('category'),
                        "brand": p.get('brand'),
                        "manufacturer": p.get('manufacturer'),
                        "product_index": p.get('product_index', i+1),
                        "section_text": p.get('section_text', '')
                    }
                    for i, p in enumerate(multiple_products)
                ]
                
                # Log all detected products
                print("üè∑Ô∏è ALL DETECTED PRODUCTS:")
                print("-" * 40)
                for i, product in enumerate(multiple_products, 1):
                    print(f"Product {i}:")
                    print(f"  Name: {product.get('product_name', 'Not detected')}")
                    print(f"  SKU: {product.get('sku', 'Not detected')}")
                    print(f"  JAN Code: {product.get('jan_code', 'Not detected')}")
                    print(f"  Price: {product.get('price', 'Not detected')}")
                    print(f"  Category: {product.get('category', 'Not detected')}")
                    print(f"  Brand: {product.get('brand', 'Not detected')}")
                    if i < len(multiple_products):
                        print("  " + "-" * 38)
            else:
                # Single product processing
                structured_data = self._parse_product_data_from_text(raw_text)
                structured_data["has_multiple_products"] = False
            
            result["structured_data"] = structured_data
            
            print(f"‚úÖ OPENAI OCR SUCCESS: Extracted {len(result['raw_text'])} characters in {processing_time_ms}ms")
            print(f"üîç PARSED STRUCTURED DATA: {structured_data}")
            
            return result
            
        except Exception as e:
            error_msg = str(e).lower()
            if "connection" in error_msg or "network" in error_msg or "timeout" in error_msg:
                logger.error(f"OpenAI API connection failed: {str(e)}")
                raise ValueError(f"OpenAI API connection failed. Please check your internet connection and API key. Error: {str(e)}")
            elif "api_key" in error_msg or "authentication" in error_msg or "unauthorized" in error_msg:
                logger.error(f"OpenAI API authentication failed: {str(e)}")
                raise ValueError(f"OpenAI API authentication failed. Please check your OPENAI_API_KEY in the .env file. Error: {str(e)}")
            elif "quota" in error_msg or "billing" in error_msg:
                logger.error(f"OpenAI API quota exceeded: {str(e)}")
                raise ValueError(f"OpenAI API quota exceeded. Please check your OpenAI account billing. Error: {str(e)}")
            else:
                logger.error(f"OpenAI OCR processing failed: {str(e)}")
                raise ValueError(f"OpenAI OCR processing failed: {str(e)}")
    
    async def extract_text_from_excel(
        self,
        excel_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from Excel file (.xls/.xlsx) using pandas and OpenAI for structured analysis.
        """
        try:
            start_time = asyncio.get_event_loop().time()
            
            # Read Excel file
            try:
                # Try to read with openpyxl engine for .xlsx files
                if excel_path.endswith('.xlsx'):
                    df = pd.read_excel(excel_path, engine='openpyxl', sheet_name=None)
                else:
                    # Use xlrd for .xls files
                    df = pd.read_excel(excel_path, engine='xlrd', sheet_name=None)
            except Exception as e:
                logger.error(f"Failed to read Excel file: {str(e)}")
                raise ValueError(f"Failed to read Excel file: {str(e)}")
            
            # Process all sheets - Extract only essential product data
            all_text = []
            product_rows = []
            
            for sheet_name, sheet_df in df.items():
                print(f"üîç PROCESSING SHEET: {sheet_name} ({len(sheet_df)} rows)")
                
                # Extract only rows containing product codes (EN-XXXX)
                for idx, row in sheet_df.iterrows():
                    row_str = " | ".join([str(cell) if pd.notna(cell) else "" for cell in row.values])
                    
                    # Only include rows with EN-codes or essential headers
                    if (re.search(r'EN-\d+', row_str) or 
                        any(keyword in row_str for keyword in ['ÂïÜÂìÅÂêç', 'JAN„Ç≥„Éº„Éâ', 'Áô∫Â£≤‰∫àÂÆöÊó•', 'Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º'])):
                        all_text.append(row_str)
                        
                        # If this is a product row, store it separately
                        if re.search(r'EN-\d+', row_str):
                            product_rows.append(row_str)
                            print(f"‚úÖ PRODUCT ROW FOUND: {row_str[:100]}")
            
            # Combine only essential text
            raw_text = "\n".join(all_text)
            print(f"üîç EXTRACTED TEXT LENGTH: {len(raw_text)} chars, {len(product_rows)} product rows")
            
            # Skip OpenAI analysis for Excel files to avoid complexity
            ai_structured = {
                "analysis": f"Excel file processed with {len(product_rows)} product rows extracted",
                "product_count": len(product_rows)
            }
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            
            # Parse structured data from combined raw text - support multiple products
            multiple_products = self._detect_multiple_products(raw_text)
            
            # Ë§áÊï∞ÂïÜÂìÅ„ÅåÊ§úÂá∫„Åï„Çå„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇ„ÄÅproduct_rows„ÅåË§áÊï∞„ÅÇ„Çå„Å∞Âº∑Âà∂ÁöÑ„Å´‰ΩúÊàê
            if not multiple_products and len(product_rows) > 1:
                print(f"üîß EXCEL: FORCING MULTI-PRODUCT from {len(product_rows)} product rows")
                multiple_products = []
                for i, product_row in enumerate(product_rows):
                    product_data = self._parse_product_data_from_text(product_row)
                    if product_data:
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = product_row
                        # „Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫„ÅÆËøΩÂä†ÊÉÖÂ†±
                        product_data['category'] = '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫'
                        product_data['brand'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                        product_data['manufacturer'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                        multiple_products.append(product_data)
                        print(f"   ‚úÖ Excel Product {i+1}: {product_data.get('product_name', 'Unknown')}")
            
            if multiple_products:
                print(f"üîç EXCEL: DETECTED MULTIPLE PRODUCTS: {len(multiple_products)} products found")
                # Return the first product as the main structured data, but include all products in _products_list
                parsed_structured_data = {
                    "product_name": multiple_products[0].get('product_name'),
                    "sku": multiple_products[0].get('sku'),
                    "jan_code": multiple_products[0].get('jan_code'),
                    "price": multiple_products[0].get('price'),
                    "release_date": multiple_products[0].get('release_date'),
                    "category": multiple_products[0].get('category'),
                    "brand": multiple_products[0].get('brand'),
                    "manufacturer": multiple_products[0].get('manufacturer'),
                    "product_index": multiple_products[0].get('product_index', 1),
                    "section_text": multiple_products[0].get('section_text', ''),
                    "total_products_detected": len(multiple_products),
                    "has_multiple_products": True
                }
                # Store complete product list for processor
                parsed_structured_data["_products_list"] = [
                    {
                        "product_name": p.get('product_name'),
                        "sku": p.get('sku'),
                        "jan_code": p.get('jan_code'),
                        "price": p.get('price'),
                        "release_date": p.get('release_date'),
                        "category": p.get('category'),
                        "brand": p.get('brand'),
                        "manufacturer": p.get('manufacturer'),
                        "product_index": p.get('product_index', i+1),
                        "section_text": p.get('section_text', '')
                    }
                    for i, p in enumerate(multiple_products)
                ]
                
                # Log all detected products
                print("üè∑Ô∏è ALL DETECTED PRODUCTS:")
                print("-" * 40)
                for i, product in enumerate(multiple_products, 1):
                    print(f"Product {i}:")
                    print(f"  Name: {product.get('product_name', 'Not detected')}")
                    print(f"  SKU: {product.get('sku', 'Not detected')}")
                    print(f"  JAN Code: {product.get('jan_code', 'Not detected')}")
                    print(f"  Price: {product.get('price', 'Not detected')}")
                    print(f"  Category: {product.get('category', 'Not detected')}")
                    print(f"  Brand: {product.get('brand', 'Not detected')}")
                    if i < len(multiple_products):
                        print("  " + "-" * 38)
                    else:
                        print("  " + "-" * 38)
            else:
                # Single product processing
                parsed_structured_data = self._parse_product_data_from_text(raw_text)
                parsed_structured_data["has_multiple_products"] = False
            
            parsed_structured_data.update({
                "ai_analysis": ai_structured,
                "file_type": "excel",
                "total_sheets": len(df),
                "total_text_length": len(raw_text),
                "product_rows_found": len(product_rows)
            })
            
            result = {
                "raw_text": raw_text,
                "confidence_score": 95.0,  # Excel parsing is very reliable
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": parsed_structured_data,
                "processing_metadata": {
                    "method": "excel_pandas_openai", 
                    "model": self.model,
                    "sheets_processed": list(df.keys())
                },
                "processing_time_ms": processing_time_ms
            }
            
            print(f"‚úÖ EXCEL OCR SUCCESS: Processed {len(df)} sheets, {len(raw_text)} characters in {processing_time_ms}ms")
            print(f"üîç PARSED STRUCTURED DATA: {parsed_structured_data}")
            
            return result
            
        except Exception as e:
            logger.error(f"Excel OCR processing failed: {str(e)}")
            raise ValueError(f"Excel OCR processing failed: {str(e)}")
    
    async def extract_text_from_pdf(
        self,
        pdf_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from PDF file by converting to images and using OpenAI Vision.
        """
        try:
            import fitz  # PyMuPDF
            
            start_time = asyncio.get_event_loop().time()
            logger.info(f"ü§ñ PDF OCR: Processing PDF with {self.model}")
            
            # Open PDF
            pdf_document = fitz.open(pdf_path)
            all_text = []
            total_confidence = 0
            processed_pages = 0
            
            # Process each page
            for page_num in range(len(pdf_document)):
                page = pdf_document.load_page(page_num)
                
                # Convert page to image
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom for better quality
                img_data = pix.tobytes("png")
                
                # Convert to base64 for OpenAI
                import base64
                base64_image = base64.b64encode(img_data).decode('utf-8')
                
                # Process with OpenAI
                try:
                    response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": f"""Please perform high-accuracy OCR (Optical Character Recognition) on this PDF page image and extract structured product information.

                                        Language: {language}
                                        
INSTRUCTIONS:
1. Extract ALL visible text from the image with 100% accuracy
2. Preserve exact spacing, line breaks, and formatting
3. Include ALL characters including punctuation, symbols, and special characters
4. If text is partially obscured or unclear, make your best interpretation
5. Maintain the original reading order (top to bottom, left to right for mixed languages)
6. For Japanese text, preserve kanji, hiragana, and katakana exactly as shown
7. For numbers, prices, codes, preserve exact formatting (including ¬•, $, -, etc.)

RESPONSE FORMAT:
You must respond with valid JSON only. Do not include any other text or markdown formatting.

{{
    "raw_text": "All extracted text exactly as it appears in the image, preserving line breaks and formatting"
}}

CRITICAL RULES:
1. Return ONLY valid JSON - no markdown, no extra text
2. Focus on accurate text extraction - do not try to interpret or structure the data
3. Extract Japanese text exactly as shown
4. Preserve all formatting, line breaks, and spacing"""
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{base64_image}",
                                            "detail": "high"
                                        }
                                    }
                                ]
                            }
                        ],
                        max_tokens=4000,
                        temperature=0.1
                    )
                    
                    response_text = response.choices[0].message.content
                    
                    print(f"üîç DEBUG: PDF Page {page_num + 1} Response:")
                    print(f"Response length: {len(response_text)}")
                    print(f"First 300 chars: {response_text[:300]}")
                    
                    # Try to parse as JSON
                    try:
                        import json
                        if response_text.strip().startswith('{'):
                            page_result = json.loads(response_text)
                        else:
                            # Try to extract JSON from markdown
                            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                            if json_match:
                                page_result = json.loads(json_match.group(1))
                            else:
                                # Try to find JSON anywhere
                                json_match = re.search(r'(\{[^{}]*"raw_text"[^{}]*\})', response_text, re.DOTALL)
                                if json_match:
                                    page_result = json.loads(json_match.group(1))
                                else:
                                    raise json.JSONDecodeError("No JSON found", response_text, 0)
                        
                        page_text = page_result.get('raw_text', response_text)
                        print(f"üîç DEBUG: PDF Page {page_num + 1} extracted {len(page_text)} characters")
                        
                    except json.JSONDecodeError as e:
                        print(f"‚ö†Ô∏è  DEBUG: PDF Page {page_num + 1} JSON parsing failed: {e}")
                        page_text = response_text
                    
                    all_text.append(f"=== Page {page_num + 1} ===\n{page_text}")
                    
                    total_confidence += 85.0  # Assume good confidence for PDF OCR
                    processed_pages += 1
                    
                except Exception as page_error:
                    logger.warning(f"Failed to process PDF page {page_num + 1}: {page_error}")
                    # Try to extract text directly from PDF
                    try:
                        # Check if document is still valid
                        if pdf_document.is_closed:
                            logger.error(f"PDF document is closed, cannot process page {page_num + 1}")
                            break
                        page_text = page.get_text()
                        if page_text.strip():
                            all_text.append(f"=== Page {page_num + 1} (Direct) ===\n{page_text}")
                            processed_pages += 1
                            total_confidence += 70.0
                    except Exception as direct_error:
                        logger.warning(f"Direct text extraction failed for page {page_num + 1}: {direct_error}")
                        # If document is closed, stop processing
                        if "document closed" in str(direct_error).lower():
                            logger.error("PDF document closed unexpectedly, stopping processing")
                            break
            
            # Store total pages before closing document
            total_pages = len(pdf_document)
            
            # Close document safely
            try:
                pdf_document.close()
            except Exception as close_error:
                logger.warning(f"Error closing PDF document: {close_error}")
            
            # Calculate final confidence
            final_confidence = total_confidence / max(processed_pages, 1) if processed_pages > 0 else 0
            
            # Combine all text
            raw_text = "\n\n".join(all_text)
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            
            # Parse structured data from combined raw text
            structured_data = self._parse_product_data_from_text(raw_text)
            structured_data.update({
                "file_type": "pdf",
                "total_pages": total_pages,
                "processed_pages": processed_pages,
                "total_text_length": len(raw_text)
            })
            
            result = {
                "raw_text": raw_text,
                "confidence_score": final_confidence,
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": structured_data,
                "processing_metadata": {
                    "method": "pdf_pymupdf_openai", 
                    "model": self.model,
                    "pages_processed": processed_pages
                },
                "processing_time_ms": processing_time_ms
            }
            
            print(f"‚úÖ PDF OCR SUCCESS: Processed {processed_pages}/{total_pages} pages, {len(raw_text)} characters in {processing_time_ms}ms")
            print(f"üîç PARSED STRUCTURED DATA: {structured_data}")
            
            return result
            
        except ImportError:
            # Fallback: try to use direct text extraction if PyMuPDF is not available
            logger.warning("PyMuPDF not available, trying alternative PDF processing")
            return await self._extract_pdf_fallback(pdf_path, language, confidence_threshold)
        except Exception as e:
            logger.error(f"PDF OCR processing failed: {str(e)}")
            # Ensure PDF document is closed even on error
            try:
                if 'pdf_document' in locals():
                    pdf_document.close()
            except:
                pass
            raise ValueError(f"PDF OCR processing failed: {str(e)}")
    
    async def _extract_pdf_fallback(self, pdf_path: str, language: str, confidence_threshold: float) -> Dict[str, Any]:
        """Fallback PDF processing without PyMuPDF"""
        try:
            # Simple fallback - return minimal structure
            return {
                "raw_text": f"PDF file: {Path(pdf_path).name} (processing unavailable)",
                "confidence_score": 0.0,
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": {
                    "file_type": "pdf",
                    "status": "processing_unavailable"
                },
                "processing_metadata": {
                    "method": "fallback", 
                    "error": "PDF processing libraries not available"
                },
                "processing_time_ms": 0
            }
        except Exception as e:
            raise ValueError(f"PDF fallback processing failed: {str(e)}")
    
    async def extract_text_from_file(
        self,
        file_path: str,
        language: str = "jpn+eng",
        preprocessing: bool = True,
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from file. Supports images and Excel files.
        """
        if not self.client:
            raise Exception("OpenAI client is not initialized. Please check OPENAI_API_KEY configuration.")
        
        file_path_obj = Path(file_path)
        file_extension = file_path_obj.suffix.lower()
        
        if file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
            return await self.extract_text_from_image(
                file_path, language, confidence_threshold
            )
        elif file_extension in ['.xls', '.xlsx']:
            return await self.extract_text_from_excel(
                file_path, language, confidence_threshold
            )
        elif file_extension == '.pdf':
            return await self.extract_text_from_pdf(
                file_path, language, confidence_threshold
            )
        else:
            raise ValueError(f"Unsupported file format: {file_extension}") 
    
    def _parse_product_data_from_text(self, raw_text: str) -> Dict[str, Any]:
        """
        Parse structured product data from raw OCR text.
        Extracts all required fields: ÂïÜÂìÅÂêç, JAN„Ç≥„Éº„Éâ, ‰æ°Ê†º, Âú®Â∫´Êï∞, „Ç´„ÉÜ„Ç¥„É™, „Éñ„É©„É≥„Éâ, Ë£ΩÈÄ†ÂÖÉ, ÂïÜÂìÅË™¨Êòé, ÈáçÈáè, Ëâ≤, Á¥†Êùê, ÂéüÁî£Âú∞, ‰øùË®º
        """
        if not raw_text:
            return {}
        
        structured_data = {}
        text_lines = raw_text.split('\n')
        text_lower = raw_text.lower()
        
        print(f"üîç TEXT PARSER: Processing {len(raw_text)} characters, {len(text_lines)} lines")
        
        # ÂâçÂá¶ÁêÜ: Áπ∞„ÇäËøî„Åó„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
        cleaned_lines = self._clean_repetitive_text(text_lines)
        
        # 1. ÂïÜÂìÅÂêç (Product Name) - Ë§áÊï∞„ÅÆ„Éë„Çø„Éº„É≥„ÅßÊ§úÁ¥¢
        product_name = self._extract_product_name(cleaned_lines, raw_text)
        if product_name:
            structured_data['product_name'] = product_name
            print(f"‚úÖ ÂïÜÂìÅÂêç: {product_name}")
        
        # 2. ÂïÜÂìÅ„Ç≥„Éº„Éâ (Product Code) - EN-XXXX, ST-XXXX „Å™„Å©
        product_code = self._extract_product_code(raw_text)
        if product_code:
            structured_data['sku'] = product_code
            print(f"‚úÖ ÂïÜÂìÅ„Ç≥„Éº„Éâ: {product_code}")
        
        # 3. JAN„Ç≥„Éº„Éâ (JAN Code) - 8Ê°Å„Åæ„Åü„ÅØ13Ê°Å„ÅÆÊï∞Â≠ó
        jan_code = self._extract_jan_code(raw_text)
        if jan_code:
            structured_data['jan_code'] = jan_code
            print(f"‚úÖ JAN„Ç≥„Éº„Éâ: {jan_code}")
        
        # 3. ‰æ°Ê†º (Price) - ¬•Ë®òÂè∑„ÇÑ‰æ°Ê†ºÈñ¢ÈÄ£„ÅÆÊñáÂ≠ó„Å®Êï∞Â≠ó
        price = self._extract_price(raw_text)
        if price:
            structured_data['price'] = price
            print(f"‚úÖ ‰æ°Ê†º: {price}")
        
        # 4. Âú®Â∫´Êï∞ (Stock) - Âú®Â∫´„ÄÅÊï∞ÈáèÈñ¢ÈÄ£„ÅÆÊï∞Â≠ó
        stock = self._extract_stock(raw_text, text_lines)
        if stock:
            structured_data['stock'] = stock
            print(f"‚úÖ Âú®Â∫´Êï∞: {stock}")
        
        # 5. „Ç´„ÉÜ„Ç¥„É™ (Category) - ÂïÜÂìÅÁ®ÆÂà•„ÅÆÊé®ÂÆö
        category = self._extract_category(raw_text)
        if category:
            structured_data['category'] = category
            print(f"‚úÖ „Ç´„ÉÜ„Ç¥„É™: {category}")
        
        # 6. „Éñ„É©„É≥„Éâ (Brand) - „Éñ„É©„É≥„ÉâÂêç„ÄÅ„É°„Éº„Ç´„ÉºÂêç
        brand = self._extract_brand(raw_text, cleaned_lines)
        if brand:
            structured_data['brand'] = brand
            print(f"‚úÖ „Éñ„É©„É≥„Éâ: {brand}")
        
        # 7. Áô∫Â£≤‰∫àÂÆöÊó• (Release Date) - Áô∫Â£≤Êó•„ÄÅ„É™„É™„Éº„ÇπÊó•
        release_date = self._extract_release_date(raw_text)
        if release_date:
            structured_data['release_date'] = release_date
            print(f"‚úÖ Áô∫Â£≤‰∫àÂÆöÊó•: {release_date}")
        
        # 8. Ë£ΩÈÄ†ÂÖÉ (Manufacturer) - Ë£ΩÈÄ†ÂÖÉ„ÄÅÁô∫Â£≤ÂÖÉ
        manufacturer = self._extract_manufacturer(raw_text, cleaned_lines, brand)
        if manufacturer:
            structured_data['manufacturer'] = manufacturer
            print(f"‚úÖ Ë£ΩÈÄ†ÂÖÉ: {manufacturer}")
        
        # 8. ÂïÜÂìÅË™¨Êòé (Description) - ÂïÜÂìÅ„ÅÆÁâπÂæ¥„ÄÅË™¨Êòé
        description = self._extract_description(raw_text, text_lines)
        if description:
            structured_data['description'] = description
            print(f"‚úÖ ÂïÜÂìÅË™¨Êòé: {description}")
        
        # 9. ÈáçÈáè (Weight) - Èáç„Åï„ÄÅ„Çµ„Ç§„Ç∫ÊÉÖÂ†±
        weight = self._extract_weight(raw_text)
        if weight:
            structured_data['weight'] = weight
            print(f"‚úÖ ÈáçÈáè: {weight}")
        
        # 10. Ëâ≤ (Color) - Ëâ≤ÊÉÖÂ†±
        color = self._extract_color(raw_text, text_lines)
        if color:
            structured_data['color'] = color
            print(f"‚úÖ Ëâ≤: {color}")
        
        # 11. Á¥†Êùê (Material) - Á¥†ÊùêÊÉÖÂ†±
        material = self._extract_material(raw_text, text_lines)
        if material:
            structured_data['material'] = material
            print(f"‚úÖ Á¥†Êùê: {material}")
        
        # 12. ÂéüÁî£Âú∞ (Origin) - Ë£ΩÈÄ†ÂõΩ„ÄÅÂéüÁî£ÂõΩ
        origin = self._extract_origin(raw_text, text_lines)
        if origin:
            structured_data['origin'] = origin
            print(f"‚úÖ ÂéüÁî£Âú∞: {origin}")
        
        # 13. ‰øùË®º (Warranty) - ‰øùË®ºÊÉÖÂ†±
        warranty = self._extract_warranty(raw_text, text_lines)
        if warranty:
            structured_data['warranty'] = warranty
            print(f"‚úÖ ‰øùË®º: {warranty}")
        
        print(f"üéØ PARSER RESULT: Extracted {len(structured_data)} fields")
        return structured_data
    
    def _detect_multiple_products(self, raw_text: str) -> list:
        """Ë§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫„Åó„Å¶ÂÄãÂà•„Å´ÊäΩÂá∫"""
        if not raw_text:
            return []
        
        print(f"üîç MULTI-PRODUCT DETECTION: Analyzing {len(raw_text)} characters")
        print(f"üìù RAW TEXT PREVIEW (first 500 chars):")
        print(f"{raw_text[:500]}...")
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        products = []
        
        # 1. JAN„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥„ÅßÂïÜÂìÅ„ÇíÂàÜÈõ¢ÔºàÊúÄÂÑ™ÂÖàÔºâ
        jan_patterns = re.findall(r'\b(4\d{12})\b', raw_text)
        # „Éè„Ç§„Éï„É≥‰ªò„ÅçJAN„Ç≥„Éº„Éâ„ÇÇÊ§úÂá∫
        jan_patterns_with_hyphen = re.findall(r'4970381-(\d{6})', raw_text)
        if jan_patterns_with_hyphen:
            jan_patterns.extend([f"4970381{code}" for code in jan_patterns_with_hyphen])
        
        # ST-„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥„ÇÇÊ§úÂá∫„Åó„Å¶ÂïÜÂìÅ„ÇíÂàÜÈõ¢
        st_patterns = re.findall(r'ST-\d{2}[A-Z]{2}', raw_text)
        print(f"üîç JAN PATTERNS FOUND: {jan_patterns}")
        print(f"üîç ST-CODE PATTERNS FOUND: {st_patterns}")
        
        # ST-„Ç≥„Éº„Éâ„ÅåË§áÊï∞„ÅÇ„ÇãÂ†¥Âêà„ÇÇÂº∑Âà∂ÁöÑ„Å´„Éû„É´„ÉÅ„Éó„É≠„ÉÄ„ÇØ„Éà„Å®„Åó„Å¶Âá¶ÁêÜ
        if len(st_patterns) > 1:
            print(f"üîß FORCING MULTI-PRODUCT BY ST-CODES: {len(st_patterns)} ST-codes detected")
            
            # ST-„Ç≥„Éº„Éâ„Å®JAN„Ç≥„Éº„Éâ„ÅÆÊ≠£Á¢∫„Å™„Éû„ÉÉ„Éî„É≥„Ç∞„Çí‰ΩúÊàê
            st_jan_mapping = self._create_st_jan_mapping(raw_text, st_patterns, jan_patterns)
            print(f"üîó ST-JAN MAPPING: {st_jan_mapping}")
            
            # ÂêÑST-„Ç≥„Éº„Éâ„Å´ÂØæ„Åó„Å¶ÂÄãÂà•„ÅÆÂïÜÂìÅ„Çí‰ΩúÊàê
            for i, st_code in enumerate(st_patterns):
                # Ë©≤ÂΩìST-„Ç≥„Éº„Éâ„Å´Âü∫„Å•„ÅÑ„Å¶„Çà„ÇäÁ≤æÂØÜ„Å™„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
                st_section = self._extract_precise_section_by_st_code(raw_text, st_code, st_patterns)
                product_data = self._parse_product_data_from_text(st_section)
                if product_data:
                    product_data['product_index'] = i + 1
                    product_data['section_text'] = st_section[:300] + "..." if len(st_section) > 300 else st_section
                    product_data['sku'] = st_code  # Á¢∫ÂÆü„Å´SKU„ÇíË®≠ÂÆö
                    
                    # Ê≠£Á¢∫„Å™JAN„Ç≥„Éº„Éâ„ÇíË®≠ÂÆö
                    if st_code in st_jan_mapping:
                        product_data['jan_code'] = st_jan_mapping[st_code]
                        print(f"   üîó Mapped JAN for {st_code}: {st_jan_mapping[st_code]}")
                    
                    # „Éù„Ç±„É¢„É≥„Ç∞„ÉÉ„Ç∫„ÅÆËøΩÂä†ÊÉÖÂ†±
                    product_data['category'] = '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫'
                    product_data['brand'] = '„Ç®„É≥„Çπ„Ç´„Ç§'
                    product_data['manufacturer'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                    
                    # „Çà„ÇäÊ≠£Á¢∫„Å™ÂïÜÂìÅÂêç„ÇíË®≠ÂÆö
                    character_name = self._get_character_for_st_code(st_code)
                    if character_name and (not product_data.get('product_name') or len(product_data['product_name']) < 10):
                        product_data['product_name'] = f"{character_name} ÂïÜÂìÅ„Ç≥„Éº„Éâ: {st_code}"
                    
                    products.append(product_data)
                    print(f"   ‚úÖ ST-Code Product {i+1}: {product_data.get('product_name', 'Unknown')}")
                    print(f"      - SKU: {st_code}")
                    print(f"      - JAN: {product_data.get('jan_code', 'N/A')}")
            return products
        
        # JAN„Ç≥„Éº„Éâ„ÅåË§áÊï∞„ÅÇ„ÇãÂ†¥Âêà„ÅØÂº∑Âà∂ÁöÑ„Å´„Éû„É´„ÉÅ„Éó„É≠„ÉÄ„ÇØ„Éà„Å®„Åó„Å¶Âá¶ÁêÜ
        if len(jan_patterns) > 1:
            print(f"üîß FORCING MULTI-PRODUCT: {len(jan_patterns)} JAN codes detected, creating individual products")
            # ÂêÑJAN„Ç≥„Éº„Éâ„Å´ÂØæ„Åó„Å¶ÂÄãÂà•„ÅÆÂïÜÂìÅ„Çí‰ΩúÊàê
            for i, jan_code in enumerate(jan_patterns):
                # Ë©≤ÂΩìJAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄ„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
                jan_section = self._extract_section_by_jan(raw_text, jan_code)
                product_data = self._parse_product_data_from_text(jan_section)
                if product_data:
                    product_data['product_index'] = i + 1
                    product_data['section_text'] = jan_section[:300] + "..." if len(jan_section) > 300 else jan_section
                    product_data['jan_code'] = jan_code  # Á¢∫ÂÆü„Å´JAN„Ç≥„Éº„Éâ„ÇíË®≠ÂÆö
                    # „Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫„ÅÆËøΩÂä†ÊÉÖÂ†±
                    product_data['category'] = '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫'
                    product_data['brand'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                    product_data['manufacturer'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                    products.append(product_data)
                    print(f"   ‚úÖ Forced Product {i+1}: {product_data.get('product_name', 'Unknown')}")
                    print(f"      - JAN: {jan_code}")
                    print(f"      - SKU: {product_data.get('sku', 'N/A')}")
        
        # 2. ÂïÜÂìÅÂêç„Éë„Çø„Éº„É≥„ÅßËøΩÂä†Ê§úÂá∫ÔºàEN-„Ç≥„Éº„Éâ„ÄÅST-„Ç≥„Éº„Éâ„Éô„Éº„ÇπÔºâ
        elif self._has_multiple_st_codes(text_lines) or self._has_multiple_en_codes(text_lines):
            print("üéØ Detected multiple EN/ST-code products")
            # EN„Ç≥„Éº„Éâ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØEN„Ç≥„Éº„Éâ„ÅßÂàÜÂâ≤„ÄÅ„Åù„ÅÜ„Åß„Å™„Åë„Çå„Å∞ST„Ç≥„Éº„Éâ„ÅßÂàÜÂâ≤
            if self._has_multiple_en_codes(text_lines):
                product_sections = self._split_by_en_codes(text_lines)
            else:
                product_sections = self._split_by_st_codes(text_lines)
            
            for i, section in enumerate(product_sections):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data and product_data.get('product_name'):
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        products.append(product_data)
                        print(f"   ‚úÖ Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        
        # 3. Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÅÆÂ†¥Âêà„ÅØË°å„Éô„Éº„Çπ„ÅßÂàÜÈõ¢
        elif self._detect_table_structure(text_lines):
            print("üéØ Detected table structure with multiple products")
            product_sections = self._split_table_rows(text_lines)
            
            for i, section in enumerate(product_sections):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data and product_data.get('product_name'):
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        products.append(product_data)
                        print(f"   ‚úÖ Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        
        # 4. „Éù„Ç±„É¢„É≥„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÅßË§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫
        elif self._detect_multiple_pokemon_characters(raw_text):
            print("üéØ Detected multiple Pokemon characters in catalog")
            character_products = self._split_by_pokemon_characters(raw_text)
            
            for i, (character, section) in enumerate(character_products):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data:
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂïÜÂìÅÂêç„Å´Âê´„ÇÅ„Çã
                        if not product_data.get('product_name') or character not in product_data['product_name']:
                            product_data['product_name'] = f"„Éù„Ç±„É¢„É≥„Ç≥„Ç§„É≥„Éê„É≥„ÇØ {character}"
                        # „Éù„Ç±„É¢„É≥„Ç∞„ÉÉ„Ç∫„ÅÆËøΩÂä†ÊÉÖÂ†±
                        product_data['category'] = '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫'
                        product_data['brand'] = '„Ç®„É≥„Çπ„Ç´„Ç§'
                        product_data['manufacturer'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                        products.append(product_data)
                        print(f"   ‚úÖ Pokemon Product {i+1}: {product_data.get('product_name', 'Unknown')}")

        # 5. „ÄåÂÖ®„Äá„ÄáÁ®ÆÈ°û„Äç„Å™„Å©„ÅÆË°®Áèæ„ÅßË§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫
        elif self._detect_multiple_by_count_expression(raw_text):
            print("üéØ Detected multiple products by count expression (ÂÖ®„Äá„ÄáÁ®ÆÈ°û)")
            # „Ç´„Éº„Éâ„Éª„Ç∞„ÉÉ„Ç∫Á≥ª„ÅÆË§áÊï∞ÂïÜÂìÅ„Å®„Åó„Å¶Êâ±„ÅÜ
            count_match = re.search(r'ÂÖ®(\d+)Á®ÆÈ°û', raw_text)
            if count_match:
                total_count = int(count_match.group(1))
                print(f"   üìä Total products indicated: {total_count}")
                
                # Âü∫Êú¨ÂïÜÂìÅ„Éá„Éº„Çø„ÇíÂèñÂæó
                base_product = self._parse_product_data_from_text(raw_text)
                
                # Ë§áÊï∞ÂïÜÂìÅ„Å®„Åó„Å¶ÊúÄÂ§ß10ÂïÜÂìÅ„Åæ„ÅßÁîüÊàêÔºàÂÆüÈöõ„ÅÆÂïÜÂìÅÊï∞„Åæ„Åü„ÅØUIË°®Á§∫Áî®Ôºâ
                max_display_products = min(total_count, 10)
                for i in range(max_display_products):
                    product_data = base_product.copy()
                    product_data['product_name'] = f"{base_product.get('product_name', 'Unknown')} - „Çø„Ç§„Éó{i+1}"
                    product_data['product_index'] = i + 1
                    product_data['section_text'] = f"Product {i+1} from {total_count} total variants"
                    products.append(product_data)
                    print(f"   ‚úÖ Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        

        
        final_products = products if len(products) > 1 else []
        if final_products:
            print(f"üéâ MULTI-PRODUCT SUCCESS: Detected {len(final_products)} products")
        else:
            print("üìù SINGLE PRODUCT: No multiple products detected")
        
        return final_products
    
    def _detect_multiple_by_count_expression(self, raw_text: str) -> bool:
        """„ÄåÂÖ®„Äá„ÄáÁ®ÆÈ°û„Äç„Å™„Å©„ÅÆË°®Áèæ„ÅßË§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫"""
        count_patterns = [
            r'ÂÖ®(\d+)Á®ÆÈ°û',
            r'ÂÖ®(\d+)Á®Æ',
            r'(\d+)Á®ÆÈ°û',
            r'(\d+)„Çø„Ç§„Éó',
            r'(\d+)„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥'
        ]
        
        for pattern in count_patterns:
            match = re.search(pattern, raw_text)
            if match:
                count = int(match.group(1))
                if count > 1:  # 2Á®ÆÈ°û‰ª•‰∏ä„Å™„ÇâË§áÊï∞ÂïÜÂìÅ
                    print(f"üîç Found count expression: {match.group(0)} ({count} products)")
                    return True
        return False
    
    def _split_by_jan_codes(self, raw_text: str, jan_codes: list) -> list:
        """JAN„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        text_parts = raw_text
        
        for jan_code in jan_codes:
            # JAN„Ç≥„Éº„ÉâÂë®Ëæ∫„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
            jan_index = text_parts.find(jan_code)
            if jan_index != -1:
                # JAN„Ç≥„Éº„Éâ„ÅÆÂâçÂæå300ÊñáÂ≠ó„ÇíÂïÜÂìÅ„Çª„ÇØ„Ç∑„Éß„É≥„Å®„Åó„Å¶ÊäΩÂá∫
                start = max(0, jan_index - 300)
                end = min(len(text_parts), jan_index + 300)
                section = text_parts[start:end]
                sections.append(section)
        
        return sections
    
    def _split_by_jan_codes_improved(self, raw_text: str, jan_codes: list) -> list:
        """JAN„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„Çà„ÇäÊ≠£Á¢∫„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        
        # ÂêÑJAN„Ç≥„Éº„Éâ„ÅÆ‰ΩçÁΩÆ„ÇíÁâπÂÆö
        jan_positions = []
        for jan_code in jan_codes:
            matches = list(re.finditer(rf'\b{jan_code}\b', raw_text))
            for match in matches:
                jan_positions.append((match.start(), match.end(), jan_code))
        
        # ‰ΩçÁΩÆÈ†Ü„Å´„ÇΩ„Éº„Éà
        jan_positions.sort(key=lambda x: x[0])
        
        # ÂêÑJAN„Ç≥„Éº„ÉâÂë®Ëæ∫„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
        for i, (start_pos, end_pos, jan_code) in enumerate(jan_positions):
            # „Çª„ÇØ„Ç∑„Éß„É≥ÁØÑÂõ≤„ÇíÊ±∫ÂÆö
            section_start = max(0, start_pos - 400)  # „Çà„ÇäÂ∫É„ÅÑÁØÑÂõ≤
            
            if i < len(jan_positions) - 1:
                next_start = jan_positions[i + 1][0]
                section_end = min(next_start - 50, start_pos + 600)
            else:
                section_end = min(len(raw_text), start_pos + 600)
            
            section = raw_text[section_start:section_end].strip()
            if section and jan_code in section:
                sections.append(section)
        
        return sections
    
    def _has_multiple_st_codes(self, text_lines: list) -> bool:
        """Ë§áÊï∞„ÅÆST-„Ç≥„Éº„Éâ„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
        st_codes = []
        for line in text_lines:
            st_matches = re.findall(r'ST-\w+', line)
            st_codes.extend(st_matches)
        
        unique_st_codes = list(set(st_codes))
        return len(unique_st_codes) > 1
    
    def _has_multiple_en_codes(self, text_lines: list) -> bool:
        """Ë§áÊï∞„ÅÆEN-„Ç≥„Éº„Éâ„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
        en_codes = []
        for line in text_lines:
            en_matches = re.findall(r'EN-\d+', line)
            en_codes.extend(en_matches)
        
        unique_en_codes = list(set(en_codes))
        print(f"üîç Found EN codes: {unique_en_codes}")
        return len(unique_en_codes) > 1
    
    def _split_by_st_codes(self, text_lines: list) -> list:
        """ST-„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        current_section = []
        
        for line in text_lines:
            # ST-„Ç≥„Éº„Éâ„ÅåÂê´„Åæ„Çå„ÇãË°å„ÅßÊñ∞„Åó„ÅÑ„Çª„ÇØ„Ç∑„Éß„É≥ÈñãÂßã
            if re.search(r'ST-\w+', line) and current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            
            current_section.append(line)
        
        # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _extract_section_by_st_code(self, raw_text: str, st_code: str) -> str:
        """ST-„Ç≥„Éº„Éâ„Å´Âü∫„Å•„ÅÑ„Å¶„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫"""
        lines = raw_text.split('\n')
        section_lines = []
        st_code_found = False
        
        for i, line in enumerate(lines):
            if st_code in line:
                st_code_found = True
                # ST-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÅÆÂâçÂæå5Ë°å„ÇíÂèñÂæó
                start_idx = max(0, i - 5)
                end_idx = min(len(lines), i + 10)
                section_lines = lines[start_idx:end_idx]
                break
        
        if not st_code_found:
            # ST-„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÂÖ®‰Ωì„ÅÆ‰∏ÄÈÉ®„ÇíËøî„Åô
            return raw_text[:500]
        
        section_text = '\n'.join(section_lines)
        
        # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÊé®Ê∏¨
        character_mapping = {
            'ST-03CB': '„Éî„Ç´„ÉÅ„É•„Ç¶',
            'ST-04CB': '„Ç§„Éº„Éñ„Ç§', 
            'ST-05CB': '„Éè„É™„Éû„É≠„É≥',
            'ST-06CB': '„Éï„Ç©„ÉÉ„Ç≥',
            'ST-07CB': '„Ç±„É≠„Éû„ÉÑ'
        }
        
        if st_code in character_mapping:
            character_name = character_mapping[st_code]
            section_text = f"{character_name} {section_text}"
        
        return section_text

    def _detect_multiple_pokemon_characters(self, raw_text: str) -> bool:
        """„Éù„Ç±„É¢„É≥„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÅÆË§áÊï∞Ê§úÂá∫"""
        pokemon_characters = [
            '„Éî„Ç´„ÉÅ„É•„Ç¶', '„Ç§„Éº„Éñ„Ç§', '„Éè„É™„Éû„É≠„É≥', '„Éï„Ç©„ÉÉ„Ç≥', '„Ç±„É≠„Éû„ÉÑ',
            '„Éï„Ç∑„ÇÆ„ÉÄ„Éç', '„Éí„Éà„Ç´„Ç≤', '„Çº„Éã„Ç¨„É°', '„ÉÅ„Ç≥„É™„Éº„Çø', '„Éí„Éé„Ç¢„É©„Ç∑',
            '„ÉØ„Éã„Éé„Ç≥', '„Ç≠„É¢„É™', '„Ç¢„ÉÅ„É£„É¢', '„Éü„Ç∫„Ç¥„É≠„Ç¶', '„Éä„Ç®„Éà„É´',
            '„Éí„Ç≥„Ç∂„É´', '„Éù„ÉÉ„ÉÅ„É£„Éû', '„ÉÑ„Çø„Éº„Ç∏„É£', '„Éù„Ç´„Éñ', '„Éü„Ç∏„É•„Éû„É´'
        ]
        
        found_characters = []
        for character in pokemon_characters:
            if character in raw_text:
                found_characters.append(character)
        
        print(f"üîç POKEMON CHARACTERS FOUND: {found_characters}")
        return len(found_characters) > 1

    def _split_by_pokemon_characters(self, raw_text: str) -> list:
        """„Éù„Ç±„É¢„É≥„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Åß„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        pokemon_characters = [
            '„Éî„Ç´„ÉÅ„É•„Ç¶', '„Ç§„Éº„Éñ„Ç§', '„Éè„É™„Éû„É≠„É≥', '„Éï„Ç©„ÉÉ„Ç≥', '„Ç±„É≠„Éû„ÉÑ',
            '„Éï„Ç∑„ÇÆ„ÉÄ„Éç', '„Éí„Éà„Ç´„Ç≤', '„Çº„Éã„Ç¨„É°', '„ÉÅ„Ç≥„É™„Éº„Çø', '„Éí„Éé„Ç¢„É©„Ç∑',
            '„ÉØ„Éã„Éé„Ç≥', '„Ç≠„É¢„É™', '„Ç¢„ÉÅ„É£„É¢', '„Éü„Ç∫„Ç¥„É≠„Ç¶', '„Éä„Ç®„Éà„É´',
            '„Éí„Ç≥„Ç∂„É´', '„Éù„ÉÉ„ÉÅ„É£„Éû', '„ÉÑ„Çø„Éº„Ç∏„É£', '„Éù„Ç´„Éñ', '„Éü„Ç∏„É•„Éû„É´'
        ]
        
        character_sections = []
        lines = raw_text.split('\n')
        
        for character in pokemon_characters:
            if character in raw_text:
                # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂê´„ÇÄË°å„ÇíÊé¢„Åó„Å¶„ÄÅ„Åù„ÅÆÂë®Ëæ∫„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
                for i, line in enumerate(lines):
                    if character in line:
                        start_idx = max(0, i - 3)
                        end_idx = min(len(lines), i + 8)
                        section = '\n'.join(lines[start_idx:end_idx])
                        character_sections.append((character, section))
                        break
        
        return character_sections

    def _extract_section_by_jan(self, raw_text: str, jan_code: str) -> str:
        """ÁâπÂÆö„ÅÆJAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄ„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫ÔºàÊîπËâØÁâàÔºâ"""
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        jan_line_index = -1
        
        # JAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÇíÊé¢„Åô
        for i, line in enumerate(text_lines):
            if jan_code in line or jan_code.replace('-', '') in line or f"{jan_code[:7]}-{jan_code[7:]}" in line:
                jan_line_index = i
                break
        
        if jan_line_index == -1:
            # JAN„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÄÅÂÖ®„ÉÜ„Ç≠„Çπ„Éà„ÇíËøî„Åô
            return raw_text
        
        # „Çà„ÇäÁ≤æÂØÜ„Å™„Çª„ÇØ„Ç∑„Éß„É≥ÊäΩÂá∫
        # ÂïÜÂìÅ„Ç≥„Éº„ÉâÔºàST-„Ç≥„Éº„ÉâÔºâ„ÇíÂü∫Ê∫ñ„Å´„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂå∫Âàá„Çã
        section_start = jan_line_index
        section_end = jan_line_index + 1
        
        # ‰∏äÂêë„Åç„Å´Ê§úÁ¥¢„Åó„Å¶„ÄÅ„Åì„ÅÆJAN„Ç≥„Éº„Éâ„Å´ÂØæÂøú„Åô„ÇãÂïÜÂìÅÊÉÖÂ†±„ÅÆÈñãÂßãÁÇπ„ÇíÊé¢„Åô
        for i in range(jan_line_index, max(0, jan_line_index - 10), -1):
            line = text_lines[i]
            # ST-„Ç≥„Éº„Éâ„ÄÅÂïÜÂìÅÂêç„ÄÅ„Åæ„Åü„ÅØÂà•„ÅÆJAN„Ç≥„Éº„Éâ„ÅßÂå∫Âàá„Çä
            if re.search(r'ST-\d{2}[A-Z]{2}', line) or 'ÂïÜÂìÅÂêç' in line:
                section_start = i
                break
            # Âà•„ÅÆJAN„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Å£„Åü„Çâ„ÄÅ„Åù„Åì„ÅßÂå∫Âàá„Çä
            if re.search(r'\b4\d{12}\b', line) and jan_code not in line:
                section_start = i + 1
                break
        
        # ‰∏ãÂêë„Åç„Å´Ê§úÁ¥¢„Åó„Å¶„ÄÅ„Åì„ÅÆJAN„Ç≥„Éº„Éâ„Å´ÂØæÂøú„Åô„ÇãÂïÜÂìÅÊÉÖÂ†±„ÅÆÁµÇ‰∫ÜÁÇπ„ÇíÊé¢„Åô
        for i in range(jan_line_index + 1, min(len(text_lines), jan_line_index + 15)):
            line = text_lines[i]
            # Ê¨°„ÅÆÂïÜÂìÅ„ÅÆST-„Ç≥„Éº„Éâ„Åæ„Åü„ÅØJAN„Ç≥„Éº„Éâ„ÅßÂå∫Âàá„Çä
            if re.search(r'ST-\d{2}[A-Z]{2}', line) or re.search(r'\b4\d{12}\b', line):
                section_end = i
                break
            # ÂïÜÂìÅ„Çµ„Ç§„Ç∫„ÅÆË°å„ÅßÁµÇ‰∫Ü
            if 'ÂïÜÂìÅ„Çµ„Ç§„Ç∫' in line:
                section_end = i + 1
                break
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
        section_lines = text_lines[section_start:section_end]
        section_text = '\n'.join(section_lines)
        
        print(f"üîç Extracted section for JAN {jan_code} (lines {section_start}-{section_end}): {section_text[:100]}...")
        return section_text
    
    def _split_by_en_codes(self, text_lines: list) -> list:
        """EN-„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        current_section = []
        
        for line in text_lines:
            # EN-„Ç≥„Éº„Éâ„ÅåÂê´„Åæ„Çå„ÇãË°å„ÅßÊñ∞„Åó„ÅÑ„Çª„ÇØ„Ç∑„Éß„É≥ÈñãÂßã
            if re.search(r'EN-\d+', line) and current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            
            current_section.append(line)
        
        # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥
        if current_section:
            sections.append('\n'.join(current_section))
        
        print(f"üîç Split into {len(sections)} EN-code sections")
        for i, section in enumerate(sections):
            print(f"   Section {i+1}: {section[:100]}...")
        
        return sections
    
    def _detect_table_structure(self, text_lines: list) -> bool:
        """Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÇíÊ§úÂá∫"""
        # Ë°®„ÅÆ„Éò„ÉÉ„ÉÄ„Éº„Ç≠„Éº„ÉØ„Éº„Éâ
        table_headers = [
            'ÂïÜÂìÅÂêç', 'ÂïÜÂìÅ„Ç≥„Éº„Éâ', 'JAN„Ç≥„Éº„Éâ', '‰æ°Ê†º', 'Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º', 
            'Áô∫Â£≤‰∫àÂÆöÊó•', 'ÂÖ•Êï∞', '„Ç´„Éº„Éà„É≥', '„Éë„ÉÉ„Ç±„Éº„Ç∏', '„Çµ„Ç§„Ç∫',
            'EN-', 'ST-', 'Product', 'Code', 'Price'
        ]
        
        # „Éò„ÉÉ„ÉÄ„ÉºË°å„ÇíÊ§úÂá∫
        header_found = False
        data_rows = 0
        
        for line in text_lines:
            # „Éò„ÉÉ„ÉÄ„ÉºË°å„ÅÆÊ§úÂá∫
            if not header_found and any(keyword in line for keyword in table_headers):
                header_found = True
                print(f"üîç TABLE HEADER DETECTED: {line[:100]}")
                continue
            
            # „Éá„Éº„ÇøË°å„ÅÆÊ§úÂá∫
            if header_found:
                # ÂïÜÂìÅ„Ç≥„Éº„Éâ„ÇÑJAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å
                if (re.search(r'EN-\d+', line) or 
                    re.search(r'ST-\w+', line) or 
                    re.search(r'4\d{12}', line) or
                    '¬•' in line or 'ÂÜÜ' in line):
                    data_rows += 1
                    print(f"üîç TABLE DATA ROW: {line[:100]}")
        
        result = header_found and data_rows >= 2
        print(f"üîç TABLE DETECTION RESULT: header_found={header_found}, data_rows={data_rows}, is_table={result}")
        return result
    
    def _split_table_rows(self, text_lines: list) -> list:
        """Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÇíË°å„Åî„Å®„Å´ÂàÜÂâ≤"""
        sections = []
        header_found = False
        header_line = ""
        processed_products = set()  # ÈáçË§áÈò≤Ê≠¢
        
        # Ë°®„ÅÆ„Éò„ÉÉ„ÉÄ„Éº„Ç≠„Éº„ÉØ„Éº„Éâ
        table_headers = [
            'ÂïÜÂìÅÂêç', 'ÂïÜÂìÅ„Ç≥„Éº„Éâ', 'JAN„Ç≥„Éº„Éâ', '‰æ°Ê†º', 'Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º', 
            'Áô∫Â£≤‰∫àÂÆöÊó•', 'ÂÖ•Êï∞', '„Ç´„Éº„Éà„É≥', '„Éë„ÉÉ„Ç±„Éº„Ç∏', '„Çµ„Ç§„Ç∫'
        ]
        
        for line in text_lines:
            line = line.strip()
            if not line:
                continue
                
            # „Éò„ÉÉ„ÉÄ„ÉºË°å„ÇíÊ§úÂá∫„Éª‰øùÂ≠ò
            if not header_found and any(keyword in line for keyword in table_headers):
                header_found = True
                header_line = line
                print(f"üîç HEADER SAVED: {header_line}")
                continue
            
            # ÂïÜÂìÅ„Éá„Éº„ÇøË°å„ÇíÊ§úÂá∫ÔºàEN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÅÆ„ÅøÔºâ
            if header_found:
                # EN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÅÆ„Åø„ÇíÂïÜÂìÅ„Éá„Éº„Çø„Å®„Åó„Å¶Ë™çË≠òÔºàÈáçË§á„ÇíÈÅø„Åë„Çã„Åü„ÇÅÔºâ
                en_match = re.search(r'EN-(\d+)', line)
                if en_match:
                    en_code = en_match.group(0)  # EN-1420 „Å™„Å©
                    
                    # Êó¢„Å´Âá¶ÁêÜÊ∏à„Åø„ÅÆÂïÜÂìÅ„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                    if en_code in processed_products:
                        continue
                    
                    processed_products.add(en_code)
                    
                    # „Éò„ÉÉ„ÉÄ„ÉºÊÉÖÂ†±„Å®ÁµÑ„ÅøÂêà„Çè„Åõ„Å¶ÂÆåÊï¥„Å™ÂïÜÂìÅÊÉÖÂ†±„Çí‰ΩúÊàê
                    product_section = f"{header_line}\n{line}"
                    sections.append(product_section)
                    print(f"üîç PRODUCT SECTION CREATED: {en_code} - {line[:100]}")
        
        print(f"üîç TOTAL PRODUCT SECTIONS: {len(sections)}")
        return sections
    
    def _has_multiple_product_names(self, text_lines: list) -> bool:
        """Ë§áÊï∞„ÅÆÂïÜÂìÅÂêç„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
        product_name_count = 0
        
        for line in text_lines:
            line = line.strip()
            # ÂïÜÂìÅÂêç„Çâ„Åó„ÅÑ„Éë„Çø„Éº„É≥„Çí„Ç´„Ç¶„É≥„Éà
            if any(keyword in line for keyword in ['ST-', '„Éù„Ç±„É¢„É≥', '„Éî„Ç´„ÉÅ„É•„Ç¶', '„Ç≥„Ç§„É≥„Éê„É≥„ÇØ']):
                if len(line) > 10 and len(line) < 100:
                    product_name_count += 1
        
        return product_name_count > 1
    
    def _split_by_product_names(self, text_lines: list) -> list:
        """ÂïÜÂìÅÂêç„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        current_section = []
        
        for line in text_lines:
            line = line.strip()
            
            # Êñ∞„Åó„ÅÑÂïÜÂìÅ„ÅÆÈñãÂßã„ÇíÊ§úÂá∫
            if any(keyword in line for keyword in ['ST-', 'JAN']):
                if current_section:
                    sections.append('\n'.join(current_section))
                    current_section = []
            
            current_section.append(line)
        
        # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíËøΩÂä†
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _clean_repetitive_text(self, text_lines: list) -> list:
        """Áπ∞„ÇäËøî„Åó„ÉÜ„Ç≠„Çπ„Éà„ÇÑ‰∏çË¶Å„Å™„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
        cleaned_lines = []
        seen_lines = set()
        
        for line in text_lines:
            line = line.strip()
            if not line or len(line) < 3:
                continue
            
            # ÂÆåÂÖ®„Å´Âêå„ÅòË°å„ÅØ1Âõû„Å†„Åë‰øùÊåÅ
            if line in seen_lines:
                continue
            seen_lines.add(line)
            
            # Èï∑„Åô„Åé„ÇãË°åÔºàÁπ∞„ÇäËøî„Åó„ÅÆÂèØËÉΩÊÄßÔºâ„Çí„Çπ„Ç≠„ÉÉ„Éó
            if len(line) > 200:
                continue
            
            # Âêå„ÅòÂçòË™û„ÅåÂ§öÊï∞Áπ∞„ÇäËøî„Åï„Çå„ÇãË°å„Çí„Çπ„Ç≠„ÉÉ„Éó
            words = line.split()
            if len(words) > 10:
                word_counts = {}
                for word in words:
                    if len(word) > 2:
                        word_counts[word] = word_counts.get(word, 0) + 1
                
                # Âêå„ÅòÂçòË™û„ÅåË°å„ÅÆ50%‰ª•‰∏ä„ÇíÂç†„ÇÅ„ÇãÂ†¥Âêà„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                max_count = max(word_counts.values()) if word_counts else 0
                if max_count > len(words) * 0.5:
                    continue
            
            cleaned_lines.append(line)
        
        return cleaned_lines
    
    def _extract_product_name(self, text_lines: list, raw_text: str) -> str:
        """ÂïÜÂìÅÂêç„ÇíÊäΩÂá∫"""
        # Èô§Â§ñ„Åô„Åπ„Åç„Éé„Ç§„Ç∫„ÉÜ„Ç≠„Çπ„Éà
        noise_patterns = [
            '„Ç™„É≥„É©„Ç§„É≥„Ç∑„Éß„ÉÉ„Éó', '„Ç¢„Éã„É°„Ç§„Éà„Ç´„Éï„Çß„Çπ„Çø„É≥„Éâ', 'ÈÄöË≤©', 'Êµ∑Â§ñÂ∫óËàó',
            'animatecafe', 'online', 'shop', 'store', 'www.', 'http',
            '‚Äª', 'Ê≥®ÊÑè', 'Ë≠¶Âëä', 'copyright', '¬©', 'reserved'
        ]
        
        # Áπ∞„ÇäËøî„Åó„Éë„Çø„Éº„É≥„ÇíÈô§Â§ñ
        def is_repetitive_text(text):
            """Áπ∞„ÇäËøî„Åó„ÅÆÂ§ö„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„Åã„Å©„ÅÜ„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
            if len(text) < 10:
                return False
            
            # Âêå„ÅòÊñáÂ≠óÂàó„Åå3Âõû‰ª•‰∏äÁπ∞„ÇäËøî„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
            words = text.split()
            if len(words) > 6:
                word_counts = {}
                for word in words:
                    if len(word) > 3:  # Áü≠„ÅÑÂçòË™û„ÅØÈô§Â§ñ
                        word_counts[word] = word_counts.get(word, 0) + 1
                
                # Âêå„ÅòÂçòË™û„Åå3Âõû‰ª•‰∏äÂá∫Áèæ„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÁπ∞„ÇäËøî„Åó„ÉÜ„Ç≠„Çπ„Éà„Å®Âà§ÂÆö
                for count in word_counts.values():
                    if count >= 3:
                        return True
            return False
        
        # ÊúâÂäπ„Å™ÂïÜÂìÅÂêçÂÄôË£ú„ÇíÊé¢„Åô
        candidates = []
        
        for line in text_lines:
            line = line.strip()
            
            # Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÅÆÂ†¥Âêà„ÄÅ„Éë„Ç§„ÉóÂå∫Âàá„Çä„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
            if '|' in line:
                # „Éë„Ç§„Éó„ÅßÂàÜÂâ≤„Åó„Å¶ÊúÄÂàù„ÅÆÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÈÉ®ÂàÜ„ÇíÂèñÂæó
                parts = [part.strip() for part in line.split('|') if part.strip()]
                if parts:
                    # EN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄÈÉ®ÂàÜ„ÇíÂÑ™ÂÖàÁöÑ„Å´ÈÅ∏Êäû
                    for part in parts:
                        if re.search(r'EN-\d+', part) and len(part) > 10:
                            line = part
                            break
                    else:
                        # EN-„Ç≥„Éº„Éâ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÊúÄÂàù„ÅÆÈï∑„ÅÑÈÉ®ÂàÜ„Çí‰ΩøÁî®
                        line = next((part for part in parts if len(part) > 10), parts[0] if parts else line)
            
            if len(line) < 5 or len(line) > 200:  # Áü≠„Åô„Åé„Çã„ÉªÈï∑„Åô„Åé„ÇãË°å„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                continue
            
            # „Éé„Ç§„Ç∫„Éë„Çø„Éº„É≥„ÇíÂê´„ÇÄË°å„Çí„Çπ„Ç≠„ÉÉ„Éó
            if any(noise in line for noise in noise_patterns):
                continue
            
            # Áπ∞„ÇäËøî„Åó„ÉÜ„Ç≠„Çπ„Éà„Çí„Çπ„Ç≠„ÉÉ„Éó
            if is_repetitive_text(line):
                continue
            
            # ÂïÜÂìÅÂêç„Çâ„Åó„ÅÑ„Éë„Çø„Éº„É≥„ÇíÂÑ™ÂÖà
            score = 0
            
            # ST-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄÂïÜÂìÅÂêçÔºàÊúÄÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if re.search(r'ST-\w+', line):
                score += 15
                
            # EN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄÂïÜÂìÅÂêçÔºàÊúÄÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if re.search(r'EN-\d+', line):
                score += 15
                
            # „Éù„Ç±„É¢„É≥Èñ¢ÈÄ£ÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if '„Éù„Ç±„É¢„É≥' in line and any(keyword in line for keyword in ['„Ç≥„Ç§„É≥„Éê„É≥„ÇØ', '„Éï„Ç£„ÇÆ„É•„Ç¢', '„Å¨„ÅÑ„Åê„Çã„Åø', '„Ç´„Éº„Éâ']):
                score += 12
                
            # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if any(keyword in line for keyword in ['„Ç≠„É£„É©„ÇØ„Çø„Éº', '„Çπ„É™„Éº„Éñ', 'ÁîòÁ•û„Åï„Çì', '„ÅÆÁ∏ÅÁµê„Å≥']):
                score += 12
            
            # „Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞Èñ¢ÈÄ£ÂïÜÂìÅÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞' in line and any(keyword in line for keyword in ['„Éê„ÉÉ„Ç∏', '„Ç´„Éº„Éâ', '„Ç≠„É£„É©', '„Éï„Ç£„ÇÆ„É•„Ç¢']):
                score += 10
            
            # „Éê„Éº„Ç∏„Éß„É≥ÊÉÖÂ†±‰ªò„ÅçÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if any(ver in line.lower() for ver in ['ver.', 'version', 'vol.', 'v.']):
                score += 8
            
            # Á®ÆÈ°ûÊï∞‰ªò„ÅçÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if 'ÂÖ®' in line and 'Á®Æ' in line:
                score += 8
            
            # ÂïÜÂìÅÂêç„Çâ„Åó„ÅÑ„Ç≠„Éº„ÉØ„Éº„ÉâÔºà‰∏≠„Çπ„Ç≥„Ç¢Ôºâ
            product_keywords = ['ÈôêÂÆö', '„Çª„ÉÉ„Éà', '„Éë„ÉÉ„ÇØ', '„Éú„ÉÉ„ÇØ„Çπ', '„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥', '„Ç∑„É™„Éº„Ç∫', 'ÂàùÂõû']
            for keyword in product_keywords:
                if keyword in line:
                    score += 3
            
            # ÈÅ©Â∫¶„Å™Èï∑„ÅïÔºà‰∏≠„Çπ„Ç≥„Ç¢Ôºâ
            if 10 <= len(line) <= 50:
                score += 2
            
            # Êï∞Â≠ó„ÅßÂßã„Åæ„Çâ„Å™„ÅÑÔºà‰Ωé„Çπ„Ç≥„Ç¢Ôºâ
            if not any(char.isdigit() for char in line[:3]):
                score += 1
            
            if score > 0:
                candidates.append((line, score))
        
        # „Çπ„Ç≥„Ç¢„ÅÆÈ´ò„ÅÑÈ†Ü„Å´„ÇΩ„Éº„Éà
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        # ÊúÄÈ´ò„Çπ„Ç≥„Ç¢„ÅÆÂïÜÂìÅÂêç„ÇíËøî„Åô
        if candidates:
            return candidates[0][0]
        
        return None
    
    def _extract_product_code(self, raw_text: str) -> str:
        """ÂïÜÂìÅ„Ç≥„Éº„Éâ„ÇíÊäΩÂá∫ÔºàEN-XXXX, ST-XXXX „Å™„Å©Ôºâ"""
        # ÂïÜÂìÅ„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥
        code_patterns = [
            r'\b(EN-\d+)\b',  # EN-1234 ÂΩ¢Âºè
            r'\b(ST-\w+)\b',  # ST-XXXX ÂΩ¢Âºè
            r'ÂïÜÂìÅ„Ç≥„Éº„Éâ[Ôºö:\s]*([A-Z0-9-]+)',  # ÂïÜÂìÅ„Ç≥„Éº„Éâ: XXXXX
            r'ÂìÅÁï™[Ôºö:\s]*([A-Z0-9-]+)',  # ÂìÅÁï™: XXXXX
        ]
        
        for pattern in code_patterns:
            match = re.search(pattern, raw_text)
            if match:
                code = match.group(1)
                print(f"‚úÖ PRODUCT CODE FOUND: {code}")
                return code
        
        return None
    
    def _extract_jan_code(self, raw_text: str) -> str:
        """JAN„Ç≥„Éº„Éâ„ÇíÊäΩÂá∫Ôºà8Ê°Å„Åæ„Åü„ÅØ13Ê°ÅÔºâ- „Éê„Éº„Ç≥„Éº„ÉâÁîªÂÉèÂØæÂøúÂº∑ÂåñÁâà"""
        # 13Ê°Å„ÅÆJAN„Ç≥„Éº„ÉâÔºà„Éê„Éº„Ç≥„Éº„Éâ„Åã„Çâ„ÅÆÊäΩÂá∫„ÇíÊúÄÂÑ™ÂÖàÔºâ
        jan_13_patterns = [
            r'\b(4\d{12})\b',  # 4„ÅßÂßã„Åæ„Çã13Ê°ÅÔºàÊúÄ„ÇÇ‰∏ÄËà¨ÁöÑÔºâ
            r'(4970381\d{6})',  # „Ç®„É≥„Çπ„Ç´„Ç§„ÅÆÁâπÂÆö„Éë„Çø„Éº„É≥
            r'4970381[-\s]?(\d{6})',  # „Éè„Ç§„Éï„É≥„Åæ„Åü„ÅØ„Çπ„Éö„Éº„Çπ‰ªò„Åç„Ç®„É≥„Çπ„Ç´„Ç§„Éë„Çø„Éº„É≥
            r'JAN[„Ç≥„Éº„ÉâÔºö:\s]*(4\d{12})',  # JAN„Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'ÂçòÂìÅ\s*JAN[„Ç≥„Éº„ÉâÔºö:\s]*(4\d{12})',  # ÂçòÂìÅJAN„Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'„Ç≥„Éº„Éâ[Ôºö:\s]*(4\d{12})',  # „Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'„Éê„Éº„Ç≥„Éº„Éâ[Ôºö:\s]*(4\d{12})',  # „Éê„Éº„Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'(\d{13})',  # ‰ªªÊÑè„ÅÆ13Ê°ÅÔºà„Éê„Éº„Ç≥„Éº„Éâ‰∏ã„ÅÆÊï∞Â≠óÔºâ
        ]
        
        print(f"üîç JAN„Ç≥„Éº„ÉâÊäΩÂá∫ÈñãÂßã: {raw_text[:100]}...")
        
        for i, pattern in enumerate(jan_13_patterns):
            matches = re.findall(pattern, raw_text)
            for match in matches:
                if isinstance(match, tuple):
                    # „Éè„Ç§„Éï„É≥‰ªò„Åç„ÅÆÂ†¥Âêà
                    if len(match) == 1:
                        jan_code = f"4970381{match[0]}"
                    else:
                        jan_code = match[0] if match[0] else match[1]
                else:
                    jan_code = match
                
                # JAN code validation
                if len(jan_code) == 13 and jan_code.isdigit():
                    # „Çà„ÇäÂé≥ÂØÜ„Å™JAN„Ç≥„Éº„Éâ„ÉÅ„Çß„ÉÉ„ÇØ
                    if jan_code.startswith('4'):
                        print(f"‚úÖ JAN CODE FOUND (pattern {i+1}): {jan_code}")
                        return jan_code
                    elif jan_code.startswith('49') or jan_code.startswith('45'):
                        print(f"‚úÖ JAN CODE FOUND (Japan specific): {jan_code}")
                        return jan_code
        
        # 8Ê°Å„ÅÆJAN„Ç≥„Éº„ÉâÔºàÁü≠Á∏ÆÂΩ¢Ôºâ- „Éê„Éº„Ç≥„Éº„Éâ„Åã„Çâ„ÇÇÊäΩÂá∫
        jan_8_patterns = [
            r'\b(\d{8})\b',
            r'Áü≠Á∏Æ[„Ç≥„Éº„ÉâÔºö:\s]*(\d{8})',
            r'8Ê°Å[„Ç≥„Éº„ÉâÔºö:\s]*(\d{8})',
        ]
        
        for pattern in jan_8_patterns:
            match = re.search(pattern, raw_text)
            if match:
                jan_code = match.group(1)
                if jan_code.isdigit() and len(jan_code) == 8:
                    print(f"‚úÖ JAN CODE FOUND (8-digit): {jan_code}")
                    return jan_code
        
        # Additional fallback for any 13-digit number that looks like a JAN code
        all_numbers = re.findall(r'\b(\d{13})\b', raw_text)
        for number in all_numbers:
            if number.startswith(('4', '49', '45')):
                print(f"‚úÖ JAN CODE FOUND (fallback): {number}")
                return number
        
        print("‚ùå No JAN code found in text")
        return None
    
    def _extract_price(self, raw_text: str) -> str:
        """‰æ°Ê†º„ÇíÊäΩÂá∫"""
        # ¬•Ë®òÂè∑‰ªò„Åç„ÅÆ‰æ°Ê†º
        yen_prices = re.findall(r'¬•\s*([0-9,]+)', raw_text)
        for price_str in yen_prices:
            price_num = int(price_str.replace(',', ''))
            if 50 <= price_num <= 100000:  # ÁèæÂÆüÁöÑ„Å™‰æ°Ê†ºÁØÑÂõ≤
                return f"¬•{price_str}"
        
        # ‰æ°Ê†º„ÄÅÂÄ§ÊÆµ„Å™„Å©„ÅÆÊñáÂ≠ó„ÅÆÂæå„ÅÆÊï∞Â≠ó
        price_patterns = [
            r'‰æ°Ê†º[Ôºö:\s]*¬•?([0-9,]+)',
            r'ÂÄ§ÊÆµ[Ôºö:\s]*¬•?([0-9,]+)',
            r'ÂÆö‰æ°[Ôºö:\s]*¬•?([0-9,]+)',
            r'Á®éËæº[Ôºö:\s]*¬•?([0-9,]+)',
            r'([0-9,]+)\s*ÂÜÜ'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, raw_text)
            for price_str in matches:
                price_num = int(price_str.replace(',', ''))
                if 50 <= price_num <= 100000:
                    return f"¬•{price_str}"
        
        return None
    
    def _extract_stock(self, raw_text: str, text_lines: list) -> int:
        """Âú®Â∫´Êï∞„ÇíÊäΩÂá∫"""
        # Âú®Â∫´Èñ¢ÈÄ£„ÅÆ„Ç≠„Éº„ÉØ„Éº„ÉâÂæå„ÅÆÊï∞Â≠ó
        stock_patterns = [
            r'Âú®Â∫´[Ôºö:\s]*(\d+)',
            r'Êï∞Èáè[Ôºö:\s]*(\d+)',
            r'ÊÆã„Çä[Ôºö:\s]*(\d+)',
            r'stock[Ôºö:\s]*(\d+)',
            r'qty[Ôºö:\s]*(\d+)'
        ]
        
        for pattern in stock_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return None
    
    def _extract_category(self, raw_text: str) -> str:
        """„Ç´„ÉÜ„Ç¥„É™„ÇíÊäΩÂá∫„ÉªÊé®ÂÆö"""
        # Áõ¥Êé•ÁöÑ„Å™„Ç´„ÉÜ„Ç¥„É™Ë°®Ë®ò
        category_patterns = [
            r'„Ç´„ÉÜ„Ç¥„É™[Ôºö:\s]*([^\n\r]+)',
            r'ÂàÜÈ°û[Ôºö:\s]*([^\n\r]+)',
            r'„Ç∏„É£„É≥„É´[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in category_patterns:
            match = re.search(pattern, raw_text)
            if match:
                return match.group(1).strip()
        
        # „Ç≠„Éº„ÉØ„Éº„Éâ„Éô„Éº„Çπ„ÅÆÊé®ÂÆö
        if '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞' in raw_text:
            if '„Ç´„Éº„Éâ' in raw_text:
                return '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞„Ç´„Éº„Éâ'
            elif any(keyword in raw_text for keyword in ['„Éê„ÉÉ„Ç∏', 'Áº∂„Éê„ÉÉ„Ç∏', '„Ç∞„ÉÉ„Ç∫']):
                return '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞„Ç∞„ÉÉ„Ç∫'
        
        category_keywords = {
            '„Éï„Ç£„ÇÆ„É•„Ç¢': ['„Éï„Ç£„ÇÆ„É•„Ç¢', 'figure', '„Å≠„Çì„Å©„Çç„ÅÑ„Å©'],
            '„Ç≤„Éº„É†': ['„Ç≤„Éº„É†', 'game', '„ÇΩ„Éï„Éà'],
            '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫': ['„Ç¢„Éã„É°', '„Ç≠„É£„É©„ÇØ„Çø„Éº', 'anime'],
            'Êú¨„ÉªÈõëË™å': ['Êú¨', 'ÈõëË™å', 'book', 'magazine'],
            'Èü≥Ê•Ω': ['CD', 'DVD', '„Éñ„É´„Éº„É¨„Ç§', '„Çµ„Ç¶„É≥„Éâ„Éà„É©„ÉÉ„ÇØ']
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in raw_text for keyword in keywords):
                return category
        
        return None
    
    def _extract_release_date(self, raw_text: str) -> str:
        """Áô∫Â£≤‰∫àÂÆöÊó•„ÇíÊäΩÂá∫"""
        # Êó•‰ªò„Éë„Çø„Éº„É≥
        date_patterns = [
            r'(\d{4})Âπ¥(\d{1,2})Êúà(\d{1,2})Êó•',  # 2024Âπ¥12Êúà15Êó•
            r'(\d{4})Âπ¥(\d{1,2})Êúà',            # 2024Âπ¥12Êúà
            r'(\d{4})/(\d{1,2})/(\d{1,2})',    # 2024/12/15
            r'(\d{4})-(\d{1,2})-(\d{1,2})',    # 2024-12-15
            r'(\d{1,2})/(\d{1,2})/(\d{4})',    # 12/15/2024
        ]
        
        # Áô∫Â£≤Êó•Èñ¢ÈÄ£„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ
        release_keywords = [
            'Áô∫Â£≤‰∫àÂÆöÊó•', 'Áô∫Â£≤Êó•', 'Áô∫Â£≤‰∫àÂÆö', 'Áô∫Â£≤ÈñãÂßãÊó•', '„É™„É™„Éº„ÇπÊó•', 
            'Áô∫Â£≤', 'Ë≤©Â£≤ÈñãÂßã', '‰∫àÂÆöÊó•', 'Áô∫Â£≤ÊôÇÊúü'
        ]
        
        # „Ç≠„Éº„ÉØ„Éº„ÉâÂë®Ëæ∫„ÅÆÊó•‰ªò„ÇíÊ§úÁ¥¢
        for keyword in release_keywords:
            if keyword in raw_text:
                # „Ç≠„Éº„ÉØ„Éº„ÉâÂë®Ëæ∫„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
                keyword_index = raw_text.find(keyword)
                surrounding_text = raw_text[max(0, keyword_index-50):keyword_index+100]
                
                # Êó•‰ªò„Éë„Çø„Éº„É≥„ÇíÊ§úÁ¥¢
                for pattern in date_patterns:
                    match = re.search(pattern, surrounding_text)
                    if match:
                        if len(match.groups()) == 3:
                            year, month, day = match.groups()
                            return f"{year}Âπ¥{int(month)}Êúà{int(day)}Êó•"
                        elif len(match.groups()) == 2:
                            year, month = match.groups()
                            return f"{year}Âπ¥{int(month)}Êúà"
        
        # ÂçòÁã¨„ÅÆÊó•‰ªò„Éë„Çø„Éº„É≥„ÇíÊ§úÁ¥¢Ôºà2024Âπ¥12Êúà„Å™„Å©Ôºâ
        for pattern in date_patterns:
            match = re.search(pattern, raw_text)
            if match:
                if len(match.groups()) == 3:
                    year, month, day = match.groups()
                    # Â¶•ÂΩì„Å™Êó•‰ªò„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                    if 2020 <= int(year) <= 2030 and 1 <= int(month) <= 12:
                        return f"{year}Âπ¥{int(month)}Êúà{int(day)}Êó•"
                elif len(match.groups()) == 2:
                    year, month = match.groups()
                    if 2020 <= int(year) <= 2030 and 1 <= int(month) <= 12:
                        return f"{year}Âπ¥{int(month)}Êúà"
        
        return None
    
    def _extract_brand(self, raw_text: str, text_lines: list) -> str:
        """„Éñ„É©„É≥„ÉâÂêç„ÇíÊäΩÂá∫"""
        # Èô§Â§ñ„Åô„Åπ„Åç„Éé„Ç§„Ç∫„ÉÜ„Ç≠„Çπ„Éà
        noise_patterns = [
            '„Ç™„É≥„É©„Ç§„É≥„Ç∑„Éß„ÉÉ„Éó', '„Ç¢„Éã„É°„Ç§„Éà„Ç´„Éï„Çß„Çπ„Çø„É≥„Éâ', 'ÈÄöË≤©', 'Êµ∑Â§ñÂ∫óËàó',
            'animatecafe', 'online', 'shop', 'store'
        ]
        
        # Áõ¥Êé•ÁöÑ„Å™„Éñ„É©„É≥„ÉâË°®Ë®ò
        brand_patterns = [
            r'„Éñ„É©„É≥„Éâ[Ôºö:\s]*([^\n\r]+)',
            r'„É°„Éº„Ç´„Éº[Ôºö:\s]*([^\n\r]+)',
            r'brand[Ôºö:\s]*([^\n\r]+)',
            r'Ë£ΩÈÄ†ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'Áô∫Â£≤ÂÖÉ[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in brand_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                brand_text = match.group(1).strip()
                # „Éé„Ç§„Ç∫„ÉÜ„Ç≠„Çπ„Éà„ÇíÂê´„Åæ„Å™„ÅÑÂ†¥Âêà„ÅÆ„ÅøËøî„Åô
                if not any(noise in brand_text for noise in noise_patterns) and len(brand_text) < 50:
                    return brand_text
        
        # Êó¢Áü•„ÅÆ„Éñ„É©„É≥„ÉâÂêçÔºàÂÑ™ÂÖàÈ†Ü‰Ωç‰ªò„ÅçÔºâ
        known_brands = [
            '„Ç®„É≥„Çπ„Ç´„Ç§', 'ENSKY', '„Éê„É≥„ÉÄ„Ç§', 'BANDAI', '„Çø„Ç´„É©„Éà„Éü„Éº', 'TAKARA TOMY',
            '„Ç≥„Éä„Éü', 'KONAMI', '„Çª„Ç¨', 'SEGA', '„Çπ„ÇØ„Ç¶„Çß„Ç¢„Éª„Ç®„Éã„ÉÉ„ÇØ„Çπ', 'SQUARE ENIX',
            '„Ç∞„ÉÉ„Éâ„Çπ„Éû„Ç§„É´„Ç´„É≥„Éë„Éã„Éº', 'Good Smile Company', '„Ç≥„Éà„Éñ„Ç≠„É§', 'KOTOBUKIYA',
            '„É°„Éá„Ç£„Ç≥„Çπ', 'MEDICOS', '„Éï„É™„É•„Éº', 'FuRyu', '„Ç¢„É´„Çø„Éº', 'ALTER',
            '„Ç¢„Éã„É°„Ç§„Éà', 'animate'
        ]
        
        # Êó¢Áü•„Éñ„É©„É≥„ÉâÂêç„ÇíÊ§úÁ¥¢ÔºàÊúÄ„ÇÇÁü≠„ÅÑ„Éû„ÉÉ„ÉÅ„ÇíÂÑ™ÂÖàÔºâ
        found_brands = []
        for brand in known_brands:
            if brand in raw_text:
                # Âë®Ëæ∫„ÉÜ„Ç≠„Çπ„Éà„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Å¶„Éé„Ç§„Ç∫„Åß„Å™„ÅÑ„ÅãÁ¢∫Ë™ç
                brand_contexts = []
                for line in text_lines:
                    if brand in line and not any(noise in line for noise in noise_patterns):
                        if len(line) < 100:  # Èï∑„Åô„Åé„ÇãË°å„ÅØÈô§Â§ñ
                            brand_contexts.append(line.strip())
                
                if brand_contexts:
                    # ÊúÄ„ÇÇÁü≠„Åè„Å¶„ÇØ„É™„Éº„É≥„Å™ÊñáËÑà„ÇíÈÅ∏Êäû
                    best_context = min(brand_contexts, key=len)
                    if len(best_context) < 50:
                        found_brands.append((brand, len(best_context)))
        
        # ÊúÄ„ÇÇ„ÇØ„É™„Éº„É≥„Å™„Éñ„É©„É≥„ÉâÂêç„ÇíËøî„Åô
        if found_brands:
            found_brands.sort(key=lambda x: x[1])  # Áü≠„ÅÑÈ†Ü
            return found_brands[0][0]
        
        return None
    
    def _extract_manufacturer(self, raw_text: str, text_lines: list, brand: str) -> str:
        """Ë£ΩÈÄ†ÂÖÉ„ÇíÊäΩÂá∫"""
        # Áõ¥Êé•ÁöÑ„Å™Ë£ΩÈÄ†ÂÖÉË°®Ë®ò
        manufacturer_patterns = [
            r'Ë£ΩÈÄ†ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'Áô∫Â£≤ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'Ë≤©Â£≤ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'manufacturer[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in manufacturer_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # „Éñ„É©„É≥„Éâ„Å®Âêå„ÅòÂ†¥Âêà„ÅåÂ§ö„ÅÑ
        if brand:
            return brand
        
        return None
    
    def _extract_description(self, raw_text: str, text_lines: list) -> str:
        """ÂïÜÂìÅË™¨Êòé„ÇíÊäΩÂá∫"""
        description_parts = []
        
        # Áõ¥Êé•ÁöÑ„Å™Ë™¨ÊòéË°®Ë®ò
        desc_patterns = [
            r'ÂïÜÂìÅË™¨Êòé[Ôºö:\s]*([^\n\r]+)',
            r'Ë™¨Êòé[Ôºö:\s]*([^\n\r]+)',
            r'Ê¶ÇË¶Å[Ôºö:\s]*([^\n\r]+)',
            r'Ë©≥Á¥∞[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in desc_patterns:
            match = re.search(pattern, raw_text)
            if match:
                description_parts.append(match.group(1).strip())
        
        # ÁâπÂæ¥ÁöÑ„Å™„Ç≠„Éº„ÉØ„Éº„Éâ„Åã„ÇâË™¨Êòé„ÇíÁîüÊàê
        feature_keywords = {
            'Overwatch': 'Overwatch',
            '„Ç™„Éº„Éê„Éº„Ç¶„Ç©„ÉÉ„ÉÅ': '„Ç™„Éº„Éê„Éº„Ç¶„Ç©„ÉÉ„ÉÅ',
            '„É°„Çø„É™„ÉÉ„ÇØ': '„É°„Çø„É™„ÉÉ„ÇØ‰ªï‰∏ä„Åí',
            '„Éõ„É≠„Ç∞„É©„É†': '„Éõ„É≠„Ç∞„É©„É†Âä†Â∑•',
            '„ÇØ„É™„Ç¢': '„ÇØ„É™„Ç¢Á¥†Êùê',
            'ÈôêÂÆö': 'ÈôêÂÆöÂïÜÂìÅ',
            '„Ç≠„É£„É©„ÇØ„Çø„Éº': '„Ç≠„É£„É©„ÇØ„Çø„Éº„Ç∞„ÉÉ„Ç∫'
        }
        
        for keyword, desc in feature_keywords.items():
            if keyword in raw_text and desc not in description_parts:
                description_parts.append(desc)
        
        # Á®ÆÈ°ûÊï∞ÊÉÖÂ†±
        spec_match = re.search(r'ÂÖ®(\d+)Á®Æ', raw_text)
        if spec_match:
            description_parts.append(f"ÂÖ®{spec_match.group(1)}Á®ÆÈ°û")
        
        return ' '.join(description_parts) if description_parts else None
    
    def _extract_weight(self, raw_text: str) -> str:
        """ÈáçÈáè„Éª„Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        weight_patterns = [
            r'ÈáçÈáè[Ôºö:\s]*([0-9.]+\s*[gkg„Ç∞„É©„É†„Ç≠„É≠]+)',
            r'Èáç„Åï[Ôºö:\s]*([0-9.]+\s*[gkg„Ç∞„É©„É†„Ç≠„É≠]+)',
            r'([0-9.]+)\s*(g|kg|„Ç∞„É©„É†|„Ç≠„É≠)',
            r'„Çµ„Ç§„Ç∫[Ôºö:\s]*([0-9.√óxX\s]*[cmmm„Ç§„É≥„ÉÅ]+)',
            r'([0-9.]+)\s*(mm|cm|„Ç§„É≥„ÉÅ)'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, raw_text)
            if match:
                return match.group(1).strip()
        
        return None
    
    def _extract_color(self, raw_text: str, text_lines: list) -> str:
        """Ëâ≤ÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        # Áõ¥Êé•ÁöÑ„Å™Ëâ≤Ë°®Ë®ò
        color_patterns = [
            r'Ëâ≤[Ôºö:\s]*([^\n\r]+)',
            r'„Ç´„É©„Éº[Ôºö:\s]*([^\n\r]+)',
            r'color[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in color_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Ëâ≤Âêç„ÅÆÊ§úÂá∫
        colors = [
            'Ëµ§', 'Èùí', 'Á∑ë', 'ÈªÑ', 'Èªí', 'ÁôΩ', 'Ëå∂', 'Á¥´', 'Ê©ô', '„Éî„É≥„ÇØ',
            '„É¨„ÉÉ„Éâ', '„Éñ„É´„Éº', '„Ç∞„É™„Éº„É≥', '„Ç§„Ç®„É≠„Éº', '„Éñ„É©„ÉÉ„ÇØ', '„Éõ„ÉØ„Ç§„Éà',
            '„Ç¥„Éº„É´„Éâ', '„Ç∑„É´„Éê„Éº', '„É°„Çø„É™„ÉÉ„ÇØ', '„ÇØ„É™„Ç¢', 'ÈÄèÊòé'
        ]
        
        for color in colors:
            if color in raw_text:
                return color
        
        return None
    
    def _extract_material(self, raw_text: str, text_lines: list) -> str:
        """Á¥†ÊùêÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        # Áõ¥Êé•ÁöÑ„Å™Á¥†ÊùêË°®Ë®ò
        material_patterns = [
            r'Á¥†Êùê[Ôºö:\s]*([^\n\r]+)',
            r'ÊùêË≥™[Ôºö:\s]*([^\n\r]+)',
            r'material[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in material_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Á¥†ÊùêÂêç„ÅÆÊ§úÂá∫
        materials = [
            '„Éó„É©„Çπ„ÉÅ„ÉÉ„ÇØ', 'PVC', 'ABS', 'ÈáëÂ±û', '„É°„Çø„É´', '„Ç¢„É´„Éü',
            'Á¥ô', '„Éö„Éº„Éë„Éº', 'Â∏É', '„Éï„Ç°„Éñ„É™„ÉÉ„ÇØ', '„É¨„Ç∂„Éº', 'Èù©',
            '„Ç¨„É©„Çπ', '„Ç¢„ÇØ„É™„É´', '„Éù„É™„Ç®„Çπ„ÉÜ„É´', 'Êú®Êùê', '„Ç¶„ÉÉ„Éâ'
        ]
        
        for material in materials:
            if material in raw_text:
                return material
        
        return None
    
    def _extract_origin(self, raw_text: str, text_lines: list) -> str:
        """ÂéüÁî£Âú∞ÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        # Áõ¥Êé•ÁöÑ„Å™ÂéüÁî£Âú∞Ë°®Ë®ò
        origin_patterns = [
            r'ÂéüÁî£ÂõΩ[Ôºö:\s]*([^\n\r]+)',
            r'Ë£ΩÈÄ†ÂõΩ[Ôºö:\s]*([^\n\r]+)',
            r'ÂéüÁî£Âú∞[Ôºö:\s]*([^\n\r]+)',
            r'made\s+in\s+([^\n\r]+)'
        ]
        
        for pattern in origin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # ÂõΩÂêç„ÅÆÊ§úÂá∫
        countries = [
            'Êó•Êú¨', 'Japan', '‰∏≠ÂõΩ', 'China', 'ÈüìÂõΩ', 'Korea',
            '„Ç¢„É°„É™„Ç´', 'USA', '„Éâ„Ç§„ÉÑ', 'Germany', '„Éï„É©„É≥„Çπ', 'France'
        ]
        
        for country in countries:
            if country in raw_text:
                return country
        
        return None
    
    def _extract_warranty(self, raw_text: str, text_lines: list) -> str:
        """‰øùË®ºÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        warranty_patterns = [
            r'‰øùË®º[Ôºö:\s]*([^\n\r]+)',
            r'warranty[Ôºö:\s]*([^\n\r]+)',
            r'‰øùË®ºÊúüÈñì[Ôºö:\s]*([^\n\r]+)',
            r'(\d+)\s*(Âπ¥|„É∂Êúà|„ÅãÊúà)\s*‰øùË®º'
        ]
        
        for pattern in warranty_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return None 
    
    def _create_st_jan_mapping(self, raw_text: str, st_patterns: list, jan_patterns: list) -> dict:
        """ST-„Ç≥„Éº„Éâ„Å®JAN„Ç≥„Éº„Éâ„ÅÆÊ≠£Á¢∫„Å™„Éû„ÉÉ„Éî„É≥„Ç∞„Çí‰ΩúÊàê"""
        mapping = {}
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        
        # ÂêÑST-„Ç≥„Éº„Éâ„Å´„Å§„ÅÑ„Å¶„ÄÅ„Åù„ÅÆËøë„Åè„Å´„ÅÇ„ÇãJAN„Ç≥„Éº„Éâ„ÇíÊé¢„Åô
        for st_code in st_patterns:
            for i, line in enumerate(text_lines):
                if st_code in line:
                    # ST-„Ç≥„Éº„Éâ„ÅÆË°å„Åã„Çâ‰∏ãÂêë„Åç„Å´ÊúÄÂ§ß10Ë°åÊ§úÁ¥¢
                    for j in range(i, min(len(text_lines), i + 10)):
                        jan_match = re.search(r'\b(4\d{12})\b', text_lines[j])
                        if jan_match:
                            jan_code = jan_match.group(1)
                            mapping[st_code] = jan_code
                            print(f"   üîó Found mapping: {st_code} -> {jan_code}")
                            break
                    break
        
        # ÊÆã„Çä„ÅÆJAN„Ç≥„Éº„Éâ„ÇíÊú™„Éû„ÉÉ„Éî„É≥„Ç∞„ÅÆST-„Ç≥„Éº„Éâ„Å´Ââ≤„ÇäÂΩì„Å¶
        used_jans = set(mapping.values())
        unused_jans = [jan for jan in jan_patterns if jan not in used_jans]
        unmapped_sts = [st for st in st_patterns if st not in mapping]
        
        for st_code, jan_code in zip(unmapped_sts, unused_jans):
            mapping[st_code] = jan_code
            print(f"   üîß Auto-mapped: {st_code} -> {jan_code}")
        
        return mapping
    
    def _extract_precise_section_by_st_code(self, raw_text: str, st_code: str, all_st_codes: list) -> str:
        """ST-„Ç≥„Éº„Éâ„Å´Âü∫„Å•„ÅÑ„Å¶„Çà„ÇäÁ≤æÂØÜ„Å™„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫"""
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        section_lines = []
        st_line_index = -1
        
        # ST-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÇíÊé¢„Åô
        for i, line in enumerate(text_lines):
            if st_code in line:
                st_line_index = i
                break
        
        if st_line_index == -1:
            return raw_text[:500]  # ST-„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÅÆÈñãÂßãÁÇπ„ÇíÊé¢„ÅôÔºà‰∏äÂêë„ÅçÊ§úÁ¥¢Ôºâ
        section_start = st_line_index
        for i in range(st_line_index, max(0, st_line_index - 15), -1):
            line = text_lines[i]
            # ÂïÜÂìÅÂêç„ÅÆË°å„Åæ„Åü„ÅØÂâç„ÅÆÂïÜÂìÅ„ÅÆST-„Ç≥„Éº„Éâ„ÅßÂå∫Âàá„Çä
            if 'ÂïÜÂìÅÂêç' in line and ('„ÇΩ„Éï„Éì' in line or '„Éù„Ç±„É¢„É≥' in line):
                section_start = i
                break
            # ‰ªñ„ÅÆST-„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Å£„Åü„Çâ„Åù„Åì„ÅßÂå∫Âàá„Çä
            other_st_codes = [code for code in all_st_codes if code != st_code]
            if any(other_st in line for other_st in other_st_codes):
                section_start = i + 1
                break
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÅÆÁµÇ‰∫ÜÁÇπ„ÇíÊé¢„ÅôÔºà‰∏ãÂêë„ÅçÊ§úÁ¥¢Ôºâ
        section_end = len(text_lines)
        for i in range(st_line_index + 1, min(len(text_lines), st_line_index + 20)):
            line = text_lines[i]
            # Ê¨°„ÅÆÂïÜÂìÅ„ÅÆST-„Ç≥„Éº„Éâ„Åæ„Åü„ÅØÂïÜÂìÅÂêç„ÅßÂå∫Âàá„Çä
            other_st_codes = [code for code in all_st_codes if code != st_code]
            if any(other_st in line for other_st in other_st_codes):
                section_end = i
                break
            if 'ÂïÜÂìÅÂêç' in line and ('„ÇΩ„Éï„Éì' in line or '„Éù„Ç±„É¢„É≥' in line) and i > st_line_index + 3:
                section_end = i
                break
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
        section_lines = text_lines[section_start:section_end]
        section_text = '\n'.join(section_lines)
        
        # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂâç„Å´ËøΩÂä†
        character_name = self._get_character_for_st_code(st_code)
        if character_name:
            section_text = f"{character_name} {section_text}"
        
        print(f"üîç Extracted precise section for {st_code} (lines {section_start}-{section_end}): {section_text[:100]}...")
        return section_text
    
    def _get_character_for_st_code(self, st_code: str) -> str:
        """ST-„Ç≥„Éº„Éâ„Å´ÂØæÂøú„Åô„Çã„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂèñÂæó"""
        character_mapping = {
            'ST-03CB': '„Éî„Ç´„ÉÅ„É•„Ç¶',
            'ST-04CB': '„Ç§„Éº„Éñ„Ç§', 
            'ST-05CB': '„Éè„É™„Éû„É≠„É≥',
            'ST-06CB': '„Éï„Ç©„ÉÉ„Ç≥',
            'ST-07CB': '„Ç±„É≠„Éû„ÉÑ'
        }
        return character_mapping.get(st_code, '')