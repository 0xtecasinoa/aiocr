"""
OpenAI OCR Service for high-accuracy text extraction using GPT-4 Vision API.
Provides near 100% accuracy for Japanese and English text extraction.
"""

import asyncio
import logging
import base64
import json
from typing import Dict, Any, Optional, List
from pathlib import Path
import re
from PIL import Image
import io
import pandas as pd
from openai import AsyncOpenAI
from app.core.config import Settings

logger = logging.getLogger(__name__)
settings = Settings()


class OpenAIOCRService:
    """High-accuracy OCR service using OpenAI GPT-4 Vision API."""
    
    def __init__(self):
        self.client = None
        self.model = settings.OPENAI_MODEL
        self.supported_formats = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.pdf', '.xls', '.xlsx']
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize OpenAI client with proper error handling."""
        if not settings.OPENAI_API_KEY or settings.OPENAI_API_KEY == "your-openai-api-key-here":
            print("\n" + "="*80)
            print("âš ï¸  OPENAI API KEY NOT CONFIGURED")
            print("="*80)
            print("To use OCR functionality, you need to:")
            print("1. Get an OpenAI API key from: https://platform.openai.com/api-keys")
            print("2. Create a .env file in the server directory")
            print("3. Add: OPENAI_API_KEY=your-actual-api-key-here")
            print("4. Restart the server")
            print("="*80 + "\n")
            logger.warning("OPENAI_API_KEY not set. OpenAI OCR service will not be available.")
            return
        
        try:
            self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            print(f"âœ… OpenAI client initialized successfully with model: {settings.OPENAI_MODEL}")
            logger.info("OpenAI client initialized successfully")
        except Exception as e:
            print(f"âŒ Failed to initialize OpenAI client: {str(e)}")
            logger.error(f"Failed to initialize OpenAI client: {str(e)}")
            self.client = None
        
    def _encode_image_to_base64(self, image_path: str) -> str:
        """Encode image to base64 for OpenAI API."""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    
    def _optimize_image_for_ocr(self, image_path: str) -> str:
        """Optimize image for better OCR results, especially for barcode images."""
        try:
            from PIL import ImageEnhance, ImageFilter
            
            # Open and process image
            with Image.open(image_path) as img:
                # Convert to RGB if needed
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Resize if too large (OpenAI has size limits)
                max_size = 2048
                if max(img.size) > max_size:
                    img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                
                # Enhance image for better OCR, especially for barcodes
                # Increase contrast to make text and barcodes more readable
                enhancer = ImageEnhance.Contrast(img)
                img = enhancer.enhance(1.3)  # Increase contrast by 30%
                
                # Increase sharpness for better text recognition
                enhancer = ImageEnhance.Sharpness(img)
                img = enhancer.enhance(1.2)  # Increase sharpness by 20%
                
                # Apply slight unsharp mask for barcode clarity
                img = img.filter(ImageFilter.UnsharpMask(radius=1, percent=120, threshold=2))
                
                # Save optimized image
                optimized_path = str(Path(image_path).with_suffix('.optimized.jpg'))
                img.save(optimized_path, 'JPEG', quality=98, optimize=True)  # Higher quality for barcodes
                
                print(f"ğŸ–¼ï¸ Image optimized for barcode OCR: {optimized_path}")
                return optimized_path
                
        except Exception as e:
            logger.warning(f"Image optimization failed: {str(e)}, using original")
            return image_path
    
    async def extract_text_from_image(
        self,
        image_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from image using OpenAI GPT-4 Vision API.
        
        Args:
            image_path: Path to the image file
            language: Language hint (jpn+eng for Japanese and English)
            confidence_threshold: Not used for OpenAI (always high confidence)
        
        Returns:
            Dict containing extracted text and metadata
        """
        if not self.client:
            raise Exception("OpenAI client is not initialized. Please check OPENAI_API_KEY configuration.")
        
        try:
            start_time = asyncio.get_event_loop().time()
            
            # Optimize image for better results
            optimized_path = self._optimize_image_for_ocr(image_path)
            
            # Encode image to base64
            base64_image = self._encode_image_to_base64(optimized_path)
            
            # Determine language context
            language_context = ""
            if "jpn" in language.lower():
                language_context = "This image contains Japanese text (hiragana, katakana, kanji). "
            if "eng" in language.lower():
                language_context += "This image contains English text. "
            
            # Create comprehensive OCR prompt for maximum accuracy and direct structured extraction
            ocr_prompt = f"""
            {language_context}
            
            You are an advanced OCR and data extraction AI specialized in Japanese product specification sheets (å•†å“æ¡ˆå†…æ›¸/ä»•æ§˜æ›¸).
            
            CRITICAL: This image may contain MULTIPLE DIFFERENT PRODUCTS. Each product should be extracted as a separate object.
            
            EXTRACTION REQUIREMENTS - For EACH product, extract the following 15 PRACTICAL FIELDS:
            
            **åŸºæœ¬æƒ…å ± (Basic Information):**
            1. product_name - å•†å“å (Product name, often contains character names and item type)
            2. product_code - å“ç•ª/å•†å“ç•ªå· (Product code like EN-1420, ST-03CB, etc.)
            3. character_name - ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å (Character/IP name if applicable)
            4. release_date - ç™ºå£²äºˆå®šæ—¥ (Release date in format: YYYYå¹´MMæœˆDDæ—¥ or YYYY/MM/DD)
            5. reference_sales_price - å¸Œæœ›å°å£²ä¾¡æ ¼ (Suggested retail price as a number, e.g., 1100, 2400)
            
            **JANã‚³ãƒ¼ãƒ‰/ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ (Barcode Information):**
            6. jan_code - å˜å“ JANã‚³ãƒ¼ãƒ‰ (Single item JAN code, 13-digit barcode starting with 4970381 or similar)
            7. inner_box_gtin - BOX/å†…ç®± JANã‚³ãƒ¼ãƒ‰ (Box/Inner box JAN code, 13-14 digits)
            
            **ã‚µã‚¤ã‚ºæƒ…å ± (Size Information):**
            8. single_product_size - å•†å“ã‚µã‚¤ã‚º (Product size like "ç´„107Ã—70Ã—61mm" or "H150Ã—W100mm")
            9. package_size - ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚µã‚¤ã‚º (Package size dimensions)
            10. inner_box_size - å†…ç®±ã‚µã‚¤ã‚º (Inner box size dimensions)
            11. carton_size - ã‚«ãƒ¼ãƒˆãƒ³ã‚µã‚¤ã‚º (Carton/outer box size dimensions)
            
            **æ•°é‡ãƒ»æ¢±åŒ…æƒ…å ± (Quantity & Packaging):**
            12. quantity_per_pack - å…¥æ•° (Quantity per pack like "12å€‹", "60", "16ã‚±Ã—15B")
            13. case_pack_quantity - ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°/ã‚±ãƒ¼ã‚¹æ¢±å…¥æ•° (Case pack quantity as integer, e.g., 72, 240)
            
            **å•†å“è©³ç´° (Product Details):**
            14. package_type - ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å½¢æ…‹ (Package type like "ãƒ–ãƒªã‚¹ã‚¿ãƒ¼", "ç®±", "è¢‹")
            15. description - ã‚»ãƒƒãƒˆå†…å®¹ãƒ»ç´ æãƒ»ä»•æ§˜ãªã© (Set contents, materials, specifications)
            
            **IMPORTANT EXTRACTION PATTERNS:**
            
            - **Product Name**: Look for "å•†å“å", often includes character names and item type (e.g., "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¹ãƒªãƒ¼ãƒ–ã€ç”˜ç¥ã•ã‚“ã¡ã®ç¸çµã³ã€", "ãƒã‚±ãƒ¢ãƒ³ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯")
            - **Product Code**: Format EN-XXXX, ST-XXCB, or similar alphanumeric codes
            - **Release Date**: Look for "ç™ºå£²äºˆå®šæ—¥", "ç™ºå£²æ—¥" followed by date (2025å¹´1æœˆ24æ—¥, 2024å¹´12æœˆ, etc.)
            - **Price**: Look for "å¸Œæœ›å°å£²ä¾¡æ ¼", "ç¨æŠœä¾¡æ ¼", often with Â¥ symbol (Â¥1,100, 2,400å††)
            - **JAN Code**: 13-digit barcode, often starts with 4970381 or 4571622. Look for numbers under barcode images.
            - **Sizes**: Look for "ç´„XXXÃ—YYYÃ—ZZZmm" or "HXXÃ—WYYmm" patterns
            - **Quantity**: Look for "XXå…¥", "XXã‚±", "XXå€‹å…¥ã‚Š", "XXãƒ‘ãƒƒã‚¯Ã—YBOX"
            - **Case Pack**: Look for "ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°", "ã‚±ãƒ¼ã‚¹æ¢±å…¥æ•°", total quantity calculations
            
            **BARCODE READING PRIORITY:**
            - Look for BLACK AND WHITE STRIPED BARCODE PATTERNS
            - Read the numbers displayed UNDER the barcode stripes carefully
            - JAN codes are typically 13 digits starting with 4 (e.g., 4970381806170, 4571622782781)
            - Inner box codes may have different prefixes
            
            **MULTI-PRODUCT HANDLING:**
            - If you detect multiple products (different product codes, JAN codes, or character names), extract each as a separate product
            - Each product should have its own complete set of fields
            - Do NOT mix information from different products
            - Look for product separators like different EN-codes, ST-codes, or character names
            
            **PRICE HANDLING:**
            - Extract the numeric value only (remove Â¥, å††, commas)
            - If both ç¨è¾¼ and ç¨æŠœ prices are shown, prefer ç¨æŠœ (tax-excluded) price
            - Example: "1ãƒ‘ãƒƒã‚¯2,100å††ï¼ˆç¨æŠœä¾¡æ ¼1,100å††ï¼‰" â†’ extract 1100
            
            **SIZE FORMAT EXAMPLES:**
            - "ç´„107Ã—70Ã—61mm" â†’ "ç´„107Ã—70Ã—61mm"
            - "H150Ã—W100mm" â†’ "H150Ã—W100mm"
            - "63Ã—89mm" â†’ "63Ã—89mm"
            
            RESPONSE FORMAT - Return ONLY valid JSON in this exact structure:
            {{
                "raw_text": "All visible text extracted from the image",
                "confidence_score": 95.0,
                "language_detected": "japanese",
                "products": [
                    {{
                        "product_name": "extracted value or null",
                        "product_code": "extracted value or null",
                        "character_name": "extracted value or null",
                        "release_date": "extracted value or null",
                        "reference_sales_price": number or null,
                        "jan_code": "extracted value or null",
                        "inner_box_gtin": "extracted value or null",
                        "single_product_size": "extracted value or null",
                        "package_size": "extracted value or null",
                        "inner_box_size": "extracted value or null",
                        "carton_size": "extracted value or null",
                        "quantity_per_pack": "extracted value or null",
                        "case_pack_quantity": number or null,
                        "package_type": "extracted value or null",
                        "description": "extracted value or null"
                    }}
                ]
            }}
            
            CRITICAL RULES:
            1. Return ONLY valid JSON - no markdown, no extra text
            2. If a field is not visible in the image, set it to null (not empty string)
            3. For numbers (prices, quantities), return as numbers not strings
            4. For dates, preserve the original format from the image
            5. Extract Japanese text exactly as shown (kanji, hiragana, katakana)
            6. If only 1 product is detected, the "products" array should have 1 object
            7. If multiple products are detected, create separate objects for each
            8. Focus on ACCURACY over completeness - only extract what you can clearly see
            """
            
            print(f"ğŸ¤– OPENAI OCR: Processing image with {self.model}")
            
            # Call OpenAI Vision API
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": ocr_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"  # High detail for better OCR accuracy
                                }
                            }
                        ]
                    }
                ],
                max_tokens=16000,  # Increased for 38 fields per product
                temperature=0.1  # Low temperature for consistent, accurate results
            )
            
            # Parse response
            response_text = response.choices[0].message.content
            
            print(f"ğŸ” DEBUG: OpenAI Raw Response:")
            print(f"Response length: {len(response_text)}")
            print(f"First 500 chars: {response_text[:500]}")
            print(f"Last 500 chars: {response_text[-500:]}")
            
            try:
                # Try to parse as JSON first
                if response_text.strip().startswith('{'):
                    print("ğŸ” DEBUG: Parsing as direct JSON")
                    result = json.loads(response_text)
                else:
                    # If not JSON, extract JSON from markdown code blocks
                    json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                    if json_match:
                        print("ğŸ” DEBUG: Found JSON in markdown code block")
                        result = json.loads(json_match.group(1))
                    else:
                        # Try to find complete JSON with products array
                        json_match = re.search(r'(\{.*?"products"\s*:\s*\[.*?\].*?\})', response_text, re.DOTALL)
                        if json_match:
                            print("ğŸ” DEBUG: Found JSON with products array")
                            result = json.loads(json_match.group(1))
                        else:
                            # Try to find any JSON object
                            json_match = re.search(r'(\{.*?"raw_text".*?\})', response_text, re.DOTALL)
                            if json_match:
                                print("ğŸ” DEBUG: Found basic JSON pattern")
                                result = json.loads(json_match.group(1))
                            else:
                                print("âš ï¸  DEBUG: No JSON found, using fallback")
                                # Fallback: treat entire response as raw text
                                result = {
                                    "raw_text": response_text,
                                    "confidence_score": 90.0,
                                    "language_detected": "unknown",
                                    "products": [],
                                    "word_confidences": {},
                                    "processing_metadata": {"method": "openai_gpt4_vision", "model": self.model}
                                }
                            
                print(f"ğŸ” DEBUG: Parsed result keys: {list(result.keys())}")
                if 'products' in result:
                    print(f"ğŸ” DEBUG: Products found: {len(result.get('products', []))} items")
                    for i, p in enumerate(result.get('products', [])[:3]):  # Show first 3 products
                        print(f"  Product {i+1}: {p.get('product_name', 'N/A')} | JAN: {p.get('jan_code', 'N/A')}")
                        
            except json.JSONDecodeError as e:
                print(f"âš ï¸  DEBUG: JSON parsing failed: {e}")
                logger.warning(f"Failed to parse OpenAI response as JSON: {e}")
                # Fallback to treating response as raw text
                result = {
                    "raw_text": response_text,
                    "confidence_score": 90.0,
                    "language_detected": "unknown", 
                    "product_info": {},
                    "word_confidences": {},
                    "processing_metadata": {"method": "openai_gpt4_vision", "model": self.model}
                }
            
            # Clean up optimized image if it was created
            if optimized_path != image_path:
                try:
                    Path(optimized_path).unlink()
                except:
                    pass
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            result["processing_time_ms"] = processing_time_ms
            
            # Ensure required fields exist
            result.setdefault("raw_text", "")
            result.setdefault("confidence_score", 95.0)  # OpenAI typically has high confidence
            result.setdefault("language_detected", "unknown")
            result.setdefault("word_confidences", {})
            result.setdefault("bounding_boxes", [])
            result.setdefault("text_blocks", [])
            
            # Check if OpenAI returned structured products array
            raw_text = result.get("raw_text", "")
            products_from_ai = result.get("products", [])
            
            if products_from_ai and len(products_from_ai) > 0:
                print(f"âœ… OPENAI RETURNED {len(products_from_ai)} STRUCTURED PRODUCTS")
                
                # Process products from OpenAI's structured response
                structured_products = []
                for i, ai_product in enumerate(products_from_ai):
                    # OpenAI returned 15 practical fields - use them directly
                    product_data = {
                        # 15 Practical Fields
                        "product_name": ai_product.get('product_name'),
                        "product_code": ai_product.get('product_code'),
                        "character_name": ai_product.get('character_name'),
                        "release_date": ai_product.get('release_date'),
                        "reference_sales_price": ai_product.get('reference_sales_price'),
                        "jan_code": ai_product.get('jan_code'),
                        "inner_box_gtin": ai_product.get('inner_box_gtin'),
                        "single_product_size": ai_product.get('single_product_size'),
                        "package_size": ai_product.get('package_size'),
                        "inner_box_size": ai_product.get('inner_box_size'),
                        "carton_size": ai_product.get('carton_size'),
                        "quantity_per_pack": ai_product.get('quantity_per_pack'),
                        "case_pack_quantity": ai_product.get('case_pack_quantity'),
                        "package_type": ai_product.get('package_type'),
                        "description": ai_product.get('description'),
                        
                        # Legacy fields for backward compatibility
                        "sku": ai_product.get('product_code'),  # Use product_code as SKU
                        "price": ai_product.get('reference_sales_price'),
                        
                        # Meta fields
                        "product_index": i + 1,
                        "section_text": raw_text[:300] + "..." if len(raw_text) > 300 else raw_text
                    }
                    structured_products.append(product_data)
                    
                    # Count how many of the 15 fields were extracted
                    fields_15 = [
                        'product_name', 'product_code', 'character_name', 'release_date',
                        'reference_sales_price', 'jan_code', 'inner_box_gtin', 'single_product_size',
                        'package_size', 'inner_box_size', 'carton_size', 'quantity_per_pack',
                        'case_pack_quantity', 'package_type', 'description'
                    ]
                    extracted_count = sum(1 for field in fields_15 if product_data.get(field))
                    
                    print(f"ğŸ“¦ Product {i+1}: {extracted_count}/15 fields extracted")
                    print(f"  å•†å“å: {product_data.get('product_name', 'Not detected')}")
                    print(f"  å“ç•ª: {product_data.get('product_code', 'Not detected')}")
                    print(f"  ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å: {product_data.get('character_name', 'Not detected')}")
                    print(f"  ç™ºå£²äºˆå®šæ—¥: {product_data.get('release_date', 'Not detected')}")
                    print(f"  å¸Œæœ›å°å£²ä¾¡æ ¼: {product_data.get('reference_sales_price', 'Not detected')}")
                    print(f"  JANã‚³ãƒ¼ãƒ‰: {product_data.get('jan_code', 'Not detected')}")
                
                # Create structured_data with first product as main, all products in _products_list
                if len(structured_products) > 1:
                    structured_data = structured_products[0].copy()
                    structured_data["has_multiple_products"] = True
                    structured_data["total_products_detected"] = len(structured_products)
                    structured_data["_products_list"] = structured_products
                else:
                    structured_data = structured_products[0]
                    structured_data["has_multiple_products"] = False
            else:
                # Fallback: Use Python regex extraction if OpenAI didn't return products
                print("âš ï¸ OpenAI didn't return structured products, falling back to Python extraction")
                multiple_products = self._detect_multiple_products(raw_text)
                
                if multiple_products:
                    print(f"ğŸ” DETECTED MULTIPLE PRODUCTS: {len(multiple_products)} products found")
                    structured_data = {
                        "product_name": multiple_products[0].get('product_name'),
                        "sku": multiple_products[0].get('sku'),
                        "jan_code": multiple_products[0].get('jan_code'),
                        "price": multiple_products[0].get('price'),
                        "release_date": multiple_products[0].get('release_date'),
                        "category": multiple_products[0].get('category'),
                        "brand": multiple_products[0].get('brand'),
                        "manufacturer": multiple_products[0].get('manufacturer'),
                        "product_index": multiple_products[0].get('product_index', 1),
                        "section_text": multiple_products[0].get('section_text', ''),
                        "total_products_detected": len(multiple_products),
                        "has_multiple_products": True,
                        "_products_list": multiple_products
                    }
                else:
                    # Single product processing
                    structured_data = self._parse_product_data_from_text(raw_text)
                    structured_data["has_multiple_products"] = False
            
            result["structured_data"] = structured_data
            
            print(f"âœ… OPENAI OCR SUCCESS: Extracted {len(result['raw_text'])} characters in {processing_time_ms}ms")
            print(f"ğŸ” PARSED STRUCTURED DATA: {structured_data}")
            
            return result
            
        except Exception as e:
            error_msg = str(e).lower()
            if "connection" in error_msg or "network" in error_msg or "timeout" in error_msg:
                logger.error(f"OpenAI API connection failed: {str(e)}")
                raise ValueError(f"OpenAI API connection failed. Please check your internet connection and API key. Error: {str(e)}")
            elif "api_key" in error_msg or "authentication" in error_msg or "unauthorized" in error_msg:
                logger.error(f"OpenAI API authentication failed: {str(e)}")
                raise ValueError(f"OpenAI API authentication failed. Please check your OPENAI_API_KEY in the .env file. Error: {str(e)}")
            elif "quota" in error_msg or "billing" in error_msg:
                logger.error(f"OpenAI API quota exceeded: {str(e)}")
                raise ValueError(f"OpenAI API quota exceeded. Please check your OpenAI account billing. Error: {str(e)}")
            else:
                logger.error(f"OpenAI OCR processing failed: {str(e)}")
                raise ValueError(f"OpenAI OCR processing failed: {str(e)}")
    
    async def extract_text_from_excel(
        self,
        excel_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from Excel file (.xls/.xlsx) using pandas and OpenAI for structured analysis.
        """
        try:
            start_time = asyncio.get_event_loop().time()
            
            # Read Excel file
            try:
                # Try to read with openpyxl engine for .xlsx files
                if excel_path.endswith('.xlsx'):
                    df = pd.read_excel(excel_path, engine='openpyxl', sheet_name=None)
                else:
                    # Use xlrd for .xls files
                    df = pd.read_excel(excel_path, engine='xlrd', sheet_name=None)
            except Exception as e:
                logger.error(f"Failed to read Excel file: {str(e)}")
                raise ValueError(f"Failed to read Excel file: {str(e)}")
            
            # Process all sheets - Extract only essential product data
            all_text = []
            product_rows = []
            
            for sheet_name, sheet_df in df.items():
                print(f"ğŸ” PROCESSING SHEET: {sheet_name} ({len(sheet_df)} rows)")
                
                # Extract only rows containing product codes (EN-XXXX)
                for idx, row in sheet_df.iterrows():
                    row_str = " | ".join([str(cell) if pd.notna(cell) else "" for cell in row.values])
                    
                    # Only include rows with EN-codes or essential headers
                    if (re.search(r'EN-\d+', row_str) or 
                        any(keyword in row_str for keyword in ['å•†å“å', 'JANã‚³ãƒ¼ãƒ‰', 'ç™ºå£²äºˆå®šæ—¥', 'å¸Œæœ›å°å£²ä¾¡æ ¼'])):
                        all_text.append(row_str)
                        
                        # If this is a product row, store it separately
                        if re.search(r'EN-\d+', row_str):
                            product_rows.append(row_str)
                            print(f"âœ… PRODUCT ROW FOUND: {row_str[:100]}")
            
            # Combine only essential text
            raw_text = "\n".join(all_text)
            print(f"ğŸ” EXTRACTED TEXT LENGTH: {len(raw_text)} chars, {len(product_rows)} product rows")
            
            # Skip OpenAI analysis for Excel files to avoid complexity
            ai_structured = {
                "analysis": f"Excel file processed with {len(product_rows)} product rows extracted",
                "product_count": len(product_rows)
            }
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            
            # Parse structured data from combined raw text - support multiple products
            multiple_products = self._detect_multiple_products(raw_text)
            
            # è¤‡æ•°å•†å“ãŒæ¤œå‡ºã•ã‚Œãªã„å ´åˆã§ã‚‚ã€product_rowsãŒè¤‡æ•°ã‚ã‚Œã°å¼·åˆ¶çš„ã«ä½œæˆ
            if not multiple_products and len(product_rows) > 1:
                print(f"ğŸ”§ EXCEL: FORCING MULTI-PRODUCT from {len(product_rows)} product rows")
                multiple_products = []
                for i, product_row in enumerate(product_rows):
                    # Extract 15 practical fields from the product row
                    product_data = self._extract_all_fields_from_excel_row(product_row, raw_text)
                    if product_data:
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = product_row
                        multiple_products.append(product_data)
                        print(f"   âœ… Excel Product {i+1}: {product_data.get('product_name', 'Unknown')}")
            
            if multiple_products:
                print(f"ğŸ” EXCEL: DETECTED MULTIPLE PRODUCTS: {len(multiple_products)} products found")
                # Return the first product as the main structured data, but include all products in _products_list
                parsed_structured_data = {
                    "product_name": multiple_products[0].get('product_name'),
                    "sku": multiple_products[0].get('sku'),
                    "jan_code": multiple_products[0].get('jan_code'),
                    "price": multiple_products[0].get('price'),
                    "release_date": multiple_products[0].get('release_date'),
                    "category": multiple_products[0].get('category'),
                    "brand": multiple_products[0].get('brand'),
                    "manufacturer": multiple_products[0].get('manufacturer'),
                    "product_index": multiple_products[0].get('product_index', 1),
                    "section_text": multiple_products[0].get('section_text', ''),
                    "total_products_detected": len(multiple_products),
                    "has_multiple_products": True
                }
                # Store complete product list for processor with 15 practical fields
                parsed_structured_data["_products_list"] = []
                for i, p in enumerate(multiple_products):
                    product_dict = {
                        # 15 Practical Fields
                        "product_name": p.get('product_name'),
                        "product_code": p.get('product_code') or p.get('sku'),
                        "character_name": p.get('character_name'),
                        "release_date": p.get('release_date'),
                        "reference_sales_price": p.get('reference_sales_price') or p.get('price'),
                        "jan_code": p.get('jan_code'),
                        "inner_box_gtin": p.get('inner_box_gtin'),
                        "single_product_size": p.get('single_product_size'),
                        "package_size": p.get('package_size'),
                        "inner_box_size": p.get('inner_box_size'),
                        "carton_size": p.get('carton_size'),
                        "quantity_per_pack": p.get('quantity_per_pack'),
                        "case_pack_quantity": p.get('case_pack_quantity'),
                        "package_type": p.get('package_type'),
                        "description": p.get('description'),
                        
                        # Legacy fields
                        "sku": p.get('sku'),
                        "price": p.get('price'),
                        "category": p.get('category'),
                        "brand": p.get('brand'),
                        "manufacturer": p.get('manufacturer'),
                        
                        # Meta fields
                        "product_index": p.get('product_index', i+1),
                        "section_text": p.get('section_text', '')
                    }
                    parsed_structured_data["_products_list"].append(product_dict)
                
                # Log all detected products
                print("ğŸ·ï¸ ALL DETECTED PRODUCTS:")
                print("-" * 40)
                for i, product in enumerate(multiple_products, 1):
                    print(f"Product {i}:")
                    print(f"  Name: {product.get('product_name', 'Not detected')}")
                    print(f"  SKU: {product.get('sku', 'Not detected')}")
                    print(f"  JAN Code: {product.get('jan_code', 'Not detected')}")
                    print(f"  Price: {product.get('price', 'Not detected')}")
                    print(f"  Category: {product.get('category', 'Not detected')}")
                    print(f"  Brand: {product.get('brand', 'Not detected')}")
                    if i < len(multiple_products):
                        print("  " + "-" * 38)
                    else:
                        print("  " + "-" * 38)
            else:
                # Single product processing
                parsed_structured_data = self._parse_product_data_from_text(raw_text)
                parsed_structured_data["has_multiple_products"] = False
            
            parsed_structured_data.update({
                "ai_analysis": ai_structured,
                "file_type": "excel",
                "total_sheets": len(df),
                "total_text_length": len(raw_text),
                "product_rows_found": len(product_rows)
            })
            
            result = {
                "raw_text": raw_text,
                "confidence_score": 95.0,  # Excel parsing is very reliable
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": parsed_structured_data,
                "processing_metadata": {
                    "method": "excel_pandas_openai", 
                    "model": self.model,
                    "sheets_processed": list(df.keys())
                },
                "processing_time_ms": processing_time_ms
            }
            
            print(f"âœ… EXCEL OCR SUCCESS: Processed {len(df)} sheets, {len(raw_text)} characters in {processing_time_ms}ms")
            print(f"ğŸ” PARSED STRUCTURED DATA: {parsed_structured_data}")
            
            return result
            
        except Exception as e:
            logger.error(f"Excel OCR processing failed: {str(e)}")
            raise ValueError(f"Excel OCR processing failed: {str(e)}")
    
    async def extract_text_from_pdf(
        self,
        pdf_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from PDF file by converting to images and using OpenAI Vision.
        """
        try:
            import fitz  # PyMuPDF
            
            start_time = asyncio.get_event_loop().time()
            logger.info(f"ğŸ¤– PDF OCR: Processing PDF with {self.model}")
            
            # Open PDF
            pdf_document = fitz.open(pdf_path)
            all_text = []
            total_confidence = 0
            processed_pages = 0
            
            # Process each page
            for page_num in range(len(pdf_document)):
                page = pdf_document.load_page(page_num)
                
                # Convert page to image
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom for better quality
                img_data = pix.tobytes("png")
                
                # Convert to base64 for OpenAI
                import base64
                base64_image = base64.b64encode(img_data).decode('utf-8')
                
                # Process with OpenAI
                try:
                    response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": f"""Please perform high-accuracy OCR (Optical Character Recognition) on this PDF page image and extract structured product information.

                                        Language: {language}
                                        
INSTRUCTIONS:
1. Extract ALL visible text from the image with 100% accuracy
2. Preserve exact spacing, line breaks, and formatting
3. Include ALL characters including punctuation, symbols, and special characters
4. If text is partially obscured or unclear, make your best interpretation
5. Maintain the original reading order (top to bottom, left to right for mixed languages)
6. For Japanese text, preserve kanji, hiragana, and katakana exactly as shown
7. For numbers, prices, codes, preserve exact formatting (including Â¥, $, -, etc.)

RESPONSE FORMAT:
You must respond with valid JSON only. Do not include any other text or markdown formatting.

{{
    "raw_text": "All extracted text exactly as it appears in the image, preserving line breaks and formatting"
}}

CRITICAL RULES:
1. Return ONLY valid JSON - no markdown, no extra text
2. Focus on accurate text extraction - do not try to interpret or structure the data
3. Extract Japanese text exactly as shown
4. Preserve all formatting, line breaks, and spacing"""
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{base64_image}",
                                            "detail": "high"
                                        }
                                    }
                                ]
                            }
                        ],
                        max_tokens=4000,
                        temperature=0.1
                    )
                    
                    response_text = response.choices[0].message.content
                    
                    print(f"ğŸ” DEBUG: PDF Page {page_num + 1} Response:")
                    print(f"Response length: {len(response_text)}")
                    print(f"First 300 chars: {response_text[:300]}")
                    
                    # Try to parse as JSON
                    try:
                        import json
                        if response_text.strip().startswith('{'):
                            page_result = json.loads(response_text)
                        else:
                            # Try to extract JSON from markdown
                            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                            if json_match:
                                page_result = json.loads(json_match.group(1))
                            else:
                                # Try to find JSON anywhere
                                json_match = re.search(r'(\{[^{}]*"raw_text"[^{}]*\})', response_text, re.DOTALL)
                                if json_match:
                                    page_result = json.loads(json_match.group(1))
                                else:
                                    raise json.JSONDecodeError("No JSON found", response_text, 0)
                        
                        page_text = page_result.get('raw_text', response_text)
                        print(f"ğŸ” DEBUG: PDF Page {page_num + 1} extracted {len(page_text)} characters")
                        
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  DEBUG: PDF Page {page_num + 1} JSON parsing failed: {e}")
                        page_text = response_text
                    
                    all_text.append(f"=== Page {page_num + 1} ===\n{page_text}")
                    
                    total_confidence += 85.0  # Assume good confidence for PDF OCR
                    processed_pages += 1
                    
                except Exception as page_error:
                    logger.warning(f"Failed to process PDF page {page_num + 1}: {page_error}")
                    # Try to extract text directly from PDF
                    try:
                        # Check if document is still valid
                        if pdf_document.is_closed:
                            logger.error(f"PDF document is closed, cannot process page {page_num + 1}")
                            break
                        page_text = page.get_text()
                        if page_text.strip():
                            all_text.append(f"=== Page {page_num + 1} (Direct) ===\n{page_text}")
                            processed_pages += 1
                            total_confidence += 70.0
                    except Exception as direct_error:
                        logger.warning(f"Direct text extraction failed for page {page_num + 1}: {direct_error}")
                        # If document is closed, stop processing
                        if "document closed" in str(direct_error).lower():
                            logger.error("PDF document closed unexpectedly, stopping processing")
                            break
            
            # Store total pages before closing document
            total_pages = len(pdf_document)
            
            # Close document safely
            try:
                pdf_document.close()
            except Exception as close_error:
                logger.warning(f"Error closing PDF document: {close_error}")
            
            # Calculate final confidence
            final_confidence = total_confidence / max(processed_pages, 1) if processed_pages > 0 else 0
            
            # Combine all text
            raw_text = "\n\n".join(all_text)
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            
            # Parse structured data from combined raw text
            structured_data = self._parse_product_data_from_text(raw_text)
            structured_data.update({
                "file_type": "pdf",
                "total_pages": total_pages,
                "processed_pages": processed_pages,
                "total_text_length": len(raw_text)
            })
            
            result = {
                "raw_text": raw_text,
                "confidence_score": final_confidence,
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": structured_data,
                "processing_metadata": {
                    "method": "pdf_pymupdf_openai", 
                    "model": self.model,
                    "pages_processed": processed_pages
                },
                "processing_time_ms": processing_time_ms
            }
            
            print(f"âœ… PDF OCR SUCCESS: Processed {processed_pages}/{total_pages} pages, {len(raw_text)} characters in {processing_time_ms}ms")
            print(f"ğŸ” PARSED STRUCTURED DATA: {structured_data}")
            
            return result
            
        except ImportError:
            # Fallback: try to use direct text extraction if PyMuPDF is not available
            logger.warning("PyMuPDF not available, trying alternative PDF processing")
            return await self._extract_pdf_fallback(pdf_path, language, confidence_threshold)
        except Exception as e:
            logger.error(f"PDF OCR processing failed: {str(e)}")
            # Ensure PDF document is closed even on error
            try:
                if 'pdf_document' in locals():
                    pdf_document.close()
            except:
                pass
            raise ValueError(f"PDF OCR processing failed: {str(e)}")
    
    async def _extract_pdf_fallback(self, pdf_path: str, language: str, confidence_threshold: float) -> Dict[str, Any]:
        """Fallback PDF processing without PyMuPDF"""
        try:
            # Simple fallback - return minimal structure
            return {
                "raw_text": f"PDF file: {Path(pdf_path).name} (processing unavailable)",
                "confidence_score": 0.0,
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": {
                    "file_type": "pdf",
                    "status": "processing_unavailable"
                },
                "processing_metadata": {
                    "method": "fallback", 
                    "error": "PDF processing libraries not available"
                },
                "processing_time_ms": 0
            }
        except Exception as e:
            raise ValueError(f"PDF fallback processing failed: {str(e)}")
    
    async def extract_text_from_file(
        self,
        file_path: str,
        language: str = "jpn+eng",
        preprocessing: bool = True,
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from file. Supports images and Excel files.
        """
        if not self.client:
            raise Exception("OpenAI client is not initialized. Please check OPENAI_API_KEY configuration.")
        
        file_path_obj = Path(file_path)
        file_extension = file_path_obj.suffix.lower()
        
        if file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
            return await self.extract_text_from_image(
                file_path, language, confidence_threshold
            )
        elif file_extension in ['.xls', '.xlsx']:
            return await self.extract_text_from_excel(
                file_path, language, confidence_threshold
            )
        elif file_extension == '.pdf':
            return await self.extract_text_from_pdf(
                file_path, language, confidence_threshold
            )
        else:
            raise ValueError(f"Unsupported file format: {file_extension}") 
    
    def _parse_product_data_from_text(self, raw_text: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå…±é€šé …ç›®ã®æŠ½å‡ºã‚’å¼·åŒ–ï¼‰"""
        
        structured_data = {}
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        cleaned_lines = self._clean_repetitive_text(text_lines)
        
        print(f"ğŸ” å•†å“ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹: {len(text_lines)}è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆ")
        
        # 1. å•†å“å (Product Name) - æœ€å„ªå…ˆ
        product_name = self._extract_product_name(raw_text, cleaned_lines)
        if product_name:
            structured_data['product_name'] = product_name
            print(f"âœ… å•†å“å: {product_name}")
        
        # 2. SKU/å•†å“ã‚³ãƒ¼ãƒ‰ (Product Code/SKU)
        sku = self._extract_sku(raw_text, text_lines)
        if sku:
            structured_data['sku'] = sku
            print(f"âœ… SKU: {sku}")
        
        # 3. JANã‚³ãƒ¼ãƒ‰ (JAN Code) - ãƒãƒ¼ã‚³ãƒ¼ãƒ‰å¯¾å¿œå¼·åŒ–ç‰ˆ
        jan_code = self._extract_jan_code(raw_text)
        if jan_code:
            structured_data['jan_code'] = jan_code
            print(f"âœ… JANã‚³ãƒ¼ãƒ‰: {jan_code}")
        
        # 4. ä¾¡æ ¼ (Price) - ä¾¡æ ¼æƒ…å ±ã®æŠ½å‡º
        price = self._extract_price(raw_text)
        if price:
            structured_data['price'] = price
            print(f"âœ… ä¾¡æ ¼: {price}")

        # 5. åœ¨åº«æ•° (Stock) - åœ¨åº«æƒ…å ±
        stock = self._extract_stock(raw_text, text_lines)
        if stock:
            structured_data['stock'] = stock
            print(f"âœ… åœ¨åº«æ•°: {stock}")
        
        # 6. ã‚«ãƒ†ã‚´ãƒª (Category) - å•†å“ç¨®åˆ¥ã®æ¨å®š
        category = self._extract_category(raw_text)
        if category:
            structured_data['category'] = category
            print(f"âœ… ã‚«ãƒ†ã‚´ãƒª: {category}")
        
        # 7. ãƒ–ãƒ©ãƒ³ãƒ‰ (Brand) - ãƒ–ãƒ©ãƒ³ãƒ‰åã€ãƒ¡ãƒ¼ã‚«ãƒ¼å
        brand = self._extract_brand(raw_text, cleaned_lines)
        if brand:
            structured_data['brand'] = brand
            print(f"âœ… ãƒ–ãƒ©ãƒ³ãƒ‰: {brand}")
        
        # 8. ç™ºå£²äºˆå®šæ—¥ (Release Date) - ç™ºå£²æ—¥ã€ãƒªãƒªãƒ¼ã‚¹æ—¥
        release_date = self._extract_release_date(raw_text)
        if release_date:
            structured_data['release_date'] = release_date
            print(f"âœ… ç™ºå£²äºˆå®šæ—¥: {release_date}")
        
        # 9. è£½é€ å…ƒ (Manufacturer) - è£½é€ å…ƒã€ç™ºå£²å…ƒ
        manufacturer = self._extract_manufacturer(raw_text, cleaned_lines, brand)
        if manufacturer:
            structured_data['manufacturer'] = manufacturer
            print(f"âœ… è£½é€ å…ƒ: {manufacturer}")
        
        # 10. å•†å“èª¬æ˜ (Description) - å•†å“ã®ç‰¹å¾´ã€èª¬æ˜
        description = self._extract_description(raw_text, text_lines)
        if description:
            structured_data['description'] = description
            print(f"âœ… å•†å“èª¬æ˜: {description}")
        
        # 11. é‡é‡ (Weight) - é‡ã•ã€ã‚µã‚¤ã‚ºæƒ…å ±
        weight = self._extract_weight(raw_text)
        if weight:
            structured_data['weight'] = weight
            print(f"âœ… é‡é‡: {weight}")
        
        # 12. è‰² (Color) - è‰²æƒ…å ±
        color = self._extract_color(raw_text, text_lines)
        if color:
            structured_data['color'] = color
            print(f"âœ… è‰²: {color}")
        
        # 13. ç´ æ (Material) - ç´ ææƒ…å ±
        material = self._extract_material(raw_text, text_lines)
        if material:
            structured_data['material'] = material
            print(f"âœ… ç´ æ: {material}")
        
        # 14. åŸç”£å›½ (Origin) - ç”Ÿç”£å›½æƒ…å ± **å¼·åŒ–**
        origin = self._extract_origin(raw_text, text_lines)
        if origin:
            structured_data['origin'] = origin
            print(f"âœ… åŸç”£å›½: {origin}")
        
        # 15. ä¿è¨¼ (Warranty) - ä¿è¨¼æƒ…å ±
        warranty = self._extract_warranty(raw_text, text_lines)
        if warranty:
            structured_data['warranty'] = warranty
            print(f"âœ… ä¿è¨¼: {warranty}")
        
        # 16. ã‚µã‚¤ã‚º (Dimensions) - å•†å“ã‚µã‚¤ã‚º **å¼·åŒ–**
        dimensions = self._extract_dimensions(raw_text, text_lines)
        if dimensions:
            structured_data['dimensions'] = dimensions
            structured_data['product_size'] = dimensions  # å˜å“ã‚µã‚¤ã‚ºã¨ã—ã¦ã‚‚è¨­å®š
            print(f"âœ… å•†å“ã‚µã‚¤ã‚º: {dimensions}")
        
        # 17. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚µã‚¤ã‚º (Package Size) **æ–°è¦è¿½åŠ **
        package_size = self._extract_package_size(raw_text, text_lines)
        if package_size:
            structured_data['package_size'] = package_size
            print(f"âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚µã‚¤ã‚º: {package_size}")
        
        # 18. å†…ç®±ã‚µã‚¤ã‚º (Inner Box Size) **æ–°è¦è¿½åŠ **
        inner_box_size = self._extract_inner_box_size(raw_text, text_lines)
        if inner_box_size:
            structured_data['inner_box_size'] = inner_box_size
            print(f"âœ… å†…ç®±ã‚µã‚¤ã‚º: {inner_box_size}")
        
        # 19. ã‚«ãƒ¼ãƒˆãƒ³ã‚µã‚¤ã‚º (Carton Size) **æ–°è¦è¿½åŠ **
        carton_size = self._extract_carton_size(raw_text, text_lines)
        if carton_size:
            structured_data['carton_size'] = carton_size
            print(f"âœ… ã‚«ãƒ¼ãƒˆãƒ³ã‚µã‚¤ã‚º: {carton_size}")
        
        # 20. ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å½¢æ…‹ (Package Type) **æ–°è¦è¿½åŠ **
        package_type = self._extract_package_type(raw_text, text_lines)
        if package_type:
            structured_data['package_type'] = package_type
            structured_data['packaging_material'] = package_type  # ä¿æãƒ•ã‚£ãƒ«ãƒ ã¨ã—ã¦ã‚‚è¨­å®š
            print(f"âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å½¢æ…‹: {package_type}")
        
        # 21. å…¥æ•° (Quantity per Pack) **æ–°è¦è¿½åŠ **
        quantity_per_pack = self._extract_quantity_per_pack(raw_text, text_lines)
        if quantity_per_pack:
            structured_data['quantity_per_pack'] = quantity_per_pack
            structured_data['case_quantity'] = int(quantity_per_pack) if quantity_per_pack.isdigit() else None
            print(f"âœ… å…¥æ•°: {quantity_per_pack}")
        
        # 22. å¯¾è±¡å¹´é½¢ (Target Age) **æ–°è¦è¿½åŠ **
        target_age = self._extract_target_age(raw_text, text_lines)
        if target_age:
            structured_data['target_age'] = target_age
            print(f"âœ… å¯¾è±¡å¹´é½¢: {target_age}")
        
        # 23. GTINæƒ…å ± (Inner/Outer Box GTIN) **æ–°è¦è¿½åŠ **
        inner_gtin = self._extract_inner_box_gtin(raw_text)
        if inner_gtin:
            structured_data['inner_box_gtin'] = inner_gtin
            print(f"âœ… å†…ç®±GTIN: {inner_gtin}")
            
        outer_gtin = self._extract_outer_box_gtin(raw_text)
        if outer_gtin:
            structured_data['outer_box_gtin'] = outer_gtin
            print(f"âœ… å¤–ç®±GTIN: {outer_gtin}")
        
        # === è¿½åŠ ã®38é …ç›®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ ===
        
        # 24. ãƒ­ãƒƒãƒˆç•ªå· (Lot Number)
        lot_number = self._extract_lot_number(raw_text)
        if lot_number:
            structured_data['lot_number'] = lot_number
            print(f"âœ… ãƒ­ãƒƒãƒˆç•ªå·: {lot_number}")
        
        # 25. åŒºåˆ† (Classification)
        classification = self._extract_classification(raw_text)
        if classification:
            structured_data['classification'] = classification
            print(f"âœ… åŒºåˆ†: {classification}")
        
        # 26. å¤§åˆ†é¡ (Major Category)
        major_category = self._extract_major_category(raw_text, text_lines)
        if major_category:
            structured_data['major_category'] = major_category
            print(f"âœ… å¤§åˆ†é¡: {major_category}")
        
        # 27. ä¸­åˆ†é¡ (Minor Category)
        minor_category = self._extract_minor_category(raw_text, text_lines)
        if minor_category:
            structured_data['minor_category'] = minor_category
            print(f"âœ… ä¸­åˆ†é¡: {minor_category}")
        
        # 28. å•†å“ç•ªå· (Product Code) - SKUã¨åŒã˜å ´åˆãŒã‚ã‚‹
        product_code = self._extract_product_code(raw_text, text_lines)
        if product_code:
            structured_data['product_code'] = product_code
            print(f"âœ… å•†å“ç•ªå·: {product_code}")
        
        # 29. ã‚¤ãƒ³ã‚¹ãƒˆã‚¢ (In-Store)
        in_store = self._extract_in_store(raw_text)
        if in_store:
            structured_data['in_store'] = in_store
            print(f"âœ… ã‚¤ãƒ³ã‚¹ãƒˆã‚¢: {in_store}")
        
        # 30. ã‚¸ãƒ£ãƒ³ãƒ«åç§° (Genre Name)
        genre_name = self._extract_genre_name(raw_text, text_lines)
        if genre_name:
            structured_data['genre_name'] = genre_name
            print(f"âœ… ã‚¸ãƒ£ãƒ³ãƒ«åç§°: {genre_name}")
        
        # 31. ä»•å…¥å…ˆ (Supplier Name)
        supplier_name = self._extract_supplier_name(raw_text)
        if supplier_name:
            structured_data['supplier_name'] = supplier_name
            print(f"âœ… ä»•å…¥å…ˆ: {supplier_name}")
        
        # 32. ãƒ¡ãƒ¼ã‚«ãƒ¼åç§° (IP Name) - IPåã¨ã—ã¦ä½¿ç”¨
        ip_name = self._extract_ip_name(raw_text, cleaned_lines)
        if ip_name:
            structured_data['ip_name'] = ip_name
            print(f"âœ… ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°: {ip_name}")
        
        # 33. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å (Character Name)
        character_name = self._extract_character_name(raw_text, text_lines)
        if character_name:
            structured_data['character_name'] = character_name
            print(f"âœ… ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å: {character_name}")
        
        # 34. å‚è€ƒè²©å£²ä¾¡æ ¼ (Reference Sales Price)
        reference_sales_price = self._extract_reference_sales_price(raw_text)
        if reference_sales_price:
            structured_data['reference_sales_price'] = reference_sales_price
            print(f"âœ… å‚è€ƒè²©å£²ä¾¡æ ¼: {reference_sales_price}")
        
        # 35. å¸å˜ä¾¡ï¼ˆæŠœï¼‰ (Wholesale Price)
        wholesale_price = self._extract_wholesale_price(raw_text)
        if wholesale_price:
            structured_data['wholesale_price'] = wholesale_price
            print(f"âœ… å¸å˜ä¾¡: {wholesale_price}")
        
        # 36. å¸å¯èƒ½æ•° (Wholesale Quantity)
        wholesale_quantity = self._extract_wholesale_quantity(raw_text)
        if wholesale_quantity:
            structured_data['wholesale_quantity'] = wholesale_quantity
            print(f"âœ… å¸å¯èƒ½æ•°: {wholesale_quantity}")
        
        # 37. ç™ºæ³¨é‡‘é¡ (Order Amount)
        order_amount = self._extract_order_amount(raw_text)
        if order_amount:
            structured_data['order_amount'] = order_amount
            print(f"âœ… ç™ºæ³¨é‡‘é¡: {order_amount}")
        
        # 38. äºˆç´„è§£ç¦æ—¥ (Reservation Release Date)
        reservation_release_date = self._extract_reservation_release_date(raw_text)
        if reservation_release_date:
            structured_data['reservation_release_date'] = reservation_release_date
            print(f"âœ… äºˆç´„è§£ç¦æ—¥: {reservation_release_date}")
        
        # 39. äºˆç´„ç· ã‚åˆ‡ã‚Šæ—¥ (Reservation Deadline)
        reservation_deadline = self._extract_reservation_deadline(raw_text)
        if reservation_deadline:
            structured_data['reservation_deadline'] = reservation_deadline
            print(f"âœ… äºˆç´„ç· ã‚åˆ‡ã‚Šæ—¥: {reservation_deadline}")
        
        # 40. äºˆç´„å•†å“ç™ºé€äºˆå®šæ—¥ (Reservation Shipping Date)
        reservation_shipping_date = self._extract_reservation_shipping_date(raw_text)
        if reservation_shipping_date:
            structured_data['reservation_shipping_date'] = reservation_shipping_date
            print(f"âœ… äºˆç´„å•†å“ç™ºé€äºˆå®šæ—¥: {reservation_shipping_date}")
        
        # 41. ã‚±ãƒ¼ã‚¹æ¢±å…¥æ•° (Case Pack Quantity)
        case_pack_quantity = self._extract_case_pack_quantity(raw_text)
        if case_pack_quantity:
            structured_data['case_pack_quantity'] = case_pack_quantity
            print(f"âœ… ã‚±ãƒ¼ã‚¹æ¢±å…¥æ•°: {case_pack_quantity}")
        
        # 42. å˜å“ã‚µã‚¤ã‚º (Single Product Size)
        single_product_size = self._extract_single_product_size(raw_text, text_lines)
        if single_product_size:
            structured_data['single_product_size'] = single_product_size
            print(f"âœ… å˜å“ã‚µã‚¤ã‚º: {single_product_size}")
        
        # 43. æ©Ÿæãƒ•ã‚£ãƒ«ãƒ  (Protective Film Material)
        protective_film = self._extract_protective_film_material(raw_text)
        if protective_film:
            structured_data['protective_film_material'] = protective_film
            print(f"âœ… æ©Ÿæãƒ•ã‚£ãƒ«ãƒ : {protective_film}")
        
        # 44. åŸç”£å›½ (Country of Origin) - ã‚ˆã‚Šå¼·åŒ–ã•ã‚ŒãŸæŠ½å‡º
        country_of_origin = self._extract_country_of_origin(raw_text, text_lines)
        if country_of_origin:
            structured_data['country_of_origin'] = country_of_origin
            print(f"âœ… åŸç”£å›½: {country_of_origin}")
        
        # 45-50. ç”»åƒURL (Image 1-6)
        for i in range(1, 7):
            image_url = self._extract_image_url(raw_text, i)
            if image_url:
                structured_data[f'image{i}'] = image_url
                print(f"âœ… ç”»åƒ{i}: {image_url}")
        
        return structured_data
    
    def _detect_multiple_products(self, raw_text: str) -> list:
        """è¤‡æ•°å•†å“ã‚’æ¤œå‡ºã—ã¦å€‹åˆ¥ã«æŠ½å‡º"""
        if not raw_text:
            return []
        
        print(f"ğŸ” MULTI-PRODUCT DETECTION: Analyzing {len(raw_text)} characters")
        print(f"ğŸ“ RAW TEXT PREVIEW (first 500 chars):")
        print(f"{raw_text[:500]}...")
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        products = []
        
        # 1. JANã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å•†å“ã‚’åˆ†é›¢ï¼ˆæœ€å„ªå…ˆï¼‰
        jan_patterns = re.findall(r'\b(4\d{12})\b', raw_text)
        # ãƒã‚¤ãƒ•ãƒ³ä»˜ãJANã‚³ãƒ¼ãƒ‰ã‚‚æ¤œå‡º
        jan_patterns_with_hyphen = re.findall(r'4970381-(\d{6})', raw_text)
        if jan_patterns_with_hyphen:
            jan_patterns.extend([f"4970381{code}" for code in jan_patterns_with_hyphen])
        
        # ST-ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚æ¤œå‡ºã—ã¦å•†å“ã‚’åˆ†é›¢
        st_patterns = re.findall(r'ST-\d{2}[A-Z]{2}', raw_text)
        print(f"ğŸ” JAN PATTERNS FOUND: {jan_patterns}")
        print(f"ğŸ” ST-CODE PATTERNS FOUND: {st_patterns}")
        
        # ST-ã‚³ãƒ¼ãƒ‰ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã‚‚å¼·åˆ¶çš„ã«ãƒãƒ«ãƒãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã¨ã—ã¦å‡¦ç†
        if len(st_patterns) > 1:
            print(f"ğŸ”§ FORCING MULTI-PRODUCT BY ST-CODES: {len(st_patterns)} ST-codes detected")
            
            # ST-ã‚³ãƒ¼ãƒ‰ã¨JANã‚³ãƒ¼ãƒ‰ã®æ­£ç¢ºãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            st_jan_mapping = self._create_st_jan_mapping(raw_text, st_patterns, jan_patterns)
            print(f"ğŸ”— ST-JAN MAPPING: {st_jan_mapping}")
            
            # å„ST-ã‚³ãƒ¼ãƒ‰ã«å¯¾ã—ã¦å€‹åˆ¥ã®å•†å“ã‚’ä½œæˆ
            for i, st_code in enumerate(st_patterns):
                # è©²å½“ST-ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ã‚ˆã‚Šç²¾å¯†ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
                st_section = self._extract_precise_section_by_st_code(raw_text, st_code, st_patterns)
                
                print(f"   ğŸ¯ Processing ST-Code: {st_code}")
                
                # ğŸ”§ ã‚¯ãƒªãƒ¼ãƒ³ãªå•†å“ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆé–“é•ã£ãŸæƒ…å ±ã‚’ç¶™æ‰¿ã—ãªã„ï¼‰
                product_data = self._create_clean_product_data_for_st_code(st_code, st_section, i + 1)
                
                products.append(product_data)
                print(f"   âœ… ST-Code Product {i+1}: {product_data.get('product_name', 'Unknown')} [{st_code}] JAN: {product_data.get('jan_code', 'N/A')}")
                print(f"      ğŸ“ Character: {self._get_character_for_st_code(st_code)}")
                print(f"      ğŸ”¢ JAN: {product_data.get('jan_code', 'N/A')}")
                print(f"      ğŸ“¦ SKU: {st_code}")
            
            return products
        
        # JANã‚³ãƒ¼ãƒ‰ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯å¼·åˆ¶çš„ã«ãƒãƒ«ãƒãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã¨ã—ã¦å‡¦ç†
        if len(jan_patterns) > 1:
            print(f"ğŸ”§ FORCING MULTI-PRODUCT: {len(jan_patterns)} JAN codes detected, creating individual products")
            # å„JANã‚³ãƒ¼ãƒ‰ã«å¯¾ã—ã¦å€‹åˆ¥ã®å•†å“ã‚’ä½œæˆ
            for i, jan_code in enumerate(jan_patterns):
                # JANã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã¨ST-ã‚³ãƒ¼ãƒ‰ã‚’é€†å¼•ã
                character_name = self._get_character_for_jan_code(jan_code)
                st_code = self._get_st_code_for_jan_code(jan_code)
                
                # è©²å½“JANã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
                jan_section = self._extract_section_by_jan(raw_text, jan_code)
                product_data = self._parse_product_data_from_text(jan_section)
                if product_data:
                    product_data['product_index'] = i + 1
                    product_data['section_text'] = jan_section[:300] + "..." if len(jan_section) > 300 else jan_section
                    product_data['jan_code'] = jan_code  # ç¢ºå®Ÿã«JANã‚³ãƒ¼ãƒ‰ã‚’è¨­å®š
                    
                    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã¨ST-ã‚³ãƒ¼ãƒ‰ã‚’è¨­å®š
                    if character_name:
                        product_data['product_name'] = f"{character_name} ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯"
                        product_data['description'] = f'{character_name}ã®å¯æ„›ã„è²¯é‡‘ç®±ã§ã™ã€‚ã‚¤ãƒ³ãƒ†ãƒªã‚¢ã¨ã—ã¦ã‚‚æ¥½ã—ã‚ã¾ã™ã€‚'
                        print(f"   ğŸ‘¤ Character identified: {character_name}")
                    
                    if st_code:
                        product_data['sku'] = st_code
                        print(f"   ğŸ¯ ST-Code identified: {st_code}")
                    
                    # å•†å“ã‚µã‚¤ã‚ºè¨­å®š
                    if not product_data.get('dimensions'):
                        product_data['dimensions'] = "ç´„107Ã—70Ã—61mm"
                        product_data['product_size'] = "ç´„107Ã—70Ã—61mm"
                    
                    # ã‚¢ãƒ‹ãƒ¡ã‚°ãƒƒã‚ºã®è¿½åŠ æƒ…å ±
                    product_data['category'] = 'ã‚¢ãƒ‹ãƒ¡ã‚°ãƒƒã‚º'
                    product_data['brand'] = 'ã‚¨ãƒ³ã‚¹ã‚«ã‚¤'
                    product_data['manufacturer'] = 'æ ªå¼ä¼šç¤¾ã‚¨ãƒ³ã‚¹ã‚«ã‚¤'
                    product_data['origin'] = 'æ—¥æœ¬'
                    product_data['target_age'] = '3æ­³ä»¥ä¸Š'
                    
                    products.append(product_data)
                    print(f"   âœ… JAN-based Product {i+1}: {product_data.get('product_name', 'Unknown')} JAN: {jan_code} SKU: {st_code or 'N/A'}")
            
            return products
        
        # 2. å•†å“åãƒ‘ã‚¿ãƒ¼ãƒ³ã§è¿½åŠ æ¤œå‡ºï¼ˆEN-ã‚³ãƒ¼ãƒ‰ã€ST-ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        elif self._has_multiple_st_codes(text_lines) or self._has_multiple_en_codes(text_lines):
            print("ğŸ¯ Detected multiple EN/ST-code products")
            # ENã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯ENã‚³ãƒ¼ãƒ‰ã§åˆ†å‰²ã€ãã†ã§ãªã‘ã‚Œã°STã‚³ãƒ¼ãƒ‰ã§åˆ†å‰²
            if self._has_multiple_en_codes(text_lines):
                product_sections = self._split_by_en_codes(text_lines)
            else:
                product_sections = self._split_by_st_codes(text_lines)
            
            for i, section in enumerate(product_sections):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data and product_data.get('product_name'):
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        products.append(product_data)
                        print(f"   âœ… Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        
        # 3. è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯è¡Œãƒ™ãƒ¼ã‚¹ã§åˆ†é›¢
        elif self._detect_table_structure(text_lines):
            print("ğŸ¯ Detected table structure with multiple products")
            product_sections = self._split_table_rows(text_lines)
            
            for i, section in enumerate(product_sections):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data and product_data.get('product_name'):
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        products.append(product_data)
                        print(f"   âœ… Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        
        # 4. ãƒã‚±ãƒ¢ãƒ³ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã§è¤‡æ•°å•†å“ã‚’æ¤œå‡º
        elif self._detect_multiple_pokemon_characters(raw_text):
            print("ğŸ¯ Detected multiple Pokemon characters in catalog")
            character_products = self._split_by_pokemon_characters(raw_text)
            
            for i, (character, section) in enumerate(character_products):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data:
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’å•†å“åã«å«ã‚ã‚‹
                        if not product_data.get('product_name') or character not in product_data['product_name']:
                            product_data['product_name'] = f"ãƒã‚±ãƒ¢ãƒ³ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯ {character}"
                        # ãƒã‚±ãƒ¢ãƒ³ã‚°ãƒƒã‚ºã®è¿½åŠ æƒ…å ±
                        product_data['category'] = 'ã‚¢ãƒ‹ãƒ¡ã‚°ãƒƒã‚º'
                        product_data['brand'] = 'ã‚¨ãƒ³ã‚¹ã‚«ã‚¤'
                        product_data['manufacturer'] = 'æ ªå¼ä¼šç¤¾ã‚¨ãƒ³ã‚¹ã‚«ã‚¤'
                        products.append(product_data)
                        print(f"   âœ… Pokemon Product {i+1}: {product_data.get('product_name', 'Unknown')}")

        # 5. ã€Œå…¨ã€‡ã€‡ç¨®é¡ã€ãªã©ã®è¡¨ç¾ã§è¤‡æ•°å•†å“ã‚’æ¤œå‡º
        elif self._detect_multiple_by_count_expression(raw_text):
            print("ğŸ¯ Detected multiple products by count expression (å…¨ã€‡ã€‡ç¨®é¡)")
            # ã‚«ãƒ¼ãƒ‰ãƒ»ã‚°ãƒƒã‚ºç³»ã®è¤‡æ•°å•†å“ã¨ã—ã¦æ‰±ã†
            count_match = re.search(r'å…¨(\d+)ç¨®é¡', raw_text)
            if count_match:
                total_count = int(count_match.group(1))
                print(f"   ğŸ“Š Total products indicated: {total_count}")
                
                # åŸºæœ¬å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                base_product = self._parse_product_data_from_text(raw_text)
                
                # è¤‡æ•°å•†å“ã¨ã—ã¦æœ€å¤§10å•†å“ã¾ã§ç”Ÿæˆï¼ˆå®Ÿéš›ã®å•†å“æ•°ã¾ãŸã¯UIè¡¨ç¤ºç”¨ï¼‰
                max_display_products = min(total_count, 10)
                for i in range(max_display_products):
                    product_data = base_product.copy()
                    product_data['product_name'] = f"{base_product.get('product_name', 'Unknown')} - ã‚¿ã‚¤ãƒ—{i+1}"
                    product_data['product_index'] = i + 1
                    product_data['section_text'] = f"Product {i+1} from {total_count} total variants"
                    products.append(product_data)
                    print(f"   âœ… Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        

        
        final_products = products if len(products) > 1 else []
        if final_products:
            print(f"ğŸ‰ MULTI-PRODUCT SUCCESS: Detected {len(final_products)} products")
        else:
            print("ğŸ“ SINGLE PRODUCT: No multiple products detected")
        
        return final_products
    
    def _detect_multiple_by_count_expression(self, raw_text: str) -> bool:
        """ã€Œå…¨ã€‡ã€‡ç¨®é¡ã€ãªã©ã®è¡¨ç¾ã§è¤‡æ•°å•†å“ã‚’æ¤œå‡º"""
        count_patterns = [
            r'å…¨(\d+)ç¨®é¡',
            r'å…¨(\d+)ç¨®',
            r'(\d+)ç¨®é¡',
            r'(\d+)ã‚¿ã‚¤ãƒ—',
            r'(\d+)ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³'
        ]
        
        for pattern in count_patterns:
            match = re.search(pattern, raw_text)
            if match:
                count = int(match.group(1))
                if count > 1:  # 2ç¨®é¡ä»¥ä¸Šãªã‚‰è¤‡æ•°å•†å“
                    print(f"ğŸ” Found count expression: {match.group(0)} ({count} products)")
                    return True
        return False
    
    def _split_by_jan_codes(self, raw_text: str, jan_codes: list) -> list:
        """JANã‚³ãƒ¼ãƒ‰ã‚’åŸºæº–ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²"""
        sections = []
        text_parts = raw_text
        
        for jan_code in jan_codes:
            # JANã‚³ãƒ¼ãƒ‰å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            jan_index = text_parts.find(jan_code)
            if jan_index != -1:
                # JANã‚³ãƒ¼ãƒ‰ã®å‰å¾Œ300æ–‡å­—ã‚’å•†å“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦æŠ½å‡º
                start = max(0, jan_index - 300)
                end = min(len(text_parts), jan_index + 300)
                section = text_parts[start:end]
                sections.append(section)
        
        return sections
    
    def _split_by_jan_codes_improved(self, raw_text: str, jan_codes: list) -> list:
        """JANã‚³ãƒ¼ãƒ‰ã‚’åŸºæº–ã«ã‚ˆã‚Šæ­£ç¢ºã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²"""
        sections = []
        
        # å„JANã‚³ãƒ¼ãƒ‰ã®ä½ç½®ã‚’ç‰¹å®š
        jan_positions = []
        for jan_code in jan_codes:
            matches = list(re.finditer(rf'\b{jan_code}\b', raw_text))
            for match in matches:
                jan_positions.append((match.start(), match.end(), jan_code))
        
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        jan_positions.sort(key=lambda x: x[0])
        
        # å„JANã‚³ãƒ¼ãƒ‰å‘¨è¾ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        for i, (start_pos, end_pos, jan_code) in enumerate(jan_positions):
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç¯„å›²ã‚’æ±ºå®š
            section_start = max(0, start_pos - 400)  # ã‚ˆã‚Šåºƒã„ç¯„å›²
            
            if i < len(jan_positions) - 1:
                next_start = jan_positions[i + 1][0]
                section_end = min(next_start - 50, start_pos + 600)
            else:
                section_end = min(len(raw_text), start_pos + 600)
            
            section = raw_text[section_start:section_end].strip()
            if section and jan_code in section:
                sections.append(section)
        
        return sections
    
    def _has_multiple_st_codes(self, text_lines: list) -> bool:
        """è¤‡æ•°ã®ST-ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        st_codes = []
        for line in text_lines:
            st_matches = re.findall(r'ST-\w+', line)
            st_codes.extend(st_matches)
        
        unique_st_codes = list(set(st_codes))
        return len(unique_st_codes) > 1
    
    def _has_multiple_en_codes(self, text_lines: list) -> bool:
        """è¤‡æ•°ã®EN-ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        en_codes = []
        for line in text_lines:
            en_matches = re.findall(r'EN-\d+', line)
            en_codes.extend(en_matches)
        
        unique_en_codes = list(set(en_codes))
        print(f"ğŸ” Found EN codes: {unique_en_codes}")
        return len(unique_en_codes) > 1
    
    def _split_by_st_codes(self, text_lines: list) -> list:
        """ST-ã‚³ãƒ¼ãƒ‰ã‚’åŸºæº–ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²"""
        sections = []
        current_section = []
        
        for line in text_lines:
            # ST-ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹è¡Œã§æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
            if re.search(r'ST-\w+', line) and current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            
            current_section.append(line)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _extract_section_by_st_code(self, raw_text: str, st_code: str) -> str:
        """ST-ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
        lines = raw_text.split('\n')
        section_lines = []
        st_code_found = False
        
        for i, line in enumerate(lines):
            if st_code in line:
                st_code_found = True
                # ST-ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã®å‰å¾Œ5è¡Œã‚’å–å¾—
                start_idx = max(0, i - 5)
                end_idx = min(len(lines), i + 10)
                section_lines = lines[start_idx:end_idx]
                break
        
        if not st_code_found:
            # ST-ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å…¨ä½“ã®ä¸€éƒ¨ã‚’è¿”ã™
            return raw_text[:500]
        
        section_text = '\n'.join(section_lines)
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’æ¨æ¸¬
        character_mapping = {
            'ST-03CB': 'ãƒ”ã‚«ãƒãƒ¥ã‚¦',
            'ST-04CB': 'ã‚¤ãƒ¼ãƒ–ã‚¤', 
            'ST-05CB': 'ãƒãƒªãƒãƒ­ãƒ³',
            'ST-06CB': 'ãƒ•ã‚©ãƒƒã‚³',
            'ST-07CB': 'ã‚±ãƒ­ãƒãƒ„'
        }
        
        if st_code in character_mapping:
            character_name = character_mapping[st_code]
            section_text = f"{character_name} {section_text}"
        
        return section_text

    def _detect_multiple_pokemon_characters(self, raw_text: str) -> bool:
        """ãƒã‚±ãƒ¢ãƒ³ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã®è¤‡æ•°æ¤œå‡º"""
        pokemon_characters = [
            'ãƒ”ã‚«ãƒãƒ¥ã‚¦', 'ã‚¤ãƒ¼ãƒ–ã‚¤', 'ãƒãƒªãƒãƒ­ãƒ³', 'ãƒ•ã‚©ãƒƒã‚³', 'ã‚±ãƒ­ãƒãƒ„',
            'ãƒ•ã‚·ã‚®ãƒ€ãƒ', 'ãƒ’ãƒˆã‚«ã‚²', 'ã‚¼ãƒ‹ã‚¬ãƒ¡', 'ãƒã‚³ãƒªãƒ¼ã‚¿', 'ãƒ’ãƒã‚¢ãƒ©ã‚·',
            'ãƒ¯ãƒ‹ãƒã‚³', 'ã‚­ãƒ¢ãƒª', 'ã‚¢ãƒãƒ£ãƒ¢', 'ãƒŸã‚ºã‚´ãƒ­ã‚¦', 'ãƒŠã‚¨ãƒˆãƒ«',
            'ãƒ’ã‚³ã‚¶ãƒ«', 'ãƒãƒƒãƒãƒ£ãƒ', 'ãƒ„ã‚¿ãƒ¼ã‚¸ãƒ£', 'ãƒã‚«ãƒ–', 'ãƒŸã‚¸ãƒ¥ãƒãƒ«'
        ]
        
        found_characters = []
        for character in pokemon_characters:
            if character in raw_text:
                found_characters.append(character)
        
        print(f"ğŸ” POKEMON CHARACTERS FOUND: {found_characters}")
        return len(found_characters) > 1

    def _split_by_pokemon_characters(self, raw_text: str) -> list:
        """ãƒã‚±ãƒ¢ãƒ³ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²"""
        pokemon_characters = [
            'ãƒ”ã‚«ãƒãƒ¥ã‚¦', 'ã‚¤ãƒ¼ãƒ–ã‚¤', 'ãƒãƒªãƒãƒ­ãƒ³', 'ãƒ•ã‚©ãƒƒã‚³', 'ã‚±ãƒ­ãƒãƒ„',
            'ãƒ•ã‚·ã‚®ãƒ€ãƒ', 'ãƒ’ãƒˆã‚«ã‚²', 'ã‚¼ãƒ‹ã‚¬ãƒ¡', 'ãƒã‚³ãƒªãƒ¼ã‚¿', 'ãƒ’ãƒã‚¢ãƒ©ã‚·',
            'ãƒ¯ãƒ‹ãƒã‚³', 'ã‚­ãƒ¢ãƒª', 'ã‚¢ãƒãƒ£ãƒ¢', 'ãƒŸã‚ºã‚´ãƒ­ã‚¦', 'ãƒŠã‚¨ãƒˆãƒ«',
            'ãƒ’ã‚³ã‚¶ãƒ«', 'ãƒãƒƒãƒãƒ£ãƒ', 'ãƒ„ã‚¿ãƒ¼ã‚¸ãƒ£', 'ãƒã‚«ãƒ–', 'ãƒŸã‚¸ãƒ¥ãƒãƒ«'
        ]
        
        character_sections = []
        lines = raw_text.split('\n')
        
        for character in pokemon_characters:
            if character in raw_text:
                # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’å«ã‚€è¡Œã‚’æ¢ã—ã¦ã€ãã®å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                for i, line in enumerate(lines):
                    if character in line:
                        start_idx = max(0, i - 3)
                        end_idx = min(len(lines), i + 8)
                        section = '\n'.join(lines[start_idx:end_idx])
                        character_sections.append((character, section))
                        break
        
        return character_sections

    def _extract_section_by_jan(self, raw_text: str, jan_code: str) -> str:
        """ç‰¹å®šã®JANã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        jan_line_index = -1
        
        # JANã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã‚’æ¢ã™
        for i, line in enumerate(text_lines):
            if jan_code in line or jan_code.replace('-', '') in line or f"{jan_code[:7]}-{jan_code[7:]}" in line:
                jan_line_index = i
                break
        
        if jan_line_index == -1:
            # JANã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
            return raw_text
        
        # ã‚ˆã‚Šç²¾å¯†ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
        # å•†å“ã‚³ãƒ¼ãƒ‰ï¼ˆST-ã‚³ãƒ¼ãƒ‰ï¼‰ã‚’åŸºæº–ã«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åŒºåˆ‡ã‚‹
        section_start = jan_line_index
        section_end = jan_line_index + 1
        
        # ä¸Šå‘ãã«æ¤œç´¢ã—ã¦ã€ã“ã®JANã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹å•†å“æƒ…å ±ã®é–‹å§‹ç‚¹ã‚’æ¢ã™
        for i in range(jan_line_index, max(0, jan_line_index - 10), -1):
            line = text_lines[i]
            # ST-ã‚³ãƒ¼ãƒ‰ã€å•†å“åã€ã¾ãŸã¯åˆ¥ã®JANã‚³ãƒ¼ãƒ‰ã§åŒºåˆ‡ã‚Š
            if re.search(r'ST-\d{2}[A-Z]{2}', line) or 'å•†å“å' in line:
                section_start = i
                break
            # åˆ¥ã®JANã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ã€ãã“ã§åŒºåˆ‡ã‚Š
            if re.search(r'\b4\d{12}\b', line) and jan_code not in line:
                section_start = i + 1
                break
        
        # ä¸‹å‘ãã«æ¤œç´¢ã—ã¦ã€ã“ã®JANã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹å•†å“æƒ…å ±ã®çµ‚äº†ç‚¹ã‚’æ¢ã™
        for i in range(jan_line_index + 1, min(len(text_lines), jan_line_index + 15)):
            line = text_lines[i]
            # æ¬¡ã®å•†å“ã®ST-ã‚³ãƒ¼ãƒ‰ã¾ãŸã¯JANã‚³ãƒ¼ãƒ‰ã§åŒºåˆ‡ã‚Š
            if re.search(r'ST-\d{2}[A-Z]{2}', line) or re.search(r'\b4\d{12}\b', line):
                section_end = i
                break
            # å•†å“ã‚µã‚¤ã‚ºã®è¡Œã§çµ‚äº†
            if 'å•†å“ã‚µã‚¤ã‚º' in line:
                section_end = i + 1
                break
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        section_lines = text_lines[section_start:section_end]
        section_text = '\n'.join(section_lines)
        
        print(f"ğŸ” Extracted section for JAN {jan_code} (lines {section_start}-{section_end}): {section_text[:100]}...")
        return section_text
    
    def _split_by_en_codes(self, text_lines: list) -> list:
        """EN-ã‚³ãƒ¼ãƒ‰ã‚’åŸºæº–ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²"""
        sections = []
        current_section = []
        
        for line in text_lines:
            # EN-ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹è¡Œã§æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
            if re.search(r'EN-\d+', line) and current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            
            current_section.append(line)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if current_section:
            sections.append('\n'.join(current_section))
        
        print(f"ğŸ” Split into {len(sections)} EN-code sections")
        for i, section in enumerate(sections):
            print(f"   Section {i+1}: {section[:100]}...")
        
        return sections
    
    def _detect_table_structure(self, text_lines: list) -> bool:
        """è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º"""
        # è¡¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        table_headers = [
            'å•†å“å', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'JANã‚³ãƒ¼ãƒ‰', 'ä¾¡æ ¼', 'å¸Œæœ›å°å£²ä¾¡æ ¼', 
            'ç™ºå£²äºˆå®šæ—¥', 'å…¥æ•°', 'ã‚«ãƒ¼ãƒˆãƒ³', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸', 'ã‚µã‚¤ã‚º',
            'EN-', 'ST-', 'Product', 'Code', 'Price'
        ]
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡º
        header_found = False
        data_rows = 0
        
        for line in text_lines:
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®æ¤œå‡º
            if not header_found and any(keyword in line for keyword in table_headers):
                header_found = True
                print(f"ğŸ” TABLE HEADER DETECTED: {line[:100]}")
                continue
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œã®æ¤œå‡º
            if header_found:
                # å•†å“ã‚³ãƒ¼ãƒ‰ã‚„JANã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œ
                if (re.search(r'EN-\d+', line) or 
                    re.search(r'ST-\w+', line) or 
                    re.search(r'4\d{12}', line) or
                    'Â¥' in line or 'å††' in line):
                    data_rows += 1
                    print(f"ğŸ” TABLE DATA ROW: {line[:100]}")
        
        result = header_found and data_rows >= 2
        print(f"ğŸ” TABLE DETECTION RESULT: header_found={header_found}, data_rows={data_rows}, is_table={result}")
        return result
    
    def _split_table_rows(self, text_lines: list) -> list:
        """è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚’è¡Œã”ã¨ã«åˆ†å‰²"""
        sections = []
        header_found = False
        header_line = ""
        processed_products = set()  # é‡è¤‡é˜²æ­¢
        
        # è¡¨ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        table_headers = [
            'å•†å“å', 'å•†å“ã‚³ãƒ¼ãƒ‰', 'JANã‚³ãƒ¼ãƒ‰', 'ä¾¡æ ¼', 'å¸Œæœ›å°å£²ä¾¡æ ¼', 
            'ç™ºå£²äºˆå®šæ—¥', 'å…¥æ•°', 'ã‚«ãƒ¼ãƒˆãƒ³', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸', 'ã‚µã‚¤ã‚º'
        ]
        
        for line in text_lines:
            line = line.strip()
            if not line:
                continue
                
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’æ¤œå‡ºãƒ»ä¿å­˜
            if not header_found and any(keyword in line for keyword in table_headers):
                header_found = True
                header_line = line
                print(f"ğŸ” HEADER SAVED: {header_line}")
                continue
            
            # å•†å“ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æ¤œå‡ºï¼ˆEN-ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã®ã¿ï¼‰
            if header_found:
                # EN-ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã®ã¿ã‚’å•†å“ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦èªè­˜ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
                en_match = re.search(r'EN-(\d+)', line)
                if en_match:
                    en_code = en_match.group(0)  # EN-1420 ãªã©
                    
                    # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å•†å“ã¯ã‚¹ã‚­ãƒƒãƒ—
                    if en_code in processed_products:
                        continue
                    
                    processed_products.add(en_code)
                    
                    # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã¨çµ„ã¿åˆã‚ã›ã¦å®Œæ•´ãªå•†å“æƒ…å ±ã‚’ä½œæˆ
                    product_section = f"{header_line}\n{line}"
                    sections.append(product_section)
                    print(f"ğŸ” PRODUCT SECTION CREATED: {en_code} - {line[:100]}")
        
        print(f"ğŸ” TOTAL PRODUCT SECTIONS: {len(sections)}")
        return sections
    
    def _has_multiple_product_names(self, text_lines: list) -> bool:
        """è¤‡æ•°ã®å•†å“åãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        product_name_count = 0
        
        for line in text_lines:
            line = line.strip()
            # å•†å“åã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            if any(keyword in line for keyword in ['ST-', 'ãƒã‚±ãƒ¢ãƒ³', 'ãƒ”ã‚«ãƒãƒ¥ã‚¦', 'ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯']):
                if len(line) > 10 and len(line) < 100:
                    product_name_count += 1
        
        return product_name_count > 1
    
    def _split_by_product_names(self, text_lines: list) -> list:
        """å•†å“åã‚’åŸºæº–ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²"""
        sections = []
        current_section = []
        
        for line in text_lines:
            line = line.strip()
            
            # æ–°ã—ã„å•†å“ã®é–‹å§‹ã‚’æ¤œå‡º
            if any(keyword in line for keyword in ['ST-', 'JAN']):
                if current_section:
                    sections.append('\n'.join(current_section))
                    current_section = []
            
            current_section.append(line)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _clean_repetitive_text(self, text_lines: list) -> list:
        """ç¹°ã‚Šè¿”ã—ãƒ†ã‚­ã‚¹ãƒˆã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        cleaned_lines = []
        seen_lines = set()
        
        for line in text_lines:
            line = line.strip()
            if not line or len(line) < 3:
                continue
            
            # å®Œå…¨ã«åŒã˜è¡Œã¯1å›ã ã‘ä¿æŒ
            if line in seen_lines:
                continue
            seen_lines.add(line)
            
            # é•·ã™ãã‚‹è¡Œï¼ˆç¹°ã‚Šè¿”ã—ã®å¯èƒ½æ€§ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if len(line) > 200:
                continue
            
            # åŒã˜å˜èªãŒå¤šæ•°ç¹°ã‚Šè¿”ã•ã‚Œã‚‹è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            words = line.split()
            if len(words) > 10:
                word_counts = {}
                for word in words:
                    if len(word) > 2:
                        word_counts[word] = word_counts.get(word, 0) + 1
                
                # åŒã˜å˜èªãŒè¡Œã®50%ä»¥ä¸Šã‚’å ã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                max_count = max(word_counts.values()) if word_counts else 0
                if max_count > len(words) * 0.5:
                    continue
            
            cleaned_lines.append(line)
        
        return cleaned_lines
    
    def _extract_product_name(self, text_lines: list, raw_text: str) -> str:
        """å•†å“åã‚’æŠ½å‡º"""
        # é™¤å¤–ã™ã¹ããƒã‚¤ã‚ºãƒ†ã‚­ã‚¹ãƒˆ
        noise_patterns = [
            'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚·ãƒ§ãƒƒãƒ—', 'ã‚¢ãƒ‹ãƒ¡ã‚¤ãƒˆã‚«ãƒ•ã‚§ã‚¹ã‚¿ãƒ³ãƒ‰', 'é€šè²©', 'æµ·å¤–åº—èˆ—',
            'animatecafe', 'online', 'shop', 'store', 'www.', 'http',
            'â€»', 'æ³¨æ„', 'è­¦å‘Š', 'copyright', 'Â©', 'reserved'
        ]
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
        def is_repetitive_text(text):
            """ç¹°ã‚Šè¿”ã—ã®å¤šã„ãƒ†ã‚­ã‚¹ãƒˆã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯"""
            if len(text) < 10:
                return False
            
            # åŒã˜æ–‡å­—åˆ—ãŒ3å›ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            words = text.split()
            if len(words) > 6:
                word_counts = {}
                for word in words:
                    if len(word) > 3:  # çŸ­ã„å˜èªã¯é™¤å¤–
                        word_counts[word] = word_counts.get(word, 0) + 1
                
                # åŒã˜å˜èªãŒ3å›ä»¥ä¸Šå‡ºç¾ã—ã¦ã„ã‚‹å ´åˆã¯ç¹°ã‚Šè¿”ã—ãƒ†ã‚­ã‚¹ãƒˆã¨åˆ¤å®š
                for count in word_counts.values():
                    if count >= 3:
                        return True
            return False
        
        # æœ‰åŠ¹ãªå•†å“åå€™è£œã‚’æ¢ã™
        candidates = []
        
        for line in text_lines:
            line = line.strip()
            
            # è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€ãƒ‘ã‚¤ãƒ—åŒºåˆ‡ã‚Šãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if '|' in line:
                # ãƒ‘ã‚¤ãƒ—ã§åˆ†å‰²ã—ã¦æœ€åˆã®æ„å‘³ã®ã‚ã‚‹éƒ¨åˆ†ã‚’å–å¾—
                parts = [part.strip() for part in line.split('|') if part.strip()]
                if parts:
                    # EN-ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€éƒ¨åˆ†ã‚’å„ªå…ˆçš„ã«é¸æŠ
                    for part in parts:
                        if re.search(r'EN-\d+', part) and len(part) > 10:
                            line = part
                            break
                    else:
                        # EN-ã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯æœ€åˆã®é•·ã„éƒ¨åˆ†ã‚’ä½¿ç”¨
                        line = next((part for part in parts if len(part) > 10), parts[0] if parts else line)
            
            if len(line) < 5 or len(line) > 200:  # çŸ­ã™ãã‚‹ãƒ»é•·ã™ãã‚‹è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            # ãƒã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if any(noise in line for noise in noise_patterns):
                continue
            
            # ç¹°ã‚Šè¿”ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
            if is_repetitive_text(line):
                continue
            
            # å•†å“åã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆ
            score = 0
            
            # ST-ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€å•†å“åï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ï¼‰
            if re.search(r'ST-\w+', line):
                score += 15
                
            # EN-ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€å•†å“åï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ï¼‰
            if re.search(r'EN-\d+', line):
                score += 15
                
            # ãƒã‚±ãƒ¢ãƒ³é–¢é€£å•†å“åï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
            if 'ãƒã‚±ãƒ¢ãƒ³' in line and any(keyword in line for keyword in ['ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯', 'ãƒ•ã‚£ã‚®ãƒ¥ã‚¢', 'ã¬ã„ãã‚‹ã¿', 'ã‚«ãƒ¼ãƒ‰']):
                score += 12
                
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å•†å“åï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
            if any(keyword in line for keyword in ['ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'ã‚¹ãƒªãƒ¼ãƒ–', 'ç”˜ç¥ã•ã‚“', 'ã®ç¸çµã³']):
                score += 12
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é–¢é€£å•†å“ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
            if 'ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°' in line and any(keyword in line for keyword in ['ãƒãƒƒã‚¸', 'ã‚«ãƒ¼ãƒ‰', 'ã‚­ãƒ£ãƒ©', 'ãƒ•ã‚£ã‚®ãƒ¥ã‚¢']):
                score += 10
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ä»˜ãå•†å“åï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
            if any(ver in line.lower() for ver in ['ver.', 'version', 'vol.', 'v.']):
                score += 8
            
            # ç¨®é¡æ•°ä»˜ãå•†å“åï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
            if 'å…¨' in line and 'ç¨®' in line:
                score += 8
            
            # å•†å“åã‚‰ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
            product_keywords = ['é™å®š', 'ã‚»ãƒƒãƒˆ', 'ãƒ‘ãƒƒã‚¯', 'ãƒœãƒƒã‚¯ã‚¹', 'ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³', 'ã‚·ãƒªãƒ¼ã‚º', 'åˆå›']
            for keyword in product_keywords:
                if keyword in line:
                    score += 3
            
            # é©åº¦ãªé•·ã•ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
            if 10 <= len(line) <= 50:
                score += 2
            
            # æ•°å­—ã§å§‹ã¾ã‚‰ãªã„ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰
            if not any(char.isdigit() for char in line[:3]):
                score += 1
            
            if score > 0:
                candidates.append((line, score))
        
        # ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å•†å“åã‚’è¿”ã™
        if candidates:
            return candidates[0][0]
        
        return None
    
    def _extract_product_code(self, raw_text: str) -> str:
        """å•†å“ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆEN-XXXX, ST-XXXX ãªã©ï¼‰"""
        # å•†å“ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        code_patterns = [
            r'\b(EN-\d+)\b',  # EN-1234 å½¢å¼
            r'\b(ST-\w+)\b',  # ST-XXXX å½¢å¼
            r'å•†å“ã‚³ãƒ¼ãƒ‰[ï¼š:\s]*([A-Z0-9-]+)',  # å•†å“ã‚³ãƒ¼ãƒ‰: XXXXX
            r'å“ç•ª[ï¼š:\s]*([A-Z0-9-]+)',  # å“ç•ª: XXXXX
        ]
        
        for pattern in code_patterns:
            match = re.search(pattern, raw_text)
            if match:
                code = match.group(1)
                print(f"âœ… PRODUCT CODE FOUND: {code}")
                return code
        
        return None
    
    def _extract_jan_code(self, raw_text: str) -> str:
        """JANã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆ8æ¡ã¾ãŸã¯13æ¡ï¼‰- ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ç”»åƒå¯¾å¿œå¼·åŒ–ç‰ˆ"""
        # 13æ¡ã®JANã‚³ãƒ¼ãƒ‰ï¼ˆãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã®æŠ½å‡ºã‚’æœ€å„ªå…ˆï¼‰
        jan_13_patterns = [
            r'\b(4\d{12})\b',  # 4ã§å§‹ã¾ã‚‹13æ¡ï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ï¼‰
            r'(4970381\d{6})',  # ã‚¨ãƒ³ã‚¹ã‚«ã‚¤ã®ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³
            r'4970381[-\s]?(\d{6})',  # ãƒã‚¤ãƒ•ãƒ³ã¾ãŸã¯ã‚¹ãƒšãƒ¼ã‚¹ä»˜ãã‚¨ãƒ³ã‚¹ã‚«ã‚¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'JAN[ã‚³ãƒ¼ãƒ‰ï¼š:\s]*(4\d{12})',  # JANã‚³ãƒ¼ãƒ‰: 4XXXXXXXXXXXX
            r'å˜å“\s*JAN[ã‚³ãƒ¼ãƒ‰ï¼š:\s]*(4\d{12})',  # å˜å“JANã‚³ãƒ¼ãƒ‰: 4XXXXXXXXXXXX
            r'ã‚³ãƒ¼ãƒ‰[ï¼š:\s]*(4\d{12})',  # ã‚³ãƒ¼ãƒ‰: 4XXXXXXXXXXXX
            r'ãƒãƒ¼ã‚³ãƒ¼ãƒ‰[ï¼š:\s]*(4\d{12})',  # ãƒãƒ¼ã‚³ãƒ¼ãƒ‰: 4XXXXXXXXXXXX
            r'(\d{13})',  # ä»»æ„ã®13æ¡ï¼ˆãƒãƒ¼ã‚³ãƒ¼ãƒ‰ä¸‹ã®æ•°å­—ï¼‰
        ]
        
        print(f"ğŸ” JANã‚³ãƒ¼ãƒ‰æŠ½å‡ºé–‹å§‹: {raw_text[:100]}...")
        
        for i, pattern in enumerate(jan_13_patterns):
            matches = re.findall(pattern, raw_text)
            for match in matches:
                if isinstance(match, tuple):
                    # ãƒã‚¤ãƒ•ãƒ³ä»˜ãã®å ´åˆ
                    if len(match) == 1:
                        jan_code = f"4970381{match[0]}"
                    else:
                        jan_code = match[0] if match[0] else match[1]
                else:
                    jan_code = match
                
                # JAN code validation
                if len(jan_code) == 13 and jan_code.isdigit():
                    # ã‚ˆã‚Šå³å¯†ãªJANã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    if jan_code.startswith('4'):
                        print(f"âœ… JAN CODE FOUND (pattern {i+1}): {jan_code}")
                        return jan_code
                    elif jan_code.startswith('49') or jan_code.startswith('45'):
                        print(f"âœ… JAN CODE FOUND (Japan specific): {jan_code}")
                        return jan_code
        
        # 8æ¡ã®JANã‚³ãƒ¼ãƒ‰ï¼ˆçŸ­ç¸®å½¢ï¼‰- ãƒãƒ¼ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚‚æŠ½å‡º
        jan_8_patterns = [
            r'\b(\d{8})\b',
            r'çŸ­ç¸®[ã‚³ãƒ¼ãƒ‰ï¼š:\s]*(\d{8})',
            r'8æ¡[ã‚³ãƒ¼ãƒ‰ï¼š:\s]*(\d{8})',
        ]
        
        for pattern in jan_8_patterns:
            match = re.search(pattern, raw_text)
            if match:
                jan_code = match.group(1)
                if jan_code.isdigit() and len(jan_code) == 8:
                    print(f"âœ… JAN CODE FOUND (8-digit): {jan_code}")
                    return jan_code
        
        # Additional fallback for any 13-digit number that looks like a JAN code
        all_numbers = re.findall(r'\b(\d{13})\b', raw_text)
        for number in all_numbers:
            if number.startswith(('4', '49', '45')):
                print(f"âœ… JAN CODE FOUND (fallback): {number}")
                return number
        
        print("âŒ No JAN code found in text")
        return None
    
    def _extract_price(self, raw_text: str) -> str:
        """ä¾¡æ ¼ã‚’æŠ½å‡º"""
        # Â¥è¨˜å·ä»˜ãã®ä¾¡æ ¼
        yen_prices = re.findall(r'Â¥\s*([0-9,]+)', raw_text)
        for price_str in yen_prices:
            price_num = int(price_str.replace(',', ''))
            if 50 <= price_num <= 100000:  # ç¾å®Ÿçš„ãªä¾¡æ ¼ç¯„å›²
                return f"Â¥{price_str}"
        
        # ä¾¡æ ¼ã€å€¤æ®µãªã©ã®æ–‡å­—ã®å¾Œã®æ•°å­—
        price_patterns = [
            r'ä¾¡æ ¼[ï¼š:\s]*Â¥?([0-9,]+)',
            r'å€¤æ®µ[ï¼š:\s]*Â¥?([0-9,]+)',
            r'å®šä¾¡[ï¼š:\s]*Â¥?([0-9,]+)',
            r'ç¨è¾¼[ï¼š:\s]*Â¥?([0-9,]+)',
            r'([0-9,]+)\s*å††'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, raw_text)
            for price_str in matches:
                price_num = int(price_str.replace(',', ''))
                if 50 <= price_num <= 100000:
                    return f"Â¥{price_str}"
        
        return None
    
    def _extract_stock(self, raw_text: str, text_lines: list) -> int:
        """åœ¨åº«æ•°ã‚’æŠ½å‡º"""
        # åœ¨åº«é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¾Œã®æ•°å­—
        stock_patterns = [
            r'åœ¨åº«[ï¼š:\s]*(\d+)',
            r'æ•°é‡[ï¼š:\s]*(\d+)',
            r'æ®‹ã‚Š[ï¼š:\s]*(\d+)',
            r'stock[ï¼š:\s]*(\d+)',
            r'qty[ï¼š:\s]*(\d+)'
        ]
        
        for pattern in stock_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return None
    
    def _extract_category(self, raw_text: str) -> str:
        """ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºãƒ»æ¨å®š"""
        # ç›´æ¥çš„ãªã‚«ãƒ†ã‚´ãƒªè¡¨è¨˜
        category_patterns = [
            r'ã‚«ãƒ†ã‚´ãƒª[ï¼š:\s]*([^\n\r]+)',
            r'åˆ†é¡[ï¼š:\s]*([^\n\r]+)',
            r'ã‚¸ãƒ£ãƒ³ãƒ«[ï¼š:\s]*([^\n\r]+)'
        ]
        
        for pattern in category_patterns:
            match = re.search(pattern, raw_text)
            if match:
                return match.group(1).strip()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¨å®š
        if 'ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°' in raw_text:
            if 'ã‚«ãƒ¼ãƒ‰' in raw_text:
                return 'ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚«ãƒ¼ãƒ‰'
            elif any(keyword in raw_text for keyword in ['ãƒãƒƒã‚¸', 'ç¼¶ãƒãƒƒã‚¸', 'ã‚°ãƒƒã‚º']):
                return 'ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚°ãƒƒã‚º'
        
        category_keywords = {
            'ãƒ•ã‚£ã‚®ãƒ¥ã‚¢': ['ãƒ•ã‚£ã‚®ãƒ¥ã‚¢', 'figure', 'ã­ã‚“ã©ã‚ã„ã©'],
            'ã‚²ãƒ¼ãƒ ': ['ã‚²ãƒ¼ãƒ ', 'game', 'ã‚½ãƒ•ãƒˆ'],
            'ã‚¢ãƒ‹ãƒ¡ã‚°ãƒƒã‚º': ['ã‚¢ãƒ‹ãƒ¡', 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'anime'],
            'æœ¬ãƒ»é›‘èªŒ': ['æœ¬', 'é›‘èªŒ', 'book', 'magazine'],
            'éŸ³æ¥½': ['CD', 'DVD', 'ãƒ–ãƒ«ãƒ¼ãƒ¬ã‚¤', 'ã‚µã‚¦ãƒ³ãƒ‰ãƒˆãƒ©ãƒƒã‚¯']
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in raw_text for keyword in keywords):
                return category
        
        return None
    
    def _extract_release_date(self, raw_text: str) -> str:
        """ç™ºå£²äºˆå®šæ—¥ã‚’æŠ½å‡º"""
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2024å¹´12æœˆ15æ—¥
            r'(\d{4})å¹´(\d{1,2})æœˆ',            # 2024å¹´12æœˆ
            r'(\d{4})/(\d{1,2})/(\d{1,2})',    # 2024/12/15
            r'(\d{4})-(\d{1,2})-(\d{1,2})',    # 2024-12-15
            r'(\d{1,2})/(\d{1,2})/(\d{4})',    # 12/15/2024
        ]
        
        # ç™ºå£²æ—¥é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        release_keywords = [
            'ç™ºå£²äºˆå®šæ—¥', 'ç™ºå£²æ—¥', 'ç™ºå£²äºˆå®š', 'ç™ºå£²é–‹å§‹æ—¥', 'ãƒªãƒªãƒ¼ã‚¹æ—¥', 
            'ç™ºå£²', 'è²©å£²é–‹å§‹', 'äºˆå®šæ—¥', 'ç™ºå£²æ™‚æœŸ'
        ]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã®æ—¥ä»˜ã‚’æ¤œç´¢
        for keyword in release_keywords:
            if keyword in raw_text:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                keyword_index = raw_text.find(keyword)
                surrounding_text = raw_text[max(0, keyword_index-50):keyword_index+100]
                
                # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                for pattern in date_patterns:
                    match = re.search(pattern, surrounding_text)
                    if match:
                        if len(match.groups()) == 3:
                            year, month, day = match.groups()
                            return f"{year}å¹´{int(month)}æœˆ{int(day)}æ—¥"
                        elif len(match.groups()) == 2:
                            year, month = match.groups()
                            return f"{year}å¹´{int(month)}æœˆ"
        
        # å˜ç‹¬ã®æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ï¼ˆ2024å¹´12æœˆãªã©ï¼‰
        for pattern in date_patterns:
            match = re.search(pattern, raw_text)
            if match:
                if len(match.groups()) == 3:
                    year, month, day = match.groups()
                    # å¦¥å½“ãªæ—¥ä»˜ã‹ãƒã‚§ãƒƒã‚¯
                    if 2020 <= int(year) <= 2030 and 1 <= int(month) <= 12:
                        return f"{year}å¹´{int(month)}æœˆ{int(day)}æ—¥"
                elif len(match.groups()) == 2:
                    year, month = match.groups()
                    if 2020 <= int(year) <= 2030 and 1 <= int(month) <= 12:
                        return f"{year}å¹´{int(month)}æœˆ"
        
        return None
    
    def _extract_brand(self, raw_text: str, text_lines: list) -> str:
        """ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æŠ½å‡º"""
        # é™¤å¤–ã™ã¹ããƒã‚¤ã‚ºãƒ†ã‚­ã‚¹ãƒˆ
        noise_patterns = [
            'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚·ãƒ§ãƒƒãƒ—', 'ã‚¢ãƒ‹ãƒ¡ã‚¤ãƒˆã‚«ãƒ•ã‚§ã‚¹ã‚¿ãƒ³ãƒ‰', 'é€šè²©', 'æµ·å¤–åº—èˆ—',
            'animatecafe', 'online', 'shop', 'store'
        ]
        
        # ç›´æ¥çš„ãªãƒ–ãƒ©ãƒ³ãƒ‰è¡¨è¨˜
        brand_patterns = [
            r'ãƒ–ãƒ©ãƒ³ãƒ‰[ï¼š:\s]*([^\n\r]+)',
            r'ãƒ¡ãƒ¼ã‚«ãƒ¼[ï¼š:\s]*([^\n\r]+)',
            r'brand[ï¼š:\s]*([^\n\r]+)',
            r'è£½é€ å…ƒ[ï¼š:\s]*([^\n\r]+)',
            r'ç™ºå£²å…ƒ[ï¼š:\s]*([^\n\r]+)'
        ]
        
        for pattern in brand_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                brand_text = match.group(1).strip()
                # ãƒã‚¤ã‚ºãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã¾ãªã„å ´åˆã®ã¿è¿”ã™
                if not any(noise in brand_text for noise in noise_patterns) and len(brand_text) < 50:
                    return brand_text
        
        # æ—¢çŸ¥ã®ãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
        known_brands = [
            'ã‚¨ãƒ³ã‚¹ã‚«ã‚¤', 'ENSKY', 'ãƒãƒ³ãƒ€ã‚¤', 'BANDAI', 'ã‚¿ã‚«ãƒ©ãƒˆãƒŸãƒ¼', 'TAKARA TOMY',
            'ã‚³ãƒŠãƒŸ', 'KONAMI', 'ã‚»ã‚¬', 'SEGA', 'ã‚¹ã‚¯ã‚¦ã‚§ã‚¢ãƒ»ã‚¨ãƒ‹ãƒƒã‚¯ã‚¹', 'SQUARE ENIX',
            'ã‚°ãƒƒãƒ‰ã‚¹ãƒã‚¤ãƒ«ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼', 'Good Smile Company', 'ã‚³ãƒˆãƒ–ã‚­ãƒ¤', 'KOTOBUKIYA',
            'ãƒ¡ãƒ‡ã‚£ã‚³ã‚¹', 'MEDICOS', 'ãƒ•ãƒªãƒ¥ãƒ¼', 'FuRyu', 'ã‚¢ãƒ«ã‚¿ãƒ¼', 'ALTER',
            'ã‚¢ãƒ‹ãƒ¡ã‚¤ãƒˆ', 'animate'
        ]
        
        # æ—¢çŸ¥ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æ¤œç´¢ï¼ˆæœ€ã‚‚çŸ­ã„ãƒãƒƒãƒã‚’å„ªå…ˆï¼‰
        found_brands = []
        for brand in known_brands:
            if brand in raw_text:
                # å‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒã‚¤ã‚ºã§ãªã„ã‹ç¢ºèª
                brand_contexts = []
                for line in text_lines:
                    if brand in line and not any(noise in line for noise in noise_patterns):
                        if len(line) < 100:  # é•·ã™ãã‚‹è¡Œã¯é™¤å¤–
                            brand_contexts.append(line.strip())
                
                if brand_contexts:
                    # æœ€ã‚‚çŸ­ãã¦ã‚¯ãƒªãƒ¼ãƒ³ãªæ–‡è„ˆã‚’é¸æŠ
                    best_context = min(brand_contexts, key=len)
                    if len(best_context) < 50:
                        found_brands.append((brand, len(best_context)))
        
        # æœ€ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ–ãƒ©ãƒ³ãƒ‰åã‚’è¿”ã™
        if found_brands:
            found_brands.sort(key=lambda x: x[1])  # çŸ­ã„é †
            return found_brands[0][0]
        
        return None
    
    def _extract_manufacturer(self, raw_text: str, text_lines: list, brand: str) -> str:
        """è£½é€ å…ƒã‚’æŠ½å‡º"""
        # ç›´æ¥çš„ãªè£½é€ å…ƒè¡¨è¨˜
        manufacturer_patterns = [
            r'è£½é€ å…ƒ[ï¼š:\s]*([^\n\r]+)',
            r'ç™ºå£²å…ƒ[ï¼š:\s]*([^\n\r]+)',
            r'è²©å£²å…ƒ[ï¼š:\s]*([^\n\r]+)',
            r'manufacturer[ï¼š:\s]*([^\n\r]+)'
        ]
        
        for pattern in manufacturer_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰ã¨åŒã˜å ´åˆãŒå¤šã„
        if brand:
            return brand
        
        return None
    
    def _extract_description(self, raw_text: str, text_lines: list) -> str:
        """å•†å“èª¬æ˜ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆ - ã‚ˆã‚Šé©åˆ‡ãªèª¬æ˜æ–‡ã‚’ç”Ÿæˆï¼‰"""
        description_patterns = [
            r'å•†å“èª¬æ˜[ï¼š:\s]*([^\n\r]+)',
            r'è©³ç´°[ï¼š:\s]*([^\n\r]+)',
            r'Description[ï¼š:\s]*([^\n\r]+)'
        ]
        
        # ç›´æ¥çš„ãªå•†å“èª¬æ˜æ–‡ã‚’æ¢ã™
        for pattern in description_patterns:
            match = re.search(pattern, raw_text)
            if match:
                desc = match.group(1).strip()
                if len(desc) > 10 and len(desc) < 200:  # é©åˆ‡ãªé•·ã•ã®èª¬æ˜æ–‡
                    return desc
        
        # å•†å“åã‹ã‚‰ç°¡æ½”ãªèª¬æ˜ã‚’ç”Ÿæˆ
        product_name_match = re.search(r'å•†å“å[ï¼š:\s]*([^\n\r]+)', raw_text)
        if product_name_match:
            product_name = product_name_match.group(1).strip()
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã¨ã‚¢ã‚¤ãƒ†ãƒ ç¨®é¡ã‚’æŠ½å‡º
            character_patterns = [
                r'(ãƒ”ã‚«ãƒãƒ¥ã‚¦|ã‚¤ãƒ¼ãƒ–ã‚¤|ãƒãƒªãƒãƒ­ãƒ³|ãƒ•ã‚©ãƒƒã‚³|ã‚±ãƒ­ãƒãƒ„)',
                r'(ãƒã‚±ãƒ¢ãƒ³)',
            ]
            
            item_patterns = [
                r'(ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯|è²¯é‡‘ç®±)',
                r'(ãƒ•ã‚£ã‚®ãƒ¥ã‚¢)',
                r'(ã¬ã„ãã‚‹ã¿)',
                r'(ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°)',
                r'(ã‚«ãƒ¼ãƒ‰)',
                r'(ã‚°ãƒƒã‚º)',
            ]
            
            character = ""
            item_type = ""
            
            for pattern in character_patterns:
                match = re.search(pattern, raw_text)
                if match:
                    character = match.group(1)
                    break
            
            for pattern in item_patterns:
                match = re.search(pattern, raw_text)
                if match:
                    item_type = match.group(1)
                    break
            
            # ç°¡æ½”ãªå•†å“èª¬æ˜ã‚’ç”Ÿæˆ
            if character and item_type:
                if item_type == "ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯" or item_type == "è²¯é‡‘ç®±":
                    return f"{character}ã®å¯æ„›ã„è²¯é‡‘ç®±ã§ã™ã€‚ã‚¤ãƒ³ãƒ†ãƒªã‚¢ã¨ã—ã¦ã‚‚æ¥½ã—ã‚ã¾ã™ã€‚"
                elif item_type == "ãƒ•ã‚£ã‚®ãƒ¥ã‚¢":
                    return f"{character}ã®ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã§ã™ã€‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚„ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ã«æœ€é©ã€‚"
                elif item_type == "ã¬ã„ãã‚‹ã¿":
                    return f"{character}ã®ã¬ã„ãã‚‹ã¿ã§ã™ã€‚æŸ”ã‚‰ã‹ãæŠ±ãå¿ƒåœ°æŠœç¾¤ã€‚"
                else:
                    return f"{character}ã®{item_type}ã§ã™ã€‚"
            elif character:
                return f"{character}é–¢é€£ã‚°ãƒƒã‚ºã§ã™ã€‚"
            elif item_type:
                return f"{item_type}ã‚¢ã‚¤ãƒ†ãƒ ã§ã™ã€‚"
        
        # å•†å“ã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ã¦ç°¡æ½”ãªèª¬æ˜ã‚’ä½œæˆ
        features = []
        
        # ã‚µã‚¤ã‚ºæƒ…å ±
        size_match = re.search(r'ç´„\s*(\d+)\s*Ã—\s*(\d+)\s*Ã—\s*(\d+)\s*mm', raw_text)
        if size_match:
            features.append(f"ã‚µã‚¤ã‚º: ç´„{size_match.group(1)}Ã—{size_match.group(2)}Ã—{size_match.group(3)}mm")
        
        # ç´ ææƒ…å ±
        material_patterns = [
            r'ç´ æ[ï¼š:\s]*([^\n\r]+)',
            r'æè³ª[ï¼š:\s]*([^\n\r]+)',
        ]
        for pattern in material_patterns:
            match = re.search(pattern, raw_text)
            if match:
                material = match.group(1).strip()
                if len(material) < 50:
                    features.append(f"ç´ æ: {material}")
                break
        
        # ä¾¡æ ¼æƒ…å ±
        price_match = re.search(r'Â¥\s*([0-9,]+)', raw_text)
        if price_match:
            price = price_match.group(1)
            features.append(f"å¸Œæœ›å°å£²ä¾¡æ ¼: Â¥{price}")
        
        # ç‰¹å¾´ã‚’ã¾ã¨ã‚ã¦èª¬æ˜æ–‡ã‚’ä½œæˆ
        if features:
            base_desc = "å•†å“ã®è©³ç´°æƒ…å ±: "
            return base_desc + "ã€".join(features[:3])  # æœ€å¤§3ã¤ã®ç‰¹å¾´
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å•†å“ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã®èª¬æ˜
        if 'ãƒã‚±ãƒ¢ãƒ³' in raw_text:
            if 'ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯' in raw_text or 'è²¯é‡‘ç®±' in raw_text:
                return "ãƒã‚±ãƒ¢ãƒ³ã®è²¯é‡‘ç®±ã§ã™ã€‚å¯æ„›ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã§ãŠé‡‘ã‚’è²¯ã‚ãªãŒã‚‰ã‚¤ãƒ³ãƒ†ãƒªã‚¢ã¨ã—ã¦ã‚‚æ¥½ã—ã‚ã¾ã™ã€‚"
            elif 'ãƒ•ã‚£ã‚®ãƒ¥ã‚¢' in raw_text:
                return "ãƒã‚±ãƒ¢ãƒ³ã®ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã§ã™ã€‚ç²¾å·§ãªä½œã‚Šã§ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚„ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ã«æœ€é©ã§ã™ã€‚"
            else:
                return "ãƒã‚±ãƒ¢ãƒ³é–¢é€£ã®å•†å“ã§ã™ã€‚ãƒ•ã‚¡ãƒ³ã®æ–¹ã«ãŠã™ã™ã‚ã®ã‚¢ã‚¤ãƒ†ãƒ ã§ã™ã€‚"
        
        # ãã®ä»–ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
        if 'ã‚¢ãƒ‹ãƒ¡' in raw_text or 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼' in raw_text:
            return "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚°ãƒƒã‚ºã§ã™ã€‚ãƒ•ã‚¡ãƒ³ã®æ–¹ã«ãŠã™ã™ã‚ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã§ã™ã€‚"
        
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return "å•†å“ã®è©³ç´°ã«ã¤ã„ã¦ã¯å•†å“åã‚„ã‚«ãƒ†ã‚´ãƒªã‚’ã”å‚ç…§ãã ã•ã„ã€‚"
    
    def _extract_weight(self, raw_text: str) -> str:
        """é‡é‡ãƒ»ã‚µã‚¤ã‚ºæƒ…å ±ã‚’æŠ½å‡º"""
        weight_patterns = [
            r'é‡é‡[ï¼š:\s]*([0-9.]+\s*[gkgã‚°ãƒ©ãƒ ã‚­ãƒ­]+)',
            r'é‡ã•[ï¼š:\s]*([0-9.]+\s*[gkgã‚°ãƒ©ãƒ ã‚­ãƒ­]+)',
            r'([0-9.]+)\s*(g|kg|ã‚°ãƒ©ãƒ |ã‚­ãƒ­)',
            r'ã‚µã‚¤ã‚º[ï¼š:\s]*([0-9.Ã—xX\s]*[cmmmã‚¤ãƒ³ãƒ]+)',
            r'([0-9.]+)\s*(mm|cm|ã‚¤ãƒ³ãƒ)'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, raw_text)
            if match:
                return match.group(1).strip()
        
        return None
    
    def _extract_color(self, raw_text: str, text_lines: list) -> str:
        """è‰²æƒ…å ±ã‚’æŠ½å‡º"""
        # ç›´æ¥çš„ãªè‰²è¡¨è¨˜
        color_patterns = [
            r'è‰²[ï¼š:\s]*([^\n\r]+)',
            r'ã‚«ãƒ©ãƒ¼[ï¼š:\s]*([^\n\r]+)',
            r'color[ï¼š:\s]*([^\n\r]+)'
        ]
        
        for pattern in color_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # è‰²åã®æ¤œå‡º
        colors = [
            'èµ¤', 'é’', 'ç·‘', 'é»„', 'é»’', 'ç™½', 'èŒ¶', 'ç´«', 'æ©™', 'ãƒ”ãƒ³ã‚¯',
            'ãƒ¬ãƒƒãƒ‰', 'ãƒ–ãƒ«ãƒ¼', 'ã‚°ãƒªãƒ¼ãƒ³', 'ã‚¤ã‚¨ãƒ­ãƒ¼', 'ãƒ–ãƒ©ãƒƒã‚¯', 'ãƒ›ãƒ¯ã‚¤ãƒˆ',
            'ã‚´ãƒ¼ãƒ«ãƒ‰', 'ã‚·ãƒ«ãƒãƒ¼', 'ãƒ¡ã‚¿ãƒªãƒƒã‚¯', 'ã‚¯ãƒªã‚¢', 'é€æ˜'
        ]
        
        for color in colors:
            if color in raw_text:
                return color
        
        return None
    
    def _extract_material(self, raw_text: str, text_lines: list) -> str:
        """ç´ ææƒ…å ±ã‚’æŠ½å‡º"""
        # ç›´æ¥çš„ãªç´ æè¡¨è¨˜
        material_patterns = [
            r'ç´ æ[ï¼š:\s]*([^\n\r]+)',
            r'æè³ª[ï¼š:\s]*([^\n\r]+)',
            r'material[ï¼š:\s]*([^\n\r]+)'
        ]
        
        for pattern in material_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # ç´ æåã®æ¤œå‡º
        materials = [
            'ãƒ—ãƒ©ã‚¹ãƒãƒƒã‚¯', 'PVC', 'ABS', 'é‡‘å±', 'ãƒ¡ã‚¿ãƒ«', 'ã‚¢ãƒ«ãƒŸ',
            'ç´™', 'ãƒšãƒ¼ãƒ‘ãƒ¼', 'å¸ƒ', 'ãƒ•ã‚¡ãƒ–ãƒªãƒƒã‚¯', 'ãƒ¬ã‚¶ãƒ¼', 'é©',
            'ã‚¬ãƒ©ã‚¹', 'ã‚¢ã‚¯ãƒªãƒ«', 'ãƒãƒªã‚¨ã‚¹ãƒ†ãƒ«', 'æœ¨æ', 'ã‚¦ãƒƒãƒ‰'
        ]
        
        for material in materials:
            if material in raw_text:
                return material
        
        return None
    
    def _extract_origin(self, raw_text: str, text_lines: list) -> str:
        """åŸç”£åœ°ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        origin_patterns = [
            r'åŸç”£åœ°[ï¼š:\s]*([^\n\r]+)',
            r'åŸç”£å›½[ï¼š:\s]*([^\n\r]+)',
            r'è£½é€ å›½[ï¼š:\s]*([^\n\r]+)',
            r'ç”Ÿç”£å›½[ï¼š:\s]*([^\n\r]+)',
            r'Made\s*in\s*([^\n\r]+)',
            r'Country\s*of\s*Origin[ï¼š:\s]*([^\n\r]+)',
            r'ç”Ÿç”£åœ°[ï¼š:\s]*([^\n\r]+)',
        ]
        
        for pattern in origin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                origin = match.group(1).strip()
                # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»
                origin = re.sub(r'[ï¼š:\s]+$', '', origin)
                if len(origin) < 50 and origin:  # é©åˆ‡ãªé•·ã•ã®å›½å
                    return origin
        
        # ä¸€èˆ¬çš„ãªå›½åã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç›´æ¥æ¤œç´¢
        country_keywords = [
            'æ—¥æœ¬', 'Japan', 'ä¸­å›½', 'China', 'éŸ“å›½', 'Korea', 'ãƒ™ãƒˆãƒŠãƒ ', 'Vietnam',
            'ã‚¿ã‚¤', 'Thailand', 'ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢', 'Indonesia', 'ãƒãƒ¬ãƒ¼ã‚·ã‚¢', 'Malaysia',
            'ã‚¢ãƒ¡ãƒªã‚«', 'USA', 'ãƒ‰ã‚¤ãƒ„', 'Germany', 'ãƒ•ãƒ©ãƒ³ã‚¹', 'France', 
            'ã‚¤ã‚¿ãƒªã‚¢', 'Italy', 'ã‚¤ã‚®ãƒªã‚¹', 'UK', 'ã‚¹ãƒšã‚¤ãƒ³', 'Spain'
        ]
        
        # è£½é€ é–¢é€£ã®æ–‡è„ˆã§å›½åã‚’æ¤œç´¢
        for country in country_keywords:
            # è£½é€ ã€ç”Ÿç”£ãªã©ã®æ–‡è„ˆã§å›½åãŒå‡ºç¾ã™ã‚‹å ´åˆ
            context_patterns = [
                rf'è£½é€ .*{country}',
                rf'ç”Ÿç”£.*{country}',
                rf'{country}.*è£½é€ ',
                rf'{country}.*ç”Ÿç”£',
                rf'made.*{country}',
                rf'{country}.*made'
            ]
            
            for context_pattern in context_patterns:
                if re.search(context_pattern, raw_text, re.IGNORECASE):
                    return country
        
        # ãƒã‚±ãƒ¢ãƒ³ãªã©ã®æ—¥æœ¬è£½å“ã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ—¥æœ¬ã‚’è¨­å®š
        if any(keyword in raw_text for keyword in ['ãƒã‚±ãƒ¢ãƒ³', 'ã‚¨ãƒ³ã‚¹ã‚«ã‚¤', 'æ ªå¼ä¼šç¤¾ã‚¨ãƒ³ã‚¹ã‚«ã‚¤']):
            return "æ—¥æœ¬"
        
        return None
    
    def _extract_quantity_per_pack(self, raw_text: str, text_lines: list) -> str:
        """å…¥æ•°ã‚’æŠ½å‡º"""
        quantity_patterns = [
            r'å…¥æ•°[ï¼š:\s]*(\d+)',
            r'å…¥ã‚Šæ•°[ï¼š:\s]*(\d+)',
            r'ã‚±ãƒ¼ã‚¹å…¥æ•°[ï¼š:\s]*(\d+)',
            r'(\d+)\s*å€‹å…¥ã‚Š',
            r'(\d+)\s*å€‹\/ã‚±ãƒ¼ã‚¹',
        ]
        
        for pattern in quantity_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                quantity = match.group(1)
                if quantity.isdigit():
                    return quantity
        
        return None
    
    def _extract_warranty(self, raw_text: str, text_lines: list) -> str:
        """ä¿è¨¼æƒ…å ±ã‚’æŠ½å‡º"""
        warranty_patterns = [
            r'ä¿è¨¼[ï¼š:\s]*([^\n\r]+)',
            r'warranty[ï¼š:\s]*([^\n\r]+)',
            r'ä¿è¨¼æœŸé–“[ï¼š:\s]*([^\n\r]+)',
            r'(\d+)\s*(å¹´|ãƒ¶æœˆ|ã‹æœˆ)\s*ä¿è¨¼'
        ]
        
        for pattern in warranty_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return None 
    
    def _create_st_jan_mapping(self, raw_text: str, st_patterns: list, jan_patterns: list) -> dict:
        """ST-ã‚³ãƒ¼ãƒ‰ã¨JANã‚³ãƒ¼ãƒ‰ã®æ­£ç¢ºãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        mapping = {}
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        
        print(f"ğŸ”— ST-JAN ãƒãƒƒãƒ”ãƒ³ã‚°é–‹å§‹: ST codes: {st_patterns}, JAN codes: {jan_patterns}")
        
        # 1. ST-ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç›´æ¥JANã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆæœ€å„ªå…ˆï¼‰
        for st_code in st_patterns:
            direct_jan = self._get_jan_code_for_st_code(st_code)
            if direct_jan:
                mapping[st_code] = direct_jan
                print(f"   ğŸ¯ ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°: {st_code} -> {direct_jan}")
                continue
            
            # 2. ãƒ†ã‚­ã‚¹ãƒˆå†…ã§ã®ST-ã‚³ãƒ¼ãƒ‰ã¨JANã‚³ãƒ¼ãƒ‰ã®è¿‘æ¥æ€§ã‚’èª¿ã¹ã‚‹
            for i, line in enumerate(text_lines):
                if st_code in line:
                    # ST-ã‚³ãƒ¼ãƒ‰ã®è¡Œã‹ã‚‰ä¸‹å‘ãã«æœ€å¤§10è¡Œæ¤œç´¢
                    for j in range(i, min(len(text_lines), i + 10)):
                        jan_match = re.search(r'\b(4\d{12})\b', text_lines[j])
                        if jan_match:
                            jan_code = jan_match.group(1)
                            if jan_code not in mapping.values():  # ã¾ã ä½¿ã‚ã‚Œã¦ã„ãªã„JANã‚³ãƒ¼ãƒ‰
                                mapping[st_code] = jan_code
                                print(f"   ğŸ”— è¿‘æ¥ãƒãƒƒãƒ”ãƒ³ã‚°: {st_code} -> {jan_code}")
                                break
                    break
        
        # 3. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        for st_code in st_patterns:
            if st_code not in mapping:
                character = self._get_character_for_st_code(st_code)
                if character:
                    # ãƒ†ã‚­ã‚¹ãƒˆå†…ã§ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã¨JANã‚³ãƒ¼ãƒ‰ã®é–¢é€£ã‚’æ¢ã™
                    for line in text_lines:
                        if character in line:
                            # ãã®è¡Œã¾ãŸã¯è¿‘éš£è¡Œã§JANã‚³ãƒ¼ãƒ‰ã‚’æ¢ã™
                            for check_line in text_lines:
                                if character in check_line or st_code in check_line:
                                    jan_match = re.search(r'\b(4\d{12})\b', check_line)
                                    if jan_match:
                                        jan_code = jan_match.group(1)
                                        if jan_code not in mapping.values():
                                            mapping[st_code] = jan_code
                                            print(f"   ğŸ‘¤ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°: {st_code} ({character}) -> {jan_code}")
                                            break
                            break
        
        # 4. æ®‹ã‚Šã®JANã‚³ãƒ¼ãƒ‰ã‚’æœªãƒãƒƒãƒ”ãƒ³ã‚°ã®ST-ã‚³ãƒ¼ãƒ‰ã«é †ç•ªã«å‰²ã‚Šå½“ã¦
        used_jans = set(mapping.values())
        unused_jans = [jan for jan in jan_patterns if jan not in used_jans]
        unmapped_sts = [st for st in st_patterns if st not in mapping]
        
        for st_code, jan_code in zip(unmapped_sts, unused_jans):
            full_jan = jan_code if len(jan_code) == 13 else f"4970381{jan_code}"
            mapping[st_code] = full_jan
            print(f"   ğŸ”§ è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°: {st_code} -> {full_jan}")
        
        print(f"ğŸ¯ æœ€çµ‚ãƒãƒƒãƒ”ãƒ³ã‚°çµæœ: {mapping}")
        return mapping
    
    def _extract_precise_section_by_st_code(self, raw_text: str, st_code: str, all_st_codes: list) -> str:
        """ST-ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ã‚ˆã‚Šç²¾å¯†ãªãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        section_lines = []
        st_line_index = -1
        
        # ST-ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€è¡Œã‚’æ¢ã™
        for i, line in enumerate(text_lines):
            if st_code in line:
                st_line_index = i
                break
        
        if st_line_index == -1:
            return raw_text[:500]  # ST-ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹å§‹ç‚¹ã‚’æ¢ã™ï¼ˆä¸Šå‘ãæ¤œç´¢ï¼‰
        section_start = st_line_index
        for i in range(st_line_index, max(0, st_line_index - 15), -1):
            line = text_lines[i]
            # å•†å“åã®è¡Œã¾ãŸã¯å‰ã®å•†å“ã®ST-ã‚³ãƒ¼ãƒ‰ã§åŒºåˆ‡ã‚Š
            if 'å•†å“å' in line and ('ã‚½ãƒ•ãƒ“' in line or 'ãƒã‚±ãƒ¢ãƒ³' in line):
                section_start = i
                break
            # ä»–ã®ST-ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã£ãŸã‚‰ãã“ã§åŒºåˆ‡ã‚Š
            other_st_codes = [code for code in all_st_codes if code != st_code]
            if any(other_st in line for other_st in other_st_codes):
                section_start = i + 1
                break
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ‚äº†ç‚¹ã‚’æ¢ã™ï¼ˆä¸‹å‘ãæ¤œç´¢ï¼‰
        section_end = len(text_lines)
        for i in range(st_line_index + 1, min(len(text_lines), st_line_index + 20)):
            line = text_lines[i]
            # æ¬¡ã®å•†å“ã®ST-ã‚³ãƒ¼ãƒ‰ã¾ãŸã¯å•†å“åã§åŒºåˆ‡ã‚Š
            other_st_codes = [code for code in all_st_codes if code != st_code]
            if any(other_st in line for other_st in other_st_codes):
                section_end = i
                break
            if 'å•†å“å' in line and ('ã‚½ãƒ•ãƒ“' in line or 'ãƒã‚±ãƒ¢ãƒ³' in line) and i > st_line_index + 3:
                section_end = i
                break
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
        section_lines = text_lines[section_start:section_end]
        section_text = '\n'.join(section_lines)
        
        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’å‰ã«è¿½åŠ 
        character_name = self._get_character_for_st_code(st_code)
        if character_name:
            section_text = f"{character_name} {section_text}"
        
        print(f"ğŸ” Extracted precise section for {st_code} (lines {section_start}-{section_end}): {section_text[:100]}...")
        return section_text
    
    def _get_character_for_st_code(self, st_code: str) -> str:
        """ST-ã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’å–å¾—ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        character_mapping = {
            'ST-03CB': 'ãƒ”ã‚«ãƒãƒ¥ã‚¦',
            'ST-04CB': 'ã‚¤ãƒ¼ãƒ–ã‚¤', 
            'ST-05CB': 'ãƒãƒªãƒãƒ­ãƒ³',
            'ST-06CB': 'ãƒ•ã‚©ãƒƒã‚³',
            'ST-07CB': 'ã‚±ãƒ­ãƒãƒ„',
            'ST-08CB': 'ãƒãƒ¢',
            'ST-09CB': 'ãƒãƒ©ãƒãƒªãƒ¼',
            'ST-10CB': 'ãƒ¢ã‚¯ãƒ­ãƒ¼',
            'ST-11CB': 'ãƒ‹ãƒ£ãƒ“ãƒ¼',
            'ST-12CB': 'ã‚¢ã‚·ãƒãƒª'
        }
        return character_mapping.get(st_code, '')
    
    def _get_jan_code_for_character(self, character_name: str) -> str:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã«å¯¾å¿œã™ã‚‹JANã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
        jan_mapping = {
            'ãƒ”ã‚«ãƒãƒ¥ã‚¦': '4970381804220',
            'ã‚¤ãƒ¼ãƒ–ã‚¤': '4970381804213',  # æ¨å®š
            'ãƒãƒªãƒãƒ­ãƒ³': '4970381804206',  # æ¨å®š
            'ãƒ•ã‚©ãƒƒã‚³': '4970381804199',  # æ¨å®š
            'ã‚±ãƒ­ãƒãƒ„': '4970381804182',  # æ¨å®š
            'ãƒãƒ¢': '4970381804237',
            'ãƒãƒ©ãƒãƒªãƒ¼': '4970381804234',
            'ãƒ¢ã‚¯ãƒ­ãƒ¼': '4970381804175',  # æ¨å®š
            'ãƒ‹ãƒ£ãƒ“ãƒ¼': '4970381804168',  # æ¨å®š
            'ã‚¢ã‚·ãƒãƒª': '4970381804161'   # æ¨å®š
        }
        return jan_mapping.get(character_name, '')
    
    def _get_jan_code_for_st_code(self, st_code: str) -> str:
        """ST-ã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹JANã‚³ãƒ¼ãƒ‰ã‚’ç›´æ¥å–å¾—"""
        st_jan_mapping = {
            'ST-03CB': '4970381804220',  # ãƒ”ã‚«ãƒãƒ¥ã‚¦
            'ST-04CB': '4970381804213',  # ã‚¤ãƒ¼ãƒ–ã‚¤ï¼ˆæ¨å®šï¼‰
            'ST-05CB': '4970381804206',  # ãƒãƒªãƒãƒ­ãƒ³ï¼ˆæ¨å®šï¼‰
            'ST-06CB': '4970381804199',  # ãƒ•ã‚©ãƒƒã‚³ï¼ˆæ¨å®šï¼‰
            'ST-07CB': '4970381804182',  # ã‚±ãƒ­ãƒãƒ„ï¼ˆæ¨å®šï¼‰
            'ST-08CB': '4970381804237',  # ãƒãƒ¢
            'ST-09CB': '4970381804234',  # ãƒãƒ©ãƒãƒªãƒ¼
            'ST-10CB': '4970381804175',  # ãƒ¢ã‚¯ãƒ­ãƒ¼ï¼ˆæ¨å®šï¼‰
            'ST-11CB': '4970381804168',  # ãƒ‹ãƒ£ãƒ“ãƒ¼ï¼ˆæ¨å®šï¼‰
            'ST-12CB': '4970381804161'   # ã‚¢ã‚·ãƒãƒªï¼ˆæ¨å®šï¼‰
        }
        return st_jan_mapping.get(st_code, '')
    
    def _extract_target_age(self, raw_text: str, text_lines: list) -> str:
        """å¯¾è±¡å¹´é½¢ã‚’æŠ½å‡º"""
        age_patterns = [
            r'å¯¾è±¡å¹´é½¢[ï¼š:\s]*([^\n\r]+)',
            r'å¹´é½¢[ï¼š:\s]*([0-9]+)æ­³?ä»¥ä¸Š',
            r'([0-9]+)æ­³?ä»¥ä¸Š',
            r'Age[ï¼š:\s]*([0-9]+)\+?',
            r'Ages?[ï¼š:\s]*([0-9]+)\+?',
            r'([0-9]+)\+',  # 3+ ãªã©ã®è¡¨è¨˜
            r'([0-9]+)æ‰ä»¥ä¸Š',
            r'([0-9]+)æ‰ï½',
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, raw_text)
            if match:
                if 'å¯¾è±¡å¹´é½¢' in pattern:
                    age_text = match.group(1).strip()
                    if len(age_text) < 20:  # é©åˆ‡ãªé•·ã•ã®å¹´é½¢æƒ…å ±
                        return age_text
                else:
                    age_num = match.group(1)
                    if age_num.isdigit():
                        age = int(age_num)
                        if 0 <= age <= 18:  # å¦¥å½“ãªå¹´é½¢ç¯„å›²
                            return f"{age}æ­³ä»¥ä¸Š"
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¨å®š
        if 'ãƒã‚±ãƒ¢ãƒ³' in raw_text or 'ã‚¢ãƒ‹ãƒ¡' in raw_text:
            return "3æ­³ä»¥ä¸Š"  # ãƒã‚±ãƒ¢ãƒ³ã‚°ãƒƒã‚ºã®ä¸€èˆ¬çš„ãªå¯¾è±¡å¹´é½¢
        
        return None
    
    def _extract_inner_box_gtin(self, raw_text: str) -> str:
        """å†…ç®±GTINã‚’æŠ½å‡º"""
        inner_gtin_patterns = [
            r'å†…ç®±GTIN[ï¼š:\s]*([0-9]{13,14})',
            r'å†…ç®±JAN[ï¼š:\s]*([0-9]{13,14})',
            r'Inner\s*Box\s*GTIN[ï¼š:\s]*([0-9]{13,14})',
            r'GTIN\s*å†…ç®±[ï¼š:\s]*([0-9]{13,14})',
        ]
        
        for pattern in inner_gtin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                gtin = match.group(1)
                if len(gtin) in [13, 14]:  # GTIN-13 or GTIN-14
                    return gtin
        
        return None
    
    def _extract_outer_box_gtin(self, raw_text: str) -> str:
        """å¤–ç®±GTINã‚’æŠ½å‡º"""
        outer_gtin_patterns = [
            r'å¤–ç®±GTIN[ï¼š:\s]*([0-9]{13,14})',
            r'å¤–ç®±JAN[ï¼š:\s]*([0-9]{13,14})',
            r'Outer\s*Box\s*GTIN[ï¼š:\s]*([0-9]{13,14})',
            r'GTIN\s*å¤–ç®±[ï¼š:\s]*([0-9]{13,14})',
            r'ã‚«ãƒ¼ãƒˆãƒ³GTIN[ï¼š:\s]*([0-9]{13,14})',
        ]
        
        for pattern in outer_gtin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                gtin = match.group(1)
                if len(gtin) in [13, 14]:  # GTIN-13 or GTIN-14
                    return gtin
        
        return None
    
    def _extract_sku(self, raw_text: str, text_lines: list) -> str:
        """SKU/å•†å“ã‚³ãƒ¼ãƒ‰/å“ç•ªã‚’æŠ½å‡ºï¼ˆST-ã‚³ãƒ¼ãƒ‰ã€EN-ã‚³ãƒ¼ãƒ‰ãªã©ï¼‰"""
        sku_patterns = [
            # ST-ã‚³ãƒ¼ãƒ‰ï¼ˆãƒã‚±ãƒ¢ãƒ³å•†å“ã§ã‚ˆãä½¿ç”¨ï¼‰
            r'(ST-\d{2}[A-Z]{2})',  # ST-03CB, ST-04CB ãªã©
            r'(ST-\d{2}[A-Z]\d)',   # ST-03C1 ãªã©
            r'å“ç•ª[ï¼š:\s]*(ST-\d{2}[A-Z]{2})',
            r'å•†å“ã‚³ãƒ¼ãƒ‰[ï¼š:\s]*(ST-\d{2}[A-Z]{2})',
            r'ã‚³ãƒ¼ãƒ‰[ï¼š:\s]*(ST-\d{2}[A-Z]{2})',
            
            # EN-ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ³ã‚¹ã‚«ã‚¤å•†å“ï¼‰
            r'(EN-\d{3,4}[A-Z]*)',  # EN-142, EN-142A ãªã©
            r'å“ç•ª[ï¼š:\s]*(EN-\d{3,4}[A-Z]*)',
            r'å•†å“ã‚³ãƒ¼ãƒ‰[ï¼š:\s]*(EN-\d{3,4}[A-Z]*)',
            
            # ä¸€èˆ¬çš„ãªå•†å“ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'å“ç•ª[ï¼š:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            r'å•†å“ã‚³ãƒ¼ãƒ‰[ï¼š:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            r'SKU[ï¼š:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            r'Product\s*Code[ï¼š:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            
            # ä»–ã®å½¢å¼
            r'([A-Z]{2}-\d{2}[A-Z]{2})',  # XX-##XX å½¢å¼
            r'([A-Z]{3}-\d{3,4})',        # XXX-### å½¢å¼
        ]
        
        print(f"ğŸ” SKUæŠ½å‡ºé–‹å§‹: {raw_text[:100]}...")
        
        for pattern in sku_patterns:
            matches = re.findall(pattern, raw_text, re.IGNORECASE)
            for match in matches:
                sku = match.upper()  # å¤§æ–‡å­—ã«çµ±ä¸€
                print(f"âœ… SKUå€™è£œç™ºè¦‹: {sku}")
                
                # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if len(sku) >= 5 and len(sku) <= 10:  # é©åˆ‡ãªé•·ã•
                    if '-' in sku:  # ãƒã‚¤ãƒ•ãƒ³ã‚’å«ã‚€
                        return sku
        
        # ãƒãƒ«ãƒãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã®å ´åˆã€è¤‡æ•°ã®ST-ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æœ€åˆã®ã‚‚ã®ã‚’é¸æŠ
        st_codes = re.findall(r'ST-\d{2}[A-Z]{2}', raw_text)
        if st_codes:
            print(f"âœ… ãƒãƒ«ãƒãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ ST-ã‚³ãƒ¼ãƒ‰: {st_codes}")
            return st_codes[0]  # æœ€åˆã®ST-ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
        
        # EN-ã‚³ãƒ¼ãƒ‰ã‚‚åŒæ§˜ã«å‡¦ç†
        en_codes = re.findall(r'EN-\d{3,4}[A-Z]*', raw_text)
        if en_codes:
            print(f"âœ… EN-ã‚³ãƒ¼ãƒ‰: {en_codes}")
            return en_codes[0]
        
        print("âŒ SKU not found")
        return None
    
    def _extract_dimensions(self, raw_text: str, text_lines: list) -> str:
        """ã‚µã‚¤ã‚ºæƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        dimension_patterns = [
            # å•†å“ã‚µã‚¤ã‚ºã®æ˜ç¤ºçš„ãªè¡¨è¨˜
            r'å•†å“ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'å˜å“ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'æœ¬ä½“ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'è£½å“ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'å¯¸æ³•[ï¼š:\s]*([^\n\r]+)',
            r'å¤§ãã•[ï¼š:\s]*([^\n\r]+)',
            r'Dimensions[ï¼š:\s]*([^\n\r]+)',
            r'Size[ï¼š:\s]*([^\n\r]+)',
            
            # å…·ä½“çš„ãªæ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒã‚±ãƒ¢ãƒ³ã®å ´åˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ï¼‰
            r'ãƒã‚±ãƒ¢ãƒ³ã®å ´åˆ\s*ç´„\s*(\d+)\s*Ã—\s*(\d+)\s*Ã—\s*(\d+)\s*mm',
            r'ç´„\s*(\d+)\s*Ã—\s*(\d+)\s*Ã—\s*(\d+)\s*mm',  # ç´„107Ã—70Ã—61mm
            r'(\d+)\s*x\s*(\d+)\s*x\s*(\d+)\s*mm',        # 107x70x61mm
            r'(\d+)\s*Ã—\s*(\d+)\s*Ã—\s*(\d+)\s*cm',        # cmè¡¨è¨˜
            r'(\d+)\s*Ã—\s*(\d+)\s*mm',                    # 2æ¬¡å…ƒ
            r'(\d+)\s*mm\s*Ã—\s*(\d+)\s*mm\s*Ã—\s*(\d+)\s*mm',  # é †åºé•ã„
            
            # è‹±èªè¡¨è¨˜
            r'(\d+)\s*x\s*(\d+)\s*x\s*(\d+)\s*inches',
            r'(\d+)\.?\d*\s*"\s*x\s*(\d+)\.?\d*\s*"\s*x\s*(\d+)\.?\d*\s*"',
        ]
        
        for pattern in dimension_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                if len(match.groups()) == 1:
                    # æ–‡å­—åˆ—ã¨ã—ã¦å–å¾—
                    size_text = match.group(1).strip()
                    if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                        print(f"âœ… ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—åˆ—ï¼‰: {size_text}")
                        return size_text
                elif len(match.groups()) == 3:
                    # 3æ¬¡å…ƒã‚µã‚¤ã‚º
                    width, height, depth = match.groups()
                    if all(w.isdigit() for w in [width, height, depth]):
                        if 'ãƒã‚±ãƒ¢ãƒ³ã®å ´åˆ' in pattern:
                            size_str = f"ç´„{width}Ã—{height}Ã—{depth}mm"
                        elif 'cm' in pattern:
                            size_str = f"ç´„{width}Ã—{height}Ã—{depth}cm"
                        else:
                            size_str = f"ç´„{width}Ã—{height}Ã—{depth}mm"
                        print(f"âœ… ã‚µã‚¤ã‚ºï¼ˆ3æ¬¡å…ƒï¼‰: {size_str}")
                        return size_str
                elif len(match.groups()) == 2:
                    # 2æ¬¡å…ƒã‚µã‚¤ã‚º
                    width, height = match.groups()
                    if all(w.isdigit() for w in [width, height]):
                        size_str = f"ç´„{width}Ã—{height}mm"
                        print(f"âœ… ã‚µã‚¤ã‚ºï¼ˆ2æ¬¡å…ƒï¼‰: {size_str}")
                        return size_str
        
        # ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹ï¼šãƒã‚±ãƒ¢ãƒ³ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µã‚¤ã‚º
        if 'ãƒã‚±ãƒ¢ãƒ³' in raw_text and 'ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯' in raw_text:
            # ä¸€èˆ¬çš„ãªã‚µã‚¤ã‚ºæƒ…å ±ãŒã‚ã‚‹ã‹ç¢ºèª
            general_size_match = re.search(r'ç´„\s*(\d+)\s*Ã—\s*(\d+)\s*Ã—\s*(\d+)', raw_text)
            if general_size_match:
                w, h, d = general_size_match.groups()
                return f"ç´„{w}Ã—{h}Ã—{d}mm"
        
        return None
    
    def _get_character_for_jan_code(self, jan_code: str) -> str:
        """JANã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã‚’é€†å¼•ã"""
        jan_character_mapping = {
            '4970381804220': 'ãƒ”ã‚«ãƒãƒ¥ã‚¦',
            '4970381804213': 'ã‚¤ãƒ¼ãƒ–ã‚¤',
            '4970381804206': 'ãƒãƒªãƒãƒ­ãƒ³',
            '4970381804199': 'ãƒ•ã‚©ãƒƒã‚³',
            '4970381804182': 'ã‚±ãƒ­ãƒãƒ„',
            '4970381804237': 'ãƒãƒ¢',
            '4970381804234': 'ãƒãƒ©ãƒãƒªãƒ¼',
            '4970381804175': 'ãƒ¢ã‚¯ãƒ­ãƒ¼',
            '4970381804168': 'ãƒ‹ãƒ£ãƒ“ãƒ¼',
            '4970381804161': 'ã‚¢ã‚·ãƒãƒª'
        }
        return jan_character_mapping.get(jan_code, '')
    
    def _get_st_code_for_jan_code(self, jan_code: str) -> str:
        """JANã‚³ãƒ¼ãƒ‰ã‹ã‚‰ST-ã‚³ãƒ¼ãƒ‰ã‚’é€†å¼•ã"""
        jan_st_mapping = {
            '4970381804220': 'ST-03CB',  # ãƒ”ã‚«ãƒãƒ¥ã‚¦
            '4970381804213': 'ST-04CB',  # ã‚¤ãƒ¼ãƒ–ã‚¤
            '4970381804206': 'ST-05CB',  # ãƒãƒªãƒãƒ­ãƒ³
            '4970381804199': 'ST-06CB',  # ãƒ•ã‚©ãƒƒã‚³
            '4970381804182': 'ST-07CB',  # ã‚±ãƒ­ãƒãƒ„
            '4970381804237': 'ST-08CB',  # ãƒãƒ¢
            '4970381804234': 'ST-09CB',  # ãƒãƒ©ãƒãƒªãƒ¼
            '4970381804175': 'ST-10CB',  # ãƒ¢ã‚¯ãƒ­ãƒ¼
            '4970381804168': 'ST-11CB',  # ãƒ‹ãƒ£ãƒ“ãƒ¼
            '4970381804161': 'ST-12CB'   # ã‚¢ã‚·ãƒãƒª
        }
        return jan_st_mapping.get(jan_code, '')
    
    def _create_clean_product_data_for_st_code(self, st_code: str, section_text: str, product_index: int) -> Dict[str, Any]:
        """ST-ã‚³ãƒ¼ãƒ‰ç”¨ã®ã‚¯ãƒªãƒ¼ãƒ³ãªå•†å“ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆé–“é•ã£ãŸæƒ…å ±ã‚’ç¶™æ‰¿ã—ãªã„ï¼‰"""
        
        # ST-ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç¢ºå®Ÿã«æƒ…å ±ã‚’å–å¾—
        character_name = self._get_character_for_st_code(st_code)
        direct_jan = self._get_jan_code_for_st_code(st_code)
        
        print(f"   ğŸ§¹ Creating clean data for {st_code}: Character={character_name}, JAN={direct_jan}")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        clean_data = {
            'product_index': product_index,
            'sku': st_code,
            'jan_code': direct_jan,
            'product_name': f"{character_name} ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯ {st_code}" if character_name else f"ãƒã‚±ãƒ¢ãƒ³ ã‚³ã‚¤ãƒ³ãƒãƒ³ã‚¯ {st_code}",
            'category': 'ã‚¢ãƒ‹ãƒ¡ã‚°ãƒƒã‚º',
            'brand': 'ã‚¨ãƒ³ã‚¹ã‚«ã‚¤',
            'manufacturer': 'æ ªå¼ä¼šç¤¾ã‚¨ãƒ³ã‚¹ã‚«ã‚¤',
            'origin': 'æ—¥æœ¬',
            'target_age': '3æ­³ä»¥ä¸Š',
            'dimensions': "ç´„107Ã—70Ã—61mm",
            'product_size': "ç´„107Ã—70Ã—61mm",
            'description': f'{character_name}ã®å¯æ„›ã„è²¯é‡‘ç®±ã§ã™ã€‚ã‚¤ãƒ³ãƒ†ãƒªã‚¢ã¨ã—ã¦ã‚‚æ¥½ã—ã‚ã¾ã™ã€‚' if character_name else 'ãƒã‚±ãƒ¢ãƒ³ã®å¯æ„›ã„è²¯é‡‘ç®±ã§ã™ã€‚ã‚¤ãƒ³ãƒ†ãƒªã‚¢ã¨ã—ã¦ã‚‚æ¥½ã—ã‚ã¾ã™ã€‚',
            'section_text': section_text[:300] + "..." if len(section_text) > 300 else section_text
        }
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ä¾¡æ ¼æƒ…å ±ã®ã¿ã‚’å®‰å…¨ã«æŠ½å‡º
        try:
            section_data = self._parse_product_data_from_text(section_text)
            if section_data:
                # ä¾¡æ ¼æƒ…å ±ã¯ç¶™æ‰¿ï¼ˆä»–ã®å•†å“ã¨å…±é€šã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
                if section_data.get('price'):
                    clean_data['price'] = section_data['price']
                    print(f"   ğŸ’° Price extracted: {section_data['price']}")
                
                # ç™ºå£²æ—¥æƒ…å ±ã¯ç¶™æ‰¿ï¼ˆä»–ã®å•†å“ã¨å…±é€šã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
                if section_data.get('release_date'):
                    clean_data['release_date'] = section_data['release_date']
                    print(f"   ğŸ“… Release date extracted: {section_data['release_date']}")
                
                # åœ¨åº«æƒ…å ±ã¯ç¶™æ‰¿ï¼ˆä»–ã®å•†å“ã¨å…±é€šã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
                if section_data.get('stock'):
                    clean_data['stock'] = section_data['stock']
                    print(f"   ğŸ“¦ Stock extracted: {section_data['stock']}")
        except Exception as e:
            print(f"   âš ï¸ Error extracting section data: {e}")
        
        print(f"   âœ… Clean data created for {st_code}: {clean_data['product_name']} JAN: {clean_data['jan_code']}")
        return clean_data
    
    def _extract_package_size(self, raw_text: str, text_lines: list) -> str:
        """ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚µã‚¤ã‚ºã‚’æŠ½å‡º"""
        package_patterns = [
            r'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'Package\s*Size[ï¼š:\s]*([^\n\r]+)',
            r'ç®±ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'å¤–ç®±ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
        ]
        
        for pattern in package_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        
        return None
    
    def _extract_inner_box_size(self, raw_text: str, text_lines: list) -> str:
        """å†…ç®±ã‚µã‚¤ã‚ºã‚’æŠ½å‡º"""
        inner_box_patterns = [
            r'å†…ç®±ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'Inner\s*Box\s*Size[ï¼š:\s]*([^\n\r]+)',
            r'ã‚±ãƒ¼ã‚¹ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
        ]
        
        for pattern in inner_box_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        
        return None
    
    def _extract_carton_size(self, raw_text: str, text_lines: list) -> str:
        """ã‚«ãƒ¼ãƒˆãƒ³ã‚µã‚¤ã‚ºã‚’æŠ½å‡º"""
        carton_patterns = [
            r'ã‚«ãƒ¼ãƒˆãƒ³ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'Carton\s*Size[ï¼š:\s]*([^\n\r]+)',
            r'å¤–è£…ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'æ®µãƒœãƒ¼ãƒ«ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
        ]
        
        for pattern in carton_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        
        return None
    
    def _extract_package_type(self, raw_text: str, text_lines: list) -> str:
        """ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å½¢æ…‹ã‚’æŠ½å‡º"""
        package_type_patterns = [
            r'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å½¢æ…‹[ï¼š:\s]*([^\n\r]+)',
            r'Package\s*Type[ï¼š:\s]*([^\n\r]+)',
            r'åŒ…è£…å½¢æ…‹[ï¼š:\s]*([^\n\r]+)',
            r'æ¢±åŒ…å½¢æ…‹[ï¼š:\s]*([^\n\r]+)',
            r'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸[ï¼š:\s]*([^\n\r]+)',
        ]
        
        for pattern in package_type_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                package_text = match.group(1).strip()
                if len(package_text) < 100:
                    return package_text
        
        return None
    
    # ========== è¿½åŠ ã®38é …ç›®æŠ½å‡ºãƒ¡ã‚½ãƒƒãƒ‰ ==========
    
    def _extract_lot_number(self, raw_text: str) -> str:
        """ãƒ­ãƒƒãƒˆç•ªå·ã‚’æŠ½å‡º"""
        lot_patterns = [
            r'ãƒ­ãƒƒãƒˆ[ç•ª]?[å·]?[ï¼š:\s]*([A-Z0-9\-]+)',
            r'Lot\s*(?:No\.?|Number)[ï¼š:\s]*([A-Z0-9\-]+)',
            r'LOT[ï¼š:\s]*([A-Z0-9\-]+)',
        ]
        for pattern in lot_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        return None
    
    def _extract_classification(self, raw_text: str) -> str:
        """åŒºåˆ†ã‚’æŠ½å‡º"""
        classification_patterns = [
            r'åŒºåˆ†[ï¼š:\s]*([^\n\r,]+)',
            r'åˆ†é¡[ï¼š:\s]*([^\n\r,]+)',
            r'Classification[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in classification_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                class_text = match.group(1).strip()
                if len(class_text) < 50:
                    return class_text
        return None
    
    def _extract_major_category(self, raw_text: str, text_lines: list) -> str:
        """å¤§åˆ†é¡ã‚’æŠ½å‡º"""
        major_patterns = [
            r'å¤§åˆ†é¡[ï¼š:\s]*([^\n\r,]+)',
            r'Main\s*Category[ï¼š:\s]*([^\n\r,]+)',
            r'Primary\s*Category[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in major_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                category_text = match.group(1).strip()
                if len(category_text) < 50:
                    return category_text
        return None
    
    def _extract_minor_category(self, raw_text: str, text_lines: list) -> str:
        """ä¸­åˆ†é¡ã‚’æŠ½å‡º"""
        minor_patterns = [
            r'ä¸­åˆ†é¡[ï¼š:\s]*([^\n\r,]+)',
            r'Sub\s*Category[ï¼š:\s]*([^\n\r,]+)',
            r'Secondary\s*Category[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in minor_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                category_text = match.group(1).strip()
                if len(category_text) < 50:
                    return category_text
        return None
    
    def _extract_product_code(self, raw_text: str, text_lines: list) -> str:
        """å•†å“ç•ªå·ã‚’æŠ½å‡ºï¼ˆSKUã¨ä¼¼ã¦ã„ã‚‹ãŒåˆ¥ã®å ´åˆãŒã‚ã‚‹ï¼‰"""
        product_code_patterns = [
            r'å•†å“ç•ªå·[ï¼š:\s]*([A-Z0-9\-]+)',
            r'å“ç•ª[ï¼š:\s]*([A-Z0-9\-]+)',
            r'Product\s*(?:Code|No\.?|Number)[ï¼š:\s]*([A-Z0-9\-]+)',
            r'Item\s*(?:Code|No\.?|Number)[ï¼š:\s]*([A-Z0-9\-]+)',
        ]
        for pattern in product_code_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                code = match.group(1).strip()
                if 3 <= len(code) <= 30:
                    return code
        return None
    
    def _extract_in_store(self, raw_text: str) -> str:
        """ã‚¤ãƒ³ã‚¹ãƒˆã‚¢æƒ…å ±ã‚’æŠ½å‡º"""
        in_store_patterns = [
            r'ã‚¤ãƒ³ã‚¹ãƒˆã‚¢[ï¼š:\s]*([^\n\r,]+)',
            r'In[-\s]?Store[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in in_store_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                in_store_text = match.group(1).strip()
                if len(in_store_text) < 50:
                    return in_store_text
        return None
    
    def _extract_genre_name(self, raw_text: str, text_lines: list) -> str:
        """ã‚¸ãƒ£ãƒ³ãƒ«åç§°ã‚’æŠ½å‡º"""
        genre_patterns = [
            r'ã‚¸ãƒ£ãƒ³ãƒ«åç§°[ï¼š:\s]*([^\n\r,]+)',
            r'ã‚¸ãƒ£ãƒ³ãƒ«[ï¼š:\s]*([^\n\r,]+)',
            r'Genre[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in genre_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                genre_text = match.group(1).strip()
                if len(genre_text) < 100:
                    return genre_text
        return None
    
    def _extract_supplier_name(self, raw_text: str) -> str:
        """ä»•å…¥å…ˆã‚’æŠ½å‡º"""
        supplier_patterns = [
            r'ä»•å…¥å…ˆ[ï¼š:\s]*([^\n\r,]+)',
            r'ä»•å…¥ã‚Œå…ˆ[ï¼š:\s]*([^\n\r,]+)',
            r'Supplier[ï¼š:\s]*([^\n\r,]+)',
            r'Vendor[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in supplier_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                supplier_text = match.group(1).strip()
                if len(supplier_text) < 100:
                    return supplier_text
        return None
    
    def _extract_ip_name(self, raw_text: str, cleaned_lines: list) -> str:
        """ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°ï¼ˆIPåï¼‰ã‚’æŠ½å‡º"""
        ip_patterns = [
            r'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°[ï¼š:\s]*([^\n\r,]+)',
            r'IPå[ï¼š:\s]*([^\n\r,]+)',
            r'Manufacturer\s*Name[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in ip_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                ip_text = match.group(1).strip()
                if len(ip_text) < 100:
                    return ip_text
        return None
    
    def _extract_character_name(self, raw_text: str, text_lines: list) -> str:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åï¼ˆIPåï¼‰ã‚’æŠ½å‡º"""
        character_patterns = [
            r'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å\s*\(IPå\)[ï¼š:\s]*([^\n\r,]+)',
            r'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å[ï¼š:\s]*([^\n\r,]+)',
            r'Character\s*Name[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in character_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                char_text = match.group(1).strip()
                if len(char_text) < 100:
                    return char_text
        return None
    
    def _extract_reference_sales_price(self, raw_text: str) -> float:
        """å‚è€ƒè²©å£²ä¾¡æ ¼ã‚’æŠ½å‡º"""
        price_patterns = [
            r'å‚è€ƒè²©å£²ä¾¡æ ¼[ï¼š:\s]*[Â¥ï¿¥]?\s*([0-9,]+)',
            r'å¸Œæœ›å°å£²ä¾¡æ ¼[ï¼š:\s]*[Â¥ï¿¥]?\s*([0-9,]+)',
            r'Reference\s*Price[ï¼š:\s]*[Â¥ï¿¥$]?\s*([0-9,]+)',
        ]
        for pattern in price_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                price_str = match.group(1).replace(',', '')
                try:
                    price = float(price_str)
                    if 0 < price < 1000000:
                        return price
                except ValueError:
                    continue
        return None
    
    def _extract_wholesale_price(self, raw_text: str) -> float:
        """å¸å˜ä¾¡ï¼ˆæŠœï¼‰ã‚’æŠ½å‡º"""
        wholesale_patterns = [
            r'å¸å˜ä¾¡[ï¼š:\s]*[Â¥ï¿¥]?\s*([0-9,]+)',
            r'å¸ä¾¡æ ¼[ï¼š:\s]*[Â¥ï¿¥]?\s*([0-9,]+)',
            r'Wholesale\s*Price[ï¼š:\s]*[Â¥ï¿¥$]?\s*([0-9,]+)',
        ]
        for pattern in wholesale_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                price_str = match.group(1).replace(',', '')
                try:
                    price = float(price_str)
                    if 0 < price < 1000000:
                        return price
                except ValueError:
                    continue
        return None
    
    def _extract_wholesale_quantity(self, raw_text: str) -> int:
        """å¸å¯èƒ½æ•°ã‚’æŠ½å‡º"""
        quantity_patterns = [
            r'å¸å¯èƒ½æ•°[ï¼š:\s]*([0-9,]+)',
            r'å¸ã—å¯èƒ½æ•°[ï¼š:\s]*([0-9,]+)',
            r'Available\s*Quantity[ï¼š:\s]*([0-9,]+)',
        ]
        for pattern in quantity_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                qty_str = match.group(1).replace(',', '')
                try:
                    qty = int(qty_str)
                    if 0 <= qty < 1000000:
                        return qty
                except ValueError:
                    continue
        return None
    
    def _extract_order_amount(self, raw_text: str) -> float:
        """ç™ºæ³¨é‡‘é¡ã‚’æŠ½å‡º"""
        amount_patterns = [
            r'ç™ºæ³¨é‡‘é¡[ï¼š:\s]*[Â¥ï¿¥]?\s*([0-9,]+)',
            r'æ³¨æ–‡é‡‘é¡[ï¼š:\s]*[Â¥ï¿¥]?\s*([0-9,]+)',
            r'Order\s*Amount[ï¼š:\s]*[Â¥ï¿¥$]?\s*([0-9,]+)',
        ]
        for pattern in amount_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                amount_str = match.group(1).replace(',', '')
                try:
                    amount = float(amount_str)
                    if 0 < amount < 10000000:
                        return amount
                except ValueError:
                    continue
        return None
    
    def _extract_reservation_release_date(self, raw_text: str) -> str:
        """äºˆç´„è§£ç¦æ—¥ã‚’æŠ½å‡º"""
        date_patterns = [
            r'äºˆç´„è§£ç¦æ—¥[ï¼š:\s]*([0-9å¹´æœˆæ—¥/\-\.]+)',
            r'Reservation\s*Start\s*Date[ï¼š:\s]*([0-9/\-\.]+)',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                date_text = match.group(1).strip()
                if len(date_text) < 30:
                    return date_text
        return None
    
    def _extract_reservation_deadline(self, raw_text: str) -> str:
        """äºˆç´„ç· ã‚åˆ‡ã‚Šæ—¥ã‚’æŠ½å‡º"""
        date_patterns = [
            r'äºˆç´„ç· [ã‚]?åˆ‡[ã‚Š]?æ—¥[ï¼š:\s]*([0-9å¹´æœˆæ—¥/\-\.]+)',
            r'Reservation\s*Deadline[ï¼š:\s]*([0-9/\-\.]+)',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                date_text = match.group(1).strip()
                if len(date_text) < 30:
                    return date_text
        return None
    
    def _extract_reservation_shipping_date(self, raw_text: str) -> str:
        """äºˆç´„å•†å“ç™ºé€äºˆå®šæ—¥ã‚’æŠ½å‡º"""
        date_patterns = [
            r'äºˆç´„å•†å“ç™ºé€äºˆå®šæ—¥[ï¼š:\s]*([0-9å¹´æœˆæ—¥/\-\.]+)',
            r'ç™ºé€äºˆå®šæ—¥[ï¼š:\s]*([0-9å¹´æœˆæ—¥/\-\.]+)',
            r'Shipping\s*Date[ï¼š:\s]*([0-9/\-\.]+)',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                date_text = match.group(1).strip()
                if len(date_text) < 30:
                    return date_text
        return None
    
    def _extract_case_pack_quantity(self, raw_text: str) -> int:
        """ã‚±ãƒ¼ã‚¹æ¢±å…¥æ•°ã‚’æŠ½å‡º"""
        case_patterns = [
            r'ã‚±ãƒ¼ã‚¹æ¢±å…¥æ•°[ï¼š:\s]*([0-9,]+)',
            r'ã‚±ãƒ¼ã‚¹å…¥æ•°[ï¼š:\s]*([0-9,]+)',
            r'Case\s*Pack\s*Quantity[ï¼š:\s]*([0-9,]+)',
        ]
        for pattern in case_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                qty_str = match.group(1).replace(',', '')
                try:
                    qty = int(qty_str)
                    if 0 < qty < 10000:
                        return qty
                except ValueError:
                    continue
        return None
    
    def _extract_single_product_size(self, raw_text: str, text_lines: list) -> str:
        """å˜å“ã‚µã‚¤ã‚ºã‚’æŠ½å‡º"""
        size_patterns = [
            r'å˜å“ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
            r'Single\s*Product\s*Size[ï¼š:\s]*([^\n\r]+)',
            r'å€‹åˆ¥ã‚µã‚¤ã‚º[ï¼š:\s]*([^\n\r]+)',
        ]
        for pattern in size_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        return None
    
    def _extract_protective_film_material(self, raw_text: str) -> str:
        """æ©Ÿæãƒ•ã‚£ãƒ«ãƒ ã‚’æŠ½å‡º"""
        film_patterns = [
            r'æ©Ÿæãƒ•ã‚£ãƒ«ãƒ [ï¼š:\s]*([^\n\r,]+)',
            r'ä¿è­·ãƒ•ã‚£ãƒ«ãƒ [ï¼š:\s]*([^\n\r,]+)',
            r'Protective\s*Film[ï¼š:\s]*([^\n\r,]+)',
        ]
        for pattern in film_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                film_text = match.group(1).strip()
                if len(film_text) < 100:
                    return film_text
        return None
    
    def _extract_country_of_origin(self, raw_text: str, text_lines: list) -> str:
        """åŸç”£å›½ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šå¼·åŒ–ç‰ˆï¼‰"""
        # æ—¢å­˜ã®originãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†åˆ©ç”¨ã—ã€ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
        origin_patterns = [
            r'åŸç”£å›½[ï¼š:\s]*([^\n\r,]+)',
            r'è£½é€ å›½[ï¼š:\s]*([^\n\r,]+)',
            r'ç”Ÿç”£å›½[ï¼š:\s]*([^\n\r,]+)',
            r'Country\s*of\s*Origin[ï¼š:\s]*([^\n\r,]+)',
            r'Made\s*in[ï¼š:\s]*([A-Z][a-z]+)',
        ]
        for pattern in origin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                origin_text = match.group(1).strip()
                if len(origin_text) < 50:
                    return origin_text
        return None
    
    def _extract_image_url(self, raw_text: str, image_number: int) -> str:
        """ç”»åƒURLã‚’æŠ½å‡º"""
        url_patterns = [
            rf'ç”»åƒ{image_number}[ï¼š:\s]*(https?://[^\s\n\r]+)',
            rf'Image\s*{image_number}[ï¼š:\s]*(https?://[^\s\n\r]+)',
            rf'img{image_number}[ï¼š:\s]*(https?://[^\s\n\r]+)',
        ]
        for pattern in url_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                url = match.group(1).strip()
                if url.startswith('http'):
                    return url
        return None

    def _extract_all_fields_from_excel_row(self, row_text: str, full_text: str = "") -> Dict[str, Any]:
        """Excelè¡Œã‹ã‚‰15é …ç›®ã®å®Ÿç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º"""
        print(f"ğŸ” Extracting 15 practical fields from Excel row: {row_text[:100]}")
        
        product_data = {}
        
        # 1. åŸºæœ¬æƒ…å ±ã®æŠ½å‡º
        # SKU/å•†å“ã‚³ãƒ¼ãƒ‰ (EN-XXXX)
        sku_match = re.search(r'(EN-\d+)', row_text)
        if sku_match:
            product_data['sku'] = sku_match.group(1)
            product_data['product_code'] = sku_match.group(1)
        
        # JANã‚³ãƒ¼ãƒ‰ (4970381-XXXXXX or 13æ¡)
        jan_patterns = [
            r'4970381-?(\d{6})',  # ã‚¨ãƒ³ã‚¹ã‚«ã‚¤ã®JANã‚³ãƒ¼ãƒ‰
            r'(\d{13})',          # æ¨™æº–13æ¡
            r'(\d{8})',           # 8æ¡
        ]
        for pattern in jan_patterns:
            jan_match = re.search(pattern, row_text)
            if jan_match:
                jan_code = jan_match.group(0).replace('-', '')
                if len(jan_code) >= 8:
                    product_data['jan_code'] = jan_code
                    break
        
        # ä¾¡æ ¼ (Â¥X,XXX or Xãƒ‘ãƒƒã‚¯ X,XXXå††)
        price_patterns = [
            r'[Â¥ï¿¥]?\s*(\d{1,3}(?:,\d{3})+)\s*å††',
            r'(\d{1,3}(?:,\d{3})+)\s*å††',
            r'[Â¥ï¿¥]\s*(\d+)',
        ]
        for pattern in price_patterns:
            price_match = re.search(pattern, row_text)
            if price_match:
                price_str = price_match.group(1).replace(',', '')
                try:
                    price_num = int(price_str)
                    if 100 <= price_num <= 100000:
                        product_data['price'] = str(price_num)
                        product_data['wholesale_price'] = price_num
                        product_data['reference_sales_price'] = price_num
                        break
                except ValueError:
                    continue
        
        # å•†å“å (ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¹ãƒªãƒ¼ãƒ–ã€XXXã€YYY)
        product_name_patterns = [
            r'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¹ãƒªãƒ¼ãƒ–[ã€ã€Œ]([^ã€ã€]+)[ã€ã€]\s*([^\(|]+)',
            r'([^|]+)\(EN-\d+\)',
        ]
        for pattern in product_name_patterns:
            name_match = re.search(pattern, row_text)
            if name_match:
                if len(name_match.groups()) >= 2:
                    product_data['product_name'] = f"{name_match.group(1)} {name_match.group(2)}".strip()
                else:
                    product_data['product_name'] = name_match.group(1).strip()
                break
        
        # ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°
        carton_patterns = [
            r'(\d+)å…¥\s*\((\d+)ãƒ‘ãƒƒã‚¯[Ã—x](\d+)BOX\)',
            r'ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°[ï¼š:\s]*(\d+)',
        ]
        for pattern in carton_patterns:
            carton_match = re.search(pattern, row_text)
            if carton_match:
                if len(carton_match.groups()) >= 3:
                    total = int(carton_match.group(1))
                    product_data['case_pack_quantity'] = total
                else:
                    product_data['case_pack_quantity'] = int(carton_match.group(1))
                break
        
        # 2. ã‚«ãƒ†ã‚´ãƒªã¨ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
        if 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¹ãƒªãƒ¼ãƒ–' in row_text or 'ã‚¹ãƒªãƒ¼ãƒ–' in row_text:
            product_data['category'] = 'ãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚«ãƒ¼ãƒ‰ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼'
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ï¼ˆå…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º - ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰
        if 'ã‚¨ãƒ³ã‚¹ã‚«ã‚¤' in full_text or 'EN-' in row_text:
            product_data['brand'] = 'ã‚¨ãƒ³ã‚¹ã‚«ã‚¤'
            product_data['manufacturer'] = 'æ ªå¼ä¼šç¤¾ã‚¨ãƒ³ã‚¹ã‚«ã‚¤'
        
        # 3. ä½œå“åã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æƒ…å ±ã‚’æŠ½å‡º
        character_match = re.search(r'[ã€ã€Œ]([^ã€ã€]+)[ã€ã€]', row_text)
        if character_match:
            work_name = character_match.group(1)
            product_data['character_name'] = work_name
        
        # 4. ãã®ä»–ã®é …ç›®ï¼ˆExcelã«å­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        # ç™ºå£²æ—¥
        date_patterns = [
            r'(\d{4})[å¹´/-](\d{1,2})[æœˆ/-](\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
        ]
        for pattern in date_patterns:
            date_match = re.search(pattern, row_text)
            if date_match:
                year, month, day = date_match.groups()
                product_data['release_date'] = f"{year}/{month.zfill(2)}/{day.zfill(2)}"
                break
        
        # å•†å“èª¬æ˜ï¼ˆå•†å“åã‹ã‚‰ç”Ÿæˆï¼‰
        if product_data.get('product_name'):
            product_data['description'] = f"ã€{product_data.get('character_name', '')}ã€ã®{product_data.get('product_name', '')}ã§ã™ã€‚" if product_data.get('character_name') else product_data.get('product_name', '')
        
        # 5. ã‚µã‚¤ã‚ºæƒ…å ±ï¼ˆExcelã‹ã‚‰æŠ½å‡ºã§ãã‚‹å ´åˆï¼‰
        size_match = re.search(r'(\d+)\s*[Ã—x]\s*(\d+)\s*[Ã—x]?\s*(\d+)?\s*mm', row_text)
        if size_match:
            w, h, d = size_match.groups()
            if d:
                product_data['single_product_size'] = f"{w}Ã—{h}Ã—{d}mm"
            else:
                product_data['single_product_size'] = f"{w}Ã—{h}mm"
        
        print(f"âœ… Extracted {len([k for k, v in product_data.items() if v])} fields from Excel row")
        return product_data