"""
OpenAI OCR Service for high-accuracy text extraction using GPT-4 Vision API.
Provides near 100% accuracy for Japanese and English text extraction.
"""

import asyncio
import logging
import base64
import json
from typing import Dict, Any, Optional, List
from pathlib import Path
import re
from PIL import Image
import io
import pandas as pd
from openai import AsyncOpenAI
from app.core.config import Settings

logger = logging.getLogger(__name__)
settings = Settings()


class OpenAIOCRService:
    """High-accuracy OCR service using OpenAI GPT-4 Vision API."""
    
    def __init__(self):
        self.client = None
        self.model = settings.OPENAI_MODEL
        self.supported_formats = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.pdf', '.xls', '.xlsx']
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize OpenAI client with proper error handling."""
        if not settings.OPENAI_API_KEY or settings.OPENAI_API_KEY == "your-openai-api-key-here":
            print("\n" + "="*80)
            print("‚ö†Ô∏è  OPENAI API KEY NOT CONFIGURED")
            print("="*80)
            print("To use OCR functionality, you need to:")
            print("1. Get an OpenAI API key from: https://platform.openai.com/api-keys")
            print("2. Create a .env file in the server directory")
            print("3. Add: OPENAI_API_KEY=your-actual-api-key-here")
            print("4. Restart the server")
            print("="*80 + "\n")
            logger.warning("OPENAI_API_KEY not set. OpenAI OCR service will not be available.")
            return
        
        try:
            self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            print(f"‚úÖ OpenAI client initialized successfully with model: {settings.OPENAI_MODEL}")
            logger.info("OpenAI client initialized successfully")
        except Exception as e:
            print(f"‚ùå Failed to initialize OpenAI client: {str(e)}")
            logger.error(f"Failed to initialize OpenAI client: {str(e)}")
            self.client = None
        
    def _encode_image_to_base64(self, image_path: str) -> str:
        """Encode image to base64 for OpenAI API."""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    
    def _optimize_image_for_ocr(self, image_path: str) -> str:
        """Optimize image for better OCR results, especially for barcode images."""
        try:
            from PIL import ImageEnhance, ImageFilter
            
            # Open and process image
            with Image.open(image_path) as img:
                # Convert to RGB if needed
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # Resize if too large (OpenAI has size limits)
                max_size = 2048
                if max(img.size) > max_size:
                    img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                
                # Enhance image for better OCR, especially for barcodes
                # Increase contrast to make text and barcodes more readable
                enhancer = ImageEnhance.Contrast(img)
                img = enhancer.enhance(1.3)  # Increase contrast by 30%
                
                # Increase sharpness for better text recognition
                enhancer = ImageEnhance.Sharpness(img)
                img = enhancer.enhance(1.2)  # Increase sharpness by 20%
                
                # Apply slight unsharp mask for barcode clarity
                img = img.filter(ImageFilter.UnsharpMask(radius=1, percent=120, threshold=2))
                
                # Save optimized image
                optimized_path = str(Path(image_path).with_suffix('.optimized.jpg'))
                img.save(optimized_path, 'JPEG', quality=98, optimize=True)  # Higher quality for barcodes
                
                print(f"üñºÔ∏è Image optimized for barcode OCR: {optimized_path}")
                return optimized_path
                
        except Exception as e:
            logger.warning(f"Image optimization failed: {str(e)}, using original")
            return image_path
    
    async def extract_text_from_image(
        self,
        image_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from image using OpenAI GPT-4 Vision API.
        
        Args:
            image_path: Path to the image file
            language: Language hint (jpn+eng for Japanese and English)
            confidence_threshold: Not used for OpenAI (always high confidence)
        
        Returns:
            Dict containing extracted text and metadata
        """
        if not self.client:
            raise Exception("OpenAI client is not initialized. Please check OPENAI_API_KEY configuration.")
        
        try:
            start_time = asyncio.get_event_loop().time()
            
            # Optimize image for better results
            optimized_path = self._optimize_image_for_ocr(image_path)
            
            # Encode image to base64
            base64_image = self._encode_image_to_base64(optimized_path)
            
            # Determine language context
            language_context = ""
            if "jpn" in language.lower():
                language_context = "This image contains Japanese text (hiragana, katakana, kanji). "
            if "eng" in language.lower():
                language_context += "This image contains English text. "
            
            # Create comprehensive OCR prompt for maximum accuracy and direct structured extraction
            ocr_prompt = f"""
            {language_context}
            
            You are an advanced OCR and data extraction AI. Extract product information from this image and return structured data.
            
            CRITICAL: This image may contain MULTIPLE DIFFERENT PRODUCTS. Each product should be extracted as a separate object.
            
            EXTRACTION REQUIREMENTS - For EACH product, extract ALL available fields from the following 38 items:
            
            **Âü∫Êú¨ÊÉÖÂ†± (Basic Information):**
            1. lot_number - „É≠„ÉÉ„ÉàÁï™Âè∑
            2. classification - Âå∫ÂàÜ
            3. major_category - Â§ßÂàÜÈ°û
            4. minor_category - ‰∏≠ÂàÜÈ°û
            5. release_date - Áô∫Â£≤Êó• (format: YYYY/MM/DD or YYYYÂπ¥MMÊúàDDÊó•)
            6. jan_code - JAN„Ç≥„Éº„Éâ (13-digit barcode number, often starts with 4970381)
            7. product_code - ÂïÜÂìÅÁï™Âè∑ (e.g., EN-1420, ST-03CB)
            8. in_store - „Ç§„É≥„Çπ„Éà„Ç¢
            9. genre_name - „Ç∏„É£„É≥„É´ÂêçÁß∞
            10. supplier_name - ‰ªïÂÖ•ÂÖà
            11. ip_name - „É°„Éº„Ç´„ÉºÂêçÁß∞
            12. character_name - „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç(IPÂêç)
            13. product_name - ÂïÜÂìÅÂêçÁß∞
            
            **‰æ°Ê†º„ÉªÊï∞ÈáèÊÉÖÂ†± (Price & Quantity):**
            14. reference_sales_price - ÂèÇËÄÉË≤©Â£≤‰æ°Ê†º (number only, e.g., 1100)
            15. wholesale_price - Âç∏Âçò‰æ°ÔºàÊäúÔºâ (number only)
            16. wholesale_quantity - Âç∏ÂèØËÉΩÊï∞ (integer)
            17. stock - Áô∫Ê≥®Êï∞ (integer)
            18. order_amount - Áô∫Ê≥®ÈáëÈ°ç (number)
            19. quantity_per_pack - ÂÖ•Êï∞ (e.g., "60", "12ÂÄãÂÖ•„Çä")
            
            **‰∫àÁ¥ÑÊÉÖÂ†± (Reservation):**
            20. reservation_release_date - ‰∫àÁ¥ÑËß£Á¶ÅÊó•
            21. reservation_deadline - ‰∫àÁ¥ÑÁ∑†„ÇÅÂàá„ÇäÊó•
            22. reservation_shipping_date - ‰∫àÁ¥ÑÂïÜÂìÅÁô∫ÈÄÅ‰∫àÂÆöÊó•
            
            **„Çµ„Ç§„Ç∫„ÉªÊ¢±ÂåÖÊÉÖÂ†± (Size & Packaging):**
            23. case_pack_quantity - „Ç±„Éº„ÇπÊ¢±ÂÖ•Êï∞ (integer, e.g., 72)
            24. single_product_size - ÂçòÂìÅ„Çµ„Ç§„Ç∫ (e.g., "91√ó66mm")
            25. inner_box_size - ÂÜÖÁÆ±„Çµ„Ç§„Ç∫
            26. carton_size - „Ç´„Éº„Éà„É≥„Çµ„Ç§„Ç∫
            27. inner_box_gtin - ÂÜÖÁÆ±GTIN (13-14 digits)
            28. outer_box_gtin - Â§ñÁÆ±GTIN (13-14 digits)
            
            **„Åù„ÅÆ‰ªñÊÉÖÂ†± (Other):**
            29. description - ÂïÜÂìÅË™¨Êòé
            30. protective_film_material - Ê©üÊùê„Éï„Ç£„É´„É†
            31. country_of_origin - ÂéüÁî£ÂõΩ (e.g., "Êó•Êú¨", "‰∏≠ÂõΩ")
            32. target_age - ÂØæË±°Âπ¥ÈΩ¢ (e.g., "3Ê≠≥‰ª•‰∏ä")
            33. image1 - ÁîªÂÉè1 (URL if present)
            34. image2 - ÁîªÂÉè2 (URL if present)
            35. image3 - ÁîªÂÉè3 (URL if present)
            36. image4 - ÁîªÂÉè4 (URL if present)
            37. image5 - ÁîªÂÉè5 (URL if present)
            38. image6 - ÁîªÂÉè6 (URL if present)
            
            **BARCODE READING PRIORITY:**
            - Look for BLACK AND WHITE STRIPED BARCODE PATTERNS
            - Read the numbers displayed UNDER the barcode stripes carefully
            - JAN codes are typically 13 digits starting with 4 (e.g., 4970381806170)
            
            **MULTI-PRODUCT HANDLING:**
            - If you detect multiple products (different product codes, JAN codes, or character names), extract each as a separate product
            - Each product should have its own complete set of fields
            - Do NOT mix information from different products
            
            RESPONSE FORMAT - Return ONLY valid JSON in this exact structure:
            {{
                "raw_text": "All visible text extracted from the image",
                "confidence_score": 95.0,
                "language_detected": "japanese",
                "products": [
                    {{
                        "product_name": "extracted value or null",
                        "jan_code": "extracted value or null",
                        "product_code": "extracted value or null",
                        "lot_number": "extracted value or null",
                        "classification": "extracted value or null",
                        "major_category": "extracted value or null",
                        "minor_category": "extracted value or null",
                        "release_date": "extracted value or null",
                        "in_store": "extracted value or null",
                        "genre_name": "extracted value or null",
                        "supplier_name": "extracted value or null",
                        "ip_name": "extracted value or null",
                        "character_name": "extracted value or null",
                        "reference_sales_price": number or null,
                        "wholesale_price": number or null,
                        "wholesale_quantity": number or null,
                        "stock": number or null,
                        "order_amount": number or null,
                        "quantity_per_pack": "extracted value or null",
                        "reservation_release_date": "extracted value or null",
                        "reservation_deadline": "extracted value or null",
                        "reservation_shipping_date": "extracted value or null",
                        "case_pack_quantity": number or null,
                        "single_product_size": "extracted value or null",
                        "inner_box_size": "extracted value or null",
                        "carton_size": "extracted value or null",
                        "inner_box_gtin": "extracted value or null",
                        "outer_box_gtin": "extracted value or null",
                        "description": "extracted value or null",
                        "protective_film_material": "extracted value or null",
                        "country_of_origin": "extracted value or null",
                        "target_age": "extracted value or null",
                        "image1": "extracted value or null",
                        "image2": "extracted value or null",
                        "image3": "extracted value or null",
                        "image4": "extracted value or null",
                        "image5": "extracted value or null",
                        "image6": "extracted value or null"
                    }}
                ]
            }}
            
            CRITICAL RULES:
            1. Return ONLY valid JSON - no markdown, no extra text
            2. If a field is not visible in the image, set it to null (not empty string)
            3. For numbers (prices, quantities), return as numbers not strings
            4. For dates, preserve the original format from the image
            5. Extract Japanese text exactly as shown (kanji, hiragana, katakana)
            6. If only 1 product is detected, the "products" array should have 1 object
            7. If multiple products are detected, create separate objects for each
            """
            
            print(f"ü§ñ OPENAI OCR: Processing image with {self.model}")
            
            # Call OpenAI Vision API
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": ocr_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}",
                                    "detail": "high"  # High detail for better OCR accuracy
                                }
                            }
                        ]
                    }
                ],
                max_tokens=16000,  # Increased for 38 fields per product
                temperature=0.1  # Low temperature for consistent, accurate results
            )
            
            # Parse response
            response_text = response.choices[0].message.content
            
            print(f"üîç DEBUG: OpenAI Raw Response:")
            print(f"Response length: {len(response_text)}")
            print(f"First 500 chars: {response_text[:500]}")
            print(f"Last 500 chars: {response_text[-500:]}")
            
            try:
                # Try to parse as JSON first
                if response_text.strip().startswith('{'):
                    print("üîç DEBUG: Parsing as direct JSON")
                    result = json.loads(response_text)
                else:
                    # If not JSON, extract JSON from markdown code blocks
                    json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                    if json_match:
                        print("üîç DEBUG: Found JSON in markdown code block")
                        result = json.loads(json_match.group(1))
                    else:
                        # Try to find complete JSON with products array
                        json_match = re.search(r'(\{.*?"products"\s*:\s*\[.*?\].*?\})', response_text, re.DOTALL)
                        if json_match:
                            print("üîç DEBUG: Found JSON with products array")
                            result = json.loads(json_match.group(1))
                        else:
                            # Try to find any JSON object
                            json_match = re.search(r'(\{.*?"raw_text".*?\})', response_text, re.DOTALL)
                            if json_match:
                                print("üîç DEBUG: Found basic JSON pattern")
                                result = json.loads(json_match.group(1))
                            else:
                                print("‚ö†Ô∏è  DEBUG: No JSON found, using fallback")
                                # Fallback: treat entire response as raw text
                                result = {
                                    "raw_text": response_text,
                                    "confidence_score": 90.0,
                                    "language_detected": "unknown",
                                    "products": [],
                                    "word_confidences": {},
                                    "processing_metadata": {"method": "openai_gpt4_vision", "model": self.model}
                                }
                            
                print(f"üîç DEBUG: Parsed result keys: {list(result.keys())}")
                if 'products' in result:
                    print(f"üîç DEBUG: Products found: {len(result.get('products', []))} items")
                    for i, p in enumerate(result.get('products', [])[:3]):  # Show first 3 products
                        print(f"  Product {i+1}: {p.get('product_name', 'N/A')} | JAN: {p.get('jan_code', 'N/A')}")
                        
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è  DEBUG: JSON parsing failed: {e}")
                logger.warning(f"Failed to parse OpenAI response as JSON: {e}")
                # Fallback to treating response as raw text
                result = {
                    "raw_text": response_text,
                    "confidence_score": 90.0,
                    "language_detected": "unknown", 
                    "product_info": {},
                    "word_confidences": {},
                    "processing_metadata": {"method": "openai_gpt4_vision", "model": self.model}
                }
            
            # Clean up optimized image if it was created
            if optimized_path != image_path:
                try:
                    Path(optimized_path).unlink()
                except:
                    pass
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            result["processing_time_ms"] = processing_time_ms
            
            # Ensure required fields exist
            result.setdefault("raw_text", "")
            result.setdefault("confidence_score", 95.0)  # OpenAI typically has high confidence
            result.setdefault("language_detected", "unknown")
            result.setdefault("word_confidences", {})
            result.setdefault("bounding_boxes", [])
            result.setdefault("text_blocks", [])
            
            # Check if OpenAI returned structured products array
            raw_text = result.get("raw_text", "")
            products_from_ai = result.get("products", [])
            
            if products_from_ai and len(products_from_ai) > 0:
                print(f"‚úÖ OPENAI RETURNED {len(products_from_ai)} STRUCTURED PRODUCTS")
                
                # Process products from OpenAI's structured response
                structured_products = []
                for i, ai_product in enumerate(products_from_ai):
                    # OpenAI returned all 38 fields - use them directly
                    product_data = {
                        # Core fields
                        "product_name": ai_product.get('product_name'),
                        "jan_code": ai_product.get('jan_code'),
                        "description": ai_product.get('description'),
                        
                        # 38 Company-Specified Fields
                        "lot_number": ai_product.get('lot_number'),
                        "classification": ai_product.get('classification'),
                        "major_category": ai_product.get('major_category'),
                        "minor_category": ai_product.get('minor_category'),
                        "release_date": ai_product.get('release_date'),
                        "product_code": ai_product.get('product_code'),
                        "in_store": ai_product.get('in_store'),
                        "genre_name": ai_product.get('genre_name'),
                        "supplier_name": ai_product.get('supplier_name'),
                        "ip_name": ai_product.get('ip_name'),
                        "character_name": ai_product.get('character_name'),
                        "reference_sales_price": ai_product.get('reference_sales_price'),
                        "wholesale_price": ai_product.get('wholesale_price'),
                        "wholesale_quantity": ai_product.get('wholesale_quantity'),
                        "stock": ai_product.get('stock'),
                        "order_amount": ai_product.get('order_amount'),
                        "quantity_per_pack": ai_product.get('quantity_per_pack'),
                        "reservation_release_date": ai_product.get('reservation_release_date'),
                        "reservation_deadline": ai_product.get('reservation_deadline'),
                        "reservation_shipping_date": ai_product.get('reservation_shipping_date'),
                        "case_pack_quantity": ai_product.get('case_pack_quantity'),
                        "single_product_size": ai_product.get('single_product_size'),
                        "inner_box_size": ai_product.get('inner_box_size'),
                        "carton_size": ai_product.get('carton_size'),
                        "inner_box_gtin": ai_product.get('inner_box_gtin'),
                        "outer_box_gtin": ai_product.get('outer_box_gtin'),
                        "protective_film_material": ai_product.get('protective_film_material'),
                        "country_of_origin": ai_product.get('country_of_origin'),
                        "target_age": ai_product.get('target_age'),
                        "image1": ai_product.get('image1'),
                        "image2": ai_product.get('image2'),
                        "image3": ai_product.get('image3'),
                        "image4": ai_product.get('image4'),
                        "image5": ai_product.get('image5'),
                        "image6": ai_product.get('image6'),
                        
                        # Legacy fields for backward compatibility
                        "sku": ai_product.get('product_code'),  # Use product_code as SKU
                        "price": ai_product.get('wholesale_price') or ai_product.get('reference_sales_price'),
                        "category": ai_product.get('major_category'),
                        "brand": ai_product.get('ip_name'),
                        "manufacturer": ai_product.get('supplier_name'),
                        
                        # Meta fields
                        "product_index": i + 1,
                        "section_text": raw_text[:300] + "..." if len(raw_text) > 300 else raw_text
                    }
                    structured_products.append(product_data)
                    
                    # Count how many of the 38 fields were extracted
                    fields_38 = [
                        'lot_number', 'classification', 'major_category', 'minor_category', 
                        'release_date', 'jan_code', 'product_code', 'in_store', 'genre_name',
                        'supplier_name', 'ip_name', 'character_name', 'product_name',
                        'reference_sales_price', 'wholesale_price', 'wholesale_quantity', 
                        'stock', 'order_amount', 'quantity_per_pack', 'reservation_release_date',
                        'reservation_deadline', 'reservation_shipping_date', 'case_pack_quantity',
                        'single_product_size', 'inner_box_size', 'carton_size', 'inner_box_gtin',
                        'outer_box_gtin', 'description', 'protective_film_material', 
                        'country_of_origin', 'target_age', 'image1', 'image2', 'image3', 
                        'image4', 'image5', 'image6'
                    ]
                    extracted_count = sum(1 for field in fields_38 if product_data.get(field))
                    
                    print(f"üì¶ Product {i+1}: {extracted_count}/38 fields extracted")
                    print(f"  ÂïÜÂìÅÂêç: {product_data.get('product_name', 'Not detected')}")
                    print(f"  JAN„Ç≥„Éº„Éâ: {product_data.get('jan_code', 'Not detected')}")
                    print(f"  ÂïÜÂìÅÁï™Âè∑: {product_data.get('product_code', 'Not detected')}")
                    print(f"  ÂèÇËÄÉË≤©Â£≤‰æ°Ê†º: {product_data.get('reference_sales_price', 'Not detected')}")
                    print(f"  Â§ßÂàÜÈ°û: {product_data.get('major_category', 'Not detected')}")
                    print(f"  ‰ªïÂÖ•ÂÖà: {product_data.get('supplier_name', 'Not detected')}")
                
                # Create structured_data with first product as main, all products in _products_list
                if len(structured_products) > 1:
                    structured_data = structured_products[0].copy()
                    structured_data["has_multiple_products"] = True
                    structured_data["total_products_detected"] = len(structured_products)
                    structured_data["_products_list"] = structured_products
                else:
                    structured_data = structured_products[0]
                    structured_data["has_multiple_products"] = False
            else:
                # Fallback: Use Python regex extraction if OpenAI didn't return products
                print("‚ö†Ô∏è OpenAI didn't return structured products, falling back to Python extraction")
                multiple_products = self._detect_multiple_products(raw_text)
                
                if multiple_products:
                    print(f"üîç DETECTED MULTIPLE PRODUCTS: {len(multiple_products)} products found")
                    structured_data = {
                        "product_name": multiple_products[0].get('product_name'),
                        "sku": multiple_products[0].get('sku'),
                        "jan_code": multiple_products[0].get('jan_code'),
                        "price": multiple_products[0].get('price'),
                        "release_date": multiple_products[0].get('release_date'),
                        "category": multiple_products[0].get('category'),
                        "brand": multiple_products[0].get('brand'),
                        "manufacturer": multiple_products[0].get('manufacturer'),
                        "product_index": multiple_products[0].get('product_index', 1),
                        "section_text": multiple_products[0].get('section_text', ''),
                        "total_products_detected": len(multiple_products),
                        "has_multiple_products": True,
                        "_products_list": multiple_products
                    }
                else:
                    # Single product processing
                    structured_data = self._parse_product_data_from_text(raw_text)
                    structured_data["has_multiple_products"] = False
            
            result["structured_data"] = structured_data
            
            print(f"‚úÖ OPENAI OCR SUCCESS: Extracted {len(result['raw_text'])} characters in {processing_time_ms}ms")
            print(f"üîç PARSED STRUCTURED DATA: {structured_data}")
            
            return result
            
        except Exception as e:
            error_msg = str(e).lower()
            if "connection" in error_msg or "network" in error_msg or "timeout" in error_msg:
                logger.error(f"OpenAI API connection failed: {str(e)}")
                raise ValueError(f"OpenAI API connection failed. Please check your internet connection and API key. Error: {str(e)}")
            elif "api_key" in error_msg or "authentication" in error_msg or "unauthorized" in error_msg:
                logger.error(f"OpenAI API authentication failed: {str(e)}")
                raise ValueError(f"OpenAI API authentication failed. Please check your OPENAI_API_KEY in the .env file. Error: {str(e)}")
            elif "quota" in error_msg or "billing" in error_msg:
                logger.error(f"OpenAI API quota exceeded: {str(e)}")
                raise ValueError(f"OpenAI API quota exceeded. Please check your OpenAI account billing. Error: {str(e)}")
            else:
                logger.error(f"OpenAI OCR processing failed: {str(e)}")
                raise ValueError(f"OpenAI OCR processing failed: {str(e)}")
    
    async def extract_text_from_excel(
        self,
        excel_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from Excel file (.xls/.xlsx) using pandas and OpenAI for structured analysis.
        """
        try:
            start_time = asyncio.get_event_loop().time()
            
            # Read Excel file
            try:
                # Try to read with openpyxl engine for .xlsx files
                if excel_path.endswith('.xlsx'):
                    df = pd.read_excel(excel_path, engine='openpyxl', sheet_name=None)
                else:
                    # Use xlrd for .xls files
                    df = pd.read_excel(excel_path, engine='xlrd', sheet_name=None)
            except Exception as e:
                logger.error(f"Failed to read Excel file: {str(e)}")
                raise ValueError(f"Failed to read Excel file: {str(e)}")
            
            # Process all sheets - Extract only essential product data
            all_text = []
            product_rows = []
            
            for sheet_name, sheet_df in df.items():
                print(f"üîç PROCESSING SHEET: {sheet_name} ({len(sheet_df)} rows)")
                
                # Extract only rows containing product codes (EN-XXXX)
                for idx, row in sheet_df.iterrows():
                    row_str = " | ".join([str(cell) if pd.notna(cell) else "" for cell in row.values])
                    
                    # Only include rows with EN-codes or essential headers
                    if (re.search(r'EN-\d+', row_str) or 
                        any(keyword in row_str for keyword in ['ÂïÜÂìÅÂêç', 'JAN„Ç≥„Éº„Éâ', 'Áô∫Â£≤‰∫àÂÆöÊó•', 'Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º'])):
                        all_text.append(row_str)
                        
                        # If this is a product row, store it separately
                        if re.search(r'EN-\d+', row_str):
                            product_rows.append(row_str)
                            print(f"‚úÖ PRODUCT ROW FOUND: {row_str[:100]}")
            
            # Combine only essential text
            raw_text = "\n".join(all_text)
            print(f"üîç EXTRACTED TEXT LENGTH: {len(raw_text)} chars, {len(product_rows)} product rows")
            
            # Skip OpenAI analysis for Excel files to avoid complexity
            ai_structured = {
                "analysis": f"Excel file processed with {len(product_rows)} product rows extracted",
                "product_count": len(product_rows)
            }
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            
            # Parse structured data from combined raw text - support multiple products
            multiple_products = self._detect_multiple_products(raw_text)
            
            # Ë§áÊï∞ÂïÜÂìÅ„ÅåÊ§úÂá∫„Åï„Çå„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇ„ÄÅproduct_rows„ÅåË§áÊï∞„ÅÇ„Çå„Å∞Âº∑Âà∂ÁöÑ„Å´‰ΩúÊàê
            if not multiple_products and len(product_rows) > 1:
                print(f"üîß EXCEL: FORCING MULTI-PRODUCT from {len(product_rows)} product rows")
                multiple_products = []
                for i, product_row in enumerate(product_rows):
                    # Extract all 38 fields from the product row
                    product_data = self._extract_all_fields_from_excel_row(product_row, raw_text)
                    if product_data:
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = product_row
                        multiple_products.append(product_data)
                        print(f"   ‚úÖ Excel Product {i+1}: {product_data.get('product_name', 'Unknown')}")
            
            if multiple_products:
                print(f"üîç EXCEL: DETECTED MULTIPLE PRODUCTS: {len(multiple_products)} products found")
                # Return the first product as the main structured data, but include all products in _products_list
                parsed_structured_data = {
                    "product_name": multiple_products[0].get('product_name'),
                    "sku": multiple_products[0].get('sku'),
                    "jan_code": multiple_products[0].get('jan_code'),
                    "price": multiple_products[0].get('price'),
                    "release_date": multiple_products[0].get('release_date'),
                    "category": multiple_products[0].get('category'),
                    "brand": multiple_products[0].get('brand'),
                    "manufacturer": multiple_products[0].get('manufacturer'),
                    "product_index": multiple_products[0].get('product_index', 1),
                    "section_text": multiple_products[0].get('section_text', ''),
                    "total_products_detected": len(multiple_products),
                    "has_multiple_products": True
                }
                # Store complete product list for processor with all 38 fields
                parsed_structured_data["_products_list"] = []
                for i, p in enumerate(multiple_products):
                    product_dict = {
                        # Core fields
                        "product_name": p.get('product_name'),
                        "jan_code": p.get('jan_code'),
                        "description": p.get('description'),
                        
                        # 38 Company-Specified Fields
                        "lot_number": p.get('lot_number'),
                        "classification": p.get('classification'),
                        "major_category": p.get('major_category'),
                        "minor_category": p.get('minor_category'),
                        "release_date": p.get('release_date'),
                        "product_code": p.get('product_code') or p.get('sku'),
                        "in_store": p.get('in_store'),
                        "genre_name": p.get('genre_name'),
                        "supplier_name": p.get('supplier_name'),
                        "ip_name": p.get('ip_name'),
                        "character_name": p.get('character_name'),
                        "reference_sales_price": p.get('reference_sales_price'),
                        "wholesale_price": p.get('wholesale_price') or p.get('price'),
                        "wholesale_quantity": p.get('wholesale_quantity'),
                        "stock": p.get('stock'),
                        "order_amount": p.get('order_amount'),
                        "quantity_per_pack": p.get('quantity_per_pack'),
                        "reservation_release_date": p.get('reservation_release_date'),
                        "reservation_deadline": p.get('reservation_deadline'),
                        "reservation_shipping_date": p.get('reservation_shipping_date'),
                        "case_pack_quantity": p.get('case_pack_quantity'),
                        "single_product_size": p.get('single_product_size'),
                        "inner_box_size": p.get('inner_box_size'),
                        "carton_size": p.get('carton_size'),
                        "inner_box_gtin": p.get('inner_box_gtin'),
                        "outer_box_gtin": p.get('outer_box_gtin'),
                        "protective_film_material": p.get('protective_film_material'),
                        "country_of_origin": p.get('country_of_origin'),
                        "target_age": p.get('target_age'),
                        "image1": p.get('image1'),
                        "image2": p.get('image2'),
                        "image3": p.get('image3'),
                        "image4": p.get('image4'),
                        "image5": p.get('image5'),
                        "image6": p.get('image6'),
                        
                        # Legacy fields
                        "sku": p.get('sku'),
                        "price": p.get('price'),
                        "category": p.get('category'),
                        "brand": p.get('brand'),
                        "manufacturer": p.get('manufacturer'),
                        
                        # Meta fields
                        "product_index": p.get('product_index', i+1),
                        "section_text": p.get('section_text', '')
                    }
                    parsed_structured_data["_products_list"].append(product_dict)
                
                # Log all detected products
                print("üè∑Ô∏è ALL DETECTED PRODUCTS:")
                print("-" * 40)
                for i, product in enumerate(multiple_products, 1):
                    print(f"Product {i}:")
                    print(f"  Name: {product.get('product_name', 'Not detected')}")
                    print(f"  SKU: {product.get('sku', 'Not detected')}")
                    print(f"  JAN Code: {product.get('jan_code', 'Not detected')}")
                    print(f"  Price: {product.get('price', 'Not detected')}")
                    print(f"  Category: {product.get('category', 'Not detected')}")
                    print(f"  Brand: {product.get('brand', 'Not detected')}")
                    if i < len(multiple_products):
                        print("  " + "-" * 38)
                    else:
                        print("  " + "-" * 38)
            else:
                # Single product processing
                parsed_structured_data = self._parse_product_data_from_text(raw_text)
                parsed_structured_data["has_multiple_products"] = False
            
            parsed_structured_data.update({
                "ai_analysis": ai_structured,
                "file_type": "excel",
                "total_sheets": len(df),
                "total_text_length": len(raw_text),
                "product_rows_found": len(product_rows)
            })
            
            result = {
                "raw_text": raw_text,
                "confidence_score": 95.0,  # Excel parsing is very reliable
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": parsed_structured_data,
                "processing_metadata": {
                    "method": "excel_pandas_openai", 
                    "model": self.model,
                    "sheets_processed": list(df.keys())
                },
                "processing_time_ms": processing_time_ms
            }
            
            print(f"‚úÖ EXCEL OCR SUCCESS: Processed {len(df)} sheets, {len(raw_text)} characters in {processing_time_ms}ms")
            print(f"üîç PARSED STRUCTURED DATA: {parsed_structured_data}")
            
            return result
            
        except Exception as e:
            logger.error(f"Excel OCR processing failed: {str(e)}")
            raise ValueError(f"Excel OCR processing failed: {str(e)}")
    
    async def extract_text_from_pdf(
        self,
        pdf_path: str,
        language: str = "jpn+eng",
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from PDF file by converting to images and using OpenAI Vision.
        """
        try:
            import fitz  # PyMuPDF
            
            start_time = asyncio.get_event_loop().time()
            logger.info(f"ü§ñ PDF OCR: Processing PDF with {self.model}")
            
            # Open PDF
            pdf_document = fitz.open(pdf_path)
            all_text = []
            total_confidence = 0
            processed_pages = 0
            
            # Process each page
            for page_num in range(len(pdf_document)):
                page = pdf_document.load_page(page_num)
                
                # Convert page to image
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom for better quality
                img_data = pix.tobytes("png")
                
                # Convert to base64 for OpenAI
                import base64
                base64_image = base64.b64encode(img_data).decode('utf-8')
                
                # Process with OpenAI
                try:
                    response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": f"""Please perform high-accuracy OCR (Optical Character Recognition) on this PDF page image and extract structured product information.

                                        Language: {language}
                                        
INSTRUCTIONS:
1. Extract ALL visible text from the image with 100% accuracy
2. Preserve exact spacing, line breaks, and formatting
3. Include ALL characters including punctuation, symbols, and special characters
4. If text is partially obscured or unclear, make your best interpretation
5. Maintain the original reading order (top to bottom, left to right for mixed languages)
6. For Japanese text, preserve kanji, hiragana, and katakana exactly as shown
7. For numbers, prices, codes, preserve exact formatting (including ¬•, $, -, etc.)

RESPONSE FORMAT:
You must respond with valid JSON only. Do not include any other text or markdown formatting.

{{
    "raw_text": "All extracted text exactly as it appears in the image, preserving line breaks and formatting"
}}

CRITICAL RULES:
1. Return ONLY valid JSON - no markdown, no extra text
2. Focus on accurate text extraction - do not try to interpret or structure the data
3. Extract Japanese text exactly as shown
4. Preserve all formatting, line breaks, and spacing"""
                                    },
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{base64_image}",
                                            "detail": "high"
                                        }
                                    }
                                ]
                            }
                        ],
                        max_tokens=4000,
                        temperature=0.1
                    )
                    
                    response_text = response.choices[0].message.content
                    
                    print(f"üîç DEBUG: PDF Page {page_num + 1} Response:")
                    print(f"Response length: {len(response_text)}")
                    print(f"First 300 chars: {response_text[:300]}")
                    
                    # Try to parse as JSON
                    try:
                        import json
                        if response_text.strip().startswith('{'):
                            page_result = json.loads(response_text)
                        else:
                            # Try to extract JSON from markdown
                            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                            if json_match:
                                page_result = json.loads(json_match.group(1))
                            else:
                                # Try to find JSON anywhere
                                json_match = re.search(r'(\{[^{}]*"raw_text"[^{}]*\})', response_text, re.DOTALL)
                                if json_match:
                                    page_result = json.loads(json_match.group(1))
                                else:
                                    raise json.JSONDecodeError("No JSON found", response_text, 0)
                        
                        page_text = page_result.get('raw_text', response_text)
                        print(f"üîç DEBUG: PDF Page {page_num + 1} extracted {len(page_text)} characters")
                        
                    except json.JSONDecodeError as e:
                        print(f"‚ö†Ô∏è  DEBUG: PDF Page {page_num + 1} JSON parsing failed: {e}")
                        page_text = response_text
                    
                    all_text.append(f"=== Page {page_num + 1} ===\n{page_text}")
                    
                    total_confidence += 85.0  # Assume good confidence for PDF OCR
                    processed_pages += 1
                    
                except Exception as page_error:
                    logger.warning(f"Failed to process PDF page {page_num + 1}: {page_error}")
                    # Try to extract text directly from PDF
                    try:
                        # Check if document is still valid
                        if pdf_document.is_closed:
                            logger.error(f"PDF document is closed, cannot process page {page_num + 1}")
                            break
                        page_text = page.get_text()
                        if page_text.strip():
                            all_text.append(f"=== Page {page_num + 1} (Direct) ===\n{page_text}")
                            processed_pages += 1
                            total_confidence += 70.0
                    except Exception as direct_error:
                        logger.warning(f"Direct text extraction failed for page {page_num + 1}: {direct_error}")
                        # If document is closed, stop processing
                        if "document closed" in str(direct_error).lower():
                            logger.error("PDF document closed unexpectedly, stopping processing")
                            break
            
            # Store total pages before closing document
            total_pages = len(pdf_document)
            
            # Close document safely
            try:
                pdf_document.close()
            except Exception as close_error:
                logger.warning(f"Error closing PDF document: {close_error}")
            
            # Calculate final confidence
            final_confidence = total_confidence / max(processed_pages, 1) if processed_pages > 0 else 0
            
            # Combine all text
            raw_text = "\n\n".join(all_text)
            
            # Calculate processing time
            processing_time_ms = int((asyncio.get_event_loop().time() - start_time) * 1000)
            
            # Parse structured data from combined raw text
            structured_data = self._parse_product_data_from_text(raw_text)
            structured_data.update({
                "file_type": "pdf",
                "total_pages": total_pages,
                "processed_pages": processed_pages,
                "total_text_length": len(raw_text)
            })
            
            result = {
                "raw_text": raw_text,
                "confidence_score": final_confidence,
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": structured_data,
                "processing_metadata": {
                    "method": "pdf_pymupdf_openai", 
                    "model": self.model,
                    "pages_processed": processed_pages
                },
                "processing_time_ms": processing_time_ms
            }
            
            print(f"‚úÖ PDF OCR SUCCESS: Processed {processed_pages}/{total_pages} pages, {len(raw_text)} characters in {processing_time_ms}ms")
            print(f"üîç PARSED STRUCTURED DATA: {structured_data}")
            
            return result
            
        except ImportError:
            # Fallback: try to use direct text extraction if PyMuPDF is not available
            logger.warning("PyMuPDF not available, trying alternative PDF processing")
            return await self._extract_pdf_fallback(pdf_path, language, confidence_threshold)
        except Exception as e:
            logger.error(f"PDF OCR processing failed: {str(e)}")
            # Ensure PDF document is closed even on error
            try:
                if 'pdf_document' in locals():
                    pdf_document.close()
            except:
                pass
            raise ValueError(f"PDF OCR processing failed: {str(e)}")
    
    async def _extract_pdf_fallback(self, pdf_path: str, language: str, confidence_threshold: float) -> Dict[str, Any]:
        """Fallback PDF processing without PyMuPDF"""
        try:
            # Simple fallback - return minimal structure
            return {
                "raw_text": f"PDF file: {Path(pdf_path).name} (processing unavailable)",
                "confidence_score": 0.0,
                "language_detected": language,
                "word_confidences": {},
                "bounding_boxes": [],
                "text_blocks": [],
                "structured_data": {
                    "file_type": "pdf",
                    "status": "processing_unavailable"
                },
                "processing_metadata": {
                    "method": "fallback", 
                    "error": "PDF processing libraries not available"
                },
                "processing_time_ms": 0
            }
        except Exception as e:
            raise ValueError(f"PDF fallback processing failed: {str(e)}")
    
    async def extract_text_from_file(
        self,
        file_path: str,
        language: str = "jpn+eng",
        preprocessing: bool = True,
        confidence_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """
        Extract text from file. Supports images and Excel files.
        """
        if not self.client:
            raise Exception("OpenAI client is not initialized. Please check OPENAI_API_KEY configuration.")
        
        file_path_obj = Path(file_path)
        file_extension = file_path_obj.suffix.lower()
        
        if file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
            return await self.extract_text_from_image(
                file_path, language, confidence_threshold
            )
        elif file_extension in ['.xls', '.xlsx']:
            return await self.extract_text_from_excel(
                file_path, language, confidence_threshold
            )
        elif file_extension == '.pdf':
            return await self.extract_text_from_pdf(
                file_path, language, confidence_threshold
            )
        else:
            raise ValueError(f"Unsupported file format: {file_extension}") 
    
    def _parse_product_data_from_text(self, raw_text: str) -> Dict[str, Any]:
        """„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÂïÜÂìÅ„Éá„Éº„Çø„ÇíÊäΩÂá∫ÔºàÂÖ±ÈÄöÈ†ÖÁõÆ„ÅÆÊäΩÂá∫„ÇíÂº∑ÂåñÔºâ"""
        
        structured_data = {}
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        cleaned_lines = self._clean_repetitive_text(text_lines)
        
        print(f"üîç ÂïÜÂìÅ„Éá„Éº„ÇøÊäΩÂá∫ÈñãÂßã: {len(text_lines)}Ë°å„ÅÆ„ÉÜ„Ç≠„Çπ„Éà")
        
        # 1. ÂïÜÂìÅÂêç (Product Name) - ÊúÄÂÑ™ÂÖà
        product_name = self._extract_product_name(raw_text, cleaned_lines)
        if product_name:
            structured_data['product_name'] = product_name
            print(f"‚úÖ ÂïÜÂìÅÂêç: {product_name}")
        
        # 2. SKU/ÂïÜÂìÅ„Ç≥„Éº„Éâ (Product Code/SKU)
        sku = self._extract_sku(raw_text, text_lines)
        if sku:
            structured_data['sku'] = sku
            print(f"‚úÖ SKU: {sku}")
        
        # 3. JAN„Ç≥„Éº„Éâ (JAN Code) - „Éê„Éº„Ç≥„Éº„ÉâÂØæÂøúÂº∑ÂåñÁâà
        jan_code = self._extract_jan_code(raw_text)
        if jan_code:
            structured_data['jan_code'] = jan_code
            print(f"‚úÖ JAN„Ç≥„Éº„Éâ: {jan_code}")
        
        # 4. ‰æ°Ê†º (Price) - ‰æ°Ê†ºÊÉÖÂ†±„ÅÆÊäΩÂá∫
        price = self._extract_price(raw_text)
        if price:
            structured_data['price'] = price
            print(f"‚úÖ ‰æ°Ê†º: {price}")

        # 5. Âú®Â∫´Êï∞ (Stock) - Âú®Â∫´ÊÉÖÂ†±
        stock = self._extract_stock(raw_text, text_lines)
        if stock:
            structured_data['stock'] = stock
            print(f"‚úÖ Âú®Â∫´Êï∞: {stock}")
        
        # 6. „Ç´„ÉÜ„Ç¥„É™ (Category) - ÂïÜÂìÅÁ®ÆÂà•„ÅÆÊé®ÂÆö
        category = self._extract_category(raw_text)
        if category:
            structured_data['category'] = category
            print(f"‚úÖ „Ç´„ÉÜ„Ç¥„É™: {category}")
        
        # 7. „Éñ„É©„É≥„Éâ (Brand) - „Éñ„É©„É≥„ÉâÂêç„ÄÅ„É°„Éº„Ç´„ÉºÂêç
        brand = self._extract_brand(raw_text, cleaned_lines)
        if brand:
            structured_data['brand'] = brand
            print(f"‚úÖ „Éñ„É©„É≥„Éâ: {brand}")
        
        # 8. Áô∫Â£≤‰∫àÂÆöÊó• (Release Date) - Áô∫Â£≤Êó•„ÄÅ„É™„É™„Éº„ÇπÊó•
        release_date = self._extract_release_date(raw_text)
        if release_date:
            structured_data['release_date'] = release_date
            print(f"‚úÖ Áô∫Â£≤‰∫àÂÆöÊó•: {release_date}")
        
        # 9. Ë£ΩÈÄ†ÂÖÉ (Manufacturer) - Ë£ΩÈÄ†ÂÖÉ„ÄÅÁô∫Â£≤ÂÖÉ
        manufacturer = self._extract_manufacturer(raw_text, cleaned_lines, brand)
        if manufacturer:
            structured_data['manufacturer'] = manufacturer
            print(f"‚úÖ Ë£ΩÈÄ†ÂÖÉ: {manufacturer}")
        
        # 10. ÂïÜÂìÅË™¨Êòé (Description) - ÂïÜÂìÅ„ÅÆÁâπÂæ¥„ÄÅË™¨Êòé
        description = self._extract_description(raw_text, text_lines)
        if description:
            structured_data['description'] = description
            print(f"‚úÖ ÂïÜÂìÅË™¨Êòé: {description}")
        
        # 11. ÈáçÈáè (Weight) - Èáç„Åï„ÄÅ„Çµ„Ç§„Ç∫ÊÉÖÂ†±
        weight = self._extract_weight(raw_text)
        if weight:
            structured_data['weight'] = weight
            print(f"‚úÖ ÈáçÈáè: {weight}")
        
        # 12. Ëâ≤ (Color) - Ëâ≤ÊÉÖÂ†±
        color = self._extract_color(raw_text, text_lines)
        if color:
            structured_data['color'] = color
            print(f"‚úÖ Ëâ≤: {color}")
        
        # 13. Á¥†Êùê (Material) - Á¥†ÊùêÊÉÖÂ†±
        material = self._extract_material(raw_text, text_lines)
        if material:
            structured_data['material'] = material
            print(f"‚úÖ Á¥†Êùê: {material}")
        
        # 14. ÂéüÁî£ÂõΩ (Origin) - ÁîüÁî£ÂõΩÊÉÖÂ†± **Âº∑Âåñ**
        origin = self._extract_origin(raw_text, text_lines)
        if origin:
            structured_data['origin'] = origin
            print(f"‚úÖ ÂéüÁî£ÂõΩ: {origin}")
        
        # 15. ‰øùË®º (Warranty) - ‰øùË®ºÊÉÖÂ†±
        warranty = self._extract_warranty(raw_text, text_lines)
        if warranty:
            structured_data['warranty'] = warranty
            print(f"‚úÖ ‰øùË®º: {warranty}")
        
        # 16. „Çµ„Ç§„Ç∫ (Dimensions) - ÂïÜÂìÅ„Çµ„Ç§„Ç∫ **Âº∑Âåñ**
        dimensions = self._extract_dimensions(raw_text, text_lines)
        if dimensions:
            structured_data['dimensions'] = dimensions
            structured_data['product_size'] = dimensions  # ÂçòÂìÅ„Çµ„Ç§„Ç∫„Å®„Åó„Å¶„ÇÇË®≠ÂÆö
            print(f"‚úÖ ÂïÜÂìÅ„Çµ„Ç§„Ç∫: {dimensions}")
        
        # 17. „Éë„ÉÉ„Ç±„Éº„Ç∏„Çµ„Ç§„Ç∫ (Package Size) **Êñ∞Ë¶èËøΩÂä†**
        package_size = self._extract_package_size(raw_text, text_lines)
        if package_size:
            structured_data['package_size'] = package_size
            print(f"‚úÖ „Éë„ÉÉ„Ç±„Éº„Ç∏„Çµ„Ç§„Ç∫: {package_size}")
        
        # 18. ÂÜÖÁÆ±„Çµ„Ç§„Ç∫ (Inner Box Size) **Êñ∞Ë¶èËøΩÂä†**
        inner_box_size = self._extract_inner_box_size(raw_text, text_lines)
        if inner_box_size:
            structured_data['inner_box_size'] = inner_box_size
            print(f"‚úÖ ÂÜÖÁÆ±„Çµ„Ç§„Ç∫: {inner_box_size}")
        
        # 19. „Ç´„Éº„Éà„É≥„Çµ„Ç§„Ç∫ (Carton Size) **Êñ∞Ë¶èËøΩÂä†**
        carton_size = self._extract_carton_size(raw_text, text_lines)
        if carton_size:
            structured_data['carton_size'] = carton_size
            print(f"‚úÖ „Ç´„Éº„Éà„É≥„Çµ„Ç§„Ç∫: {carton_size}")
        
        # 20. „Éë„ÉÉ„Ç±„Éº„Ç∏ÂΩ¢ÊÖã (Package Type) **Êñ∞Ë¶èËøΩÂä†**
        package_type = self._extract_package_type(raw_text, text_lines)
        if package_type:
            structured_data['package_type'] = package_type
            structured_data['packaging_material'] = package_type  # ‰øùÊùê„Éï„Ç£„É´„É†„Å®„Åó„Å¶„ÇÇË®≠ÂÆö
            print(f"‚úÖ „Éë„ÉÉ„Ç±„Éº„Ç∏ÂΩ¢ÊÖã: {package_type}")
        
        # 21. ÂÖ•Êï∞ (Quantity per Pack) **Êñ∞Ë¶èËøΩÂä†**
        quantity_per_pack = self._extract_quantity_per_pack(raw_text, text_lines)
        if quantity_per_pack:
            structured_data['quantity_per_pack'] = quantity_per_pack
            structured_data['case_quantity'] = int(quantity_per_pack) if quantity_per_pack.isdigit() else None
            print(f"‚úÖ ÂÖ•Êï∞: {quantity_per_pack}")
        
        # 22. ÂØæË±°Âπ¥ÈΩ¢ (Target Age) **Êñ∞Ë¶èËøΩÂä†**
        target_age = self._extract_target_age(raw_text, text_lines)
        if target_age:
            structured_data['target_age'] = target_age
            print(f"‚úÖ ÂØæË±°Âπ¥ÈΩ¢: {target_age}")
        
        # 23. GTINÊÉÖÂ†± (Inner/Outer Box GTIN) **Êñ∞Ë¶èËøΩÂä†**
        inner_gtin = self._extract_inner_box_gtin(raw_text)
        if inner_gtin:
            structured_data['inner_box_gtin'] = inner_gtin
            print(f"‚úÖ ÂÜÖÁÆ±GTIN: {inner_gtin}")
            
        outer_gtin = self._extract_outer_box_gtin(raw_text)
        if outer_gtin:
            structured_data['outer_box_gtin'] = outer_gtin
            print(f"‚úÖ Â§ñÁÆ±GTIN: {outer_gtin}")
        
        # === ËøΩÂä†„ÅÆ38È†ÖÁõÆ„Éï„Ç£„Éº„É´„Éâ ===
        
        # 24. „É≠„ÉÉ„ÉàÁï™Âè∑ (Lot Number)
        lot_number = self._extract_lot_number(raw_text)
        if lot_number:
            structured_data['lot_number'] = lot_number
            print(f"‚úÖ „É≠„ÉÉ„ÉàÁï™Âè∑: {lot_number}")
        
        # 25. Âå∫ÂàÜ (Classification)
        classification = self._extract_classification(raw_text)
        if classification:
            structured_data['classification'] = classification
            print(f"‚úÖ Âå∫ÂàÜ: {classification}")
        
        # 26. Â§ßÂàÜÈ°û (Major Category)
        major_category = self._extract_major_category(raw_text, text_lines)
        if major_category:
            structured_data['major_category'] = major_category
            print(f"‚úÖ Â§ßÂàÜÈ°û: {major_category}")
        
        # 27. ‰∏≠ÂàÜÈ°û (Minor Category)
        minor_category = self._extract_minor_category(raw_text, text_lines)
        if minor_category:
            structured_data['minor_category'] = minor_category
            print(f"‚úÖ ‰∏≠ÂàÜÈ°û: {minor_category}")
        
        # 28. ÂïÜÂìÅÁï™Âè∑ (Product Code) - SKU„Å®Âêå„ÅòÂ†¥Âêà„Åå„ÅÇ„Çã
        product_code = self._extract_product_code(raw_text, text_lines)
        if product_code:
            structured_data['product_code'] = product_code
            print(f"‚úÖ ÂïÜÂìÅÁï™Âè∑: {product_code}")
        
        # 29. „Ç§„É≥„Çπ„Éà„Ç¢ (In-Store)
        in_store = self._extract_in_store(raw_text)
        if in_store:
            structured_data['in_store'] = in_store
            print(f"‚úÖ „Ç§„É≥„Çπ„Éà„Ç¢: {in_store}")
        
        # 30. „Ç∏„É£„É≥„É´ÂêçÁß∞ (Genre Name)
        genre_name = self._extract_genre_name(raw_text, text_lines)
        if genre_name:
            structured_data['genre_name'] = genre_name
            print(f"‚úÖ „Ç∏„É£„É≥„É´ÂêçÁß∞: {genre_name}")
        
        # 31. ‰ªïÂÖ•ÂÖà (Supplier Name)
        supplier_name = self._extract_supplier_name(raw_text)
        if supplier_name:
            structured_data['supplier_name'] = supplier_name
            print(f"‚úÖ ‰ªïÂÖ•ÂÖà: {supplier_name}")
        
        # 32. „É°„Éº„Ç´„ÉºÂêçÁß∞ (IP Name) - IPÂêç„Å®„Åó„Å¶‰ΩøÁî®
        ip_name = self._extract_ip_name(raw_text, cleaned_lines)
        if ip_name:
            structured_data['ip_name'] = ip_name
            print(f"‚úÖ „É°„Éº„Ç´„ÉºÂêçÁß∞: {ip_name}")
        
        # 33. „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç (Character Name)
        character_name = self._extract_character_name(raw_text, text_lines)
        if character_name:
            structured_data['character_name'] = character_name
            print(f"‚úÖ „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç: {character_name}")
        
        # 34. ÂèÇËÄÉË≤©Â£≤‰æ°Ê†º (Reference Sales Price)
        reference_sales_price = self._extract_reference_sales_price(raw_text)
        if reference_sales_price:
            structured_data['reference_sales_price'] = reference_sales_price
            print(f"‚úÖ ÂèÇËÄÉË≤©Â£≤‰æ°Ê†º: {reference_sales_price}")
        
        # 35. Âç∏Âçò‰æ°ÔºàÊäúÔºâ (Wholesale Price)
        wholesale_price = self._extract_wholesale_price(raw_text)
        if wholesale_price:
            structured_data['wholesale_price'] = wholesale_price
            print(f"‚úÖ Âç∏Âçò‰æ°: {wholesale_price}")
        
        # 36. Âç∏ÂèØËÉΩÊï∞ (Wholesale Quantity)
        wholesale_quantity = self._extract_wholesale_quantity(raw_text)
        if wholesale_quantity:
            structured_data['wholesale_quantity'] = wholesale_quantity
            print(f"‚úÖ Âç∏ÂèØËÉΩÊï∞: {wholesale_quantity}")
        
        # 37. Áô∫Ê≥®ÈáëÈ°ç (Order Amount)
        order_amount = self._extract_order_amount(raw_text)
        if order_amount:
            structured_data['order_amount'] = order_amount
            print(f"‚úÖ Áô∫Ê≥®ÈáëÈ°ç: {order_amount}")
        
        # 38. ‰∫àÁ¥ÑËß£Á¶ÅÊó• (Reservation Release Date)
        reservation_release_date = self._extract_reservation_release_date(raw_text)
        if reservation_release_date:
            structured_data['reservation_release_date'] = reservation_release_date
            print(f"‚úÖ ‰∫àÁ¥ÑËß£Á¶ÅÊó•: {reservation_release_date}")
        
        # 39. ‰∫àÁ¥ÑÁ∑†„ÇÅÂàá„ÇäÊó• (Reservation Deadline)
        reservation_deadline = self._extract_reservation_deadline(raw_text)
        if reservation_deadline:
            structured_data['reservation_deadline'] = reservation_deadline
            print(f"‚úÖ ‰∫àÁ¥ÑÁ∑†„ÇÅÂàá„ÇäÊó•: {reservation_deadline}")
        
        # 40. ‰∫àÁ¥ÑÂïÜÂìÅÁô∫ÈÄÅ‰∫àÂÆöÊó• (Reservation Shipping Date)
        reservation_shipping_date = self._extract_reservation_shipping_date(raw_text)
        if reservation_shipping_date:
            structured_data['reservation_shipping_date'] = reservation_shipping_date
            print(f"‚úÖ ‰∫àÁ¥ÑÂïÜÂìÅÁô∫ÈÄÅ‰∫àÂÆöÊó•: {reservation_shipping_date}")
        
        # 41. „Ç±„Éº„ÇπÊ¢±ÂÖ•Êï∞ (Case Pack Quantity)
        case_pack_quantity = self._extract_case_pack_quantity(raw_text)
        if case_pack_quantity:
            structured_data['case_pack_quantity'] = case_pack_quantity
            print(f"‚úÖ „Ç±„Éº„ÇπÊ¢±ÂÖ•Êï∞: {case_pack_quantity}")
        
        # 42. ÂçòÂìÅ„Çµ„Ç§„Ç∫ (Single Product Size)
        single_product_size = self._extract_single_product_size(raw_text, text_lines)
        if single_product_size:
            structured_data['single_product_size'] = single_product_size
            print(f"‚úÖ ÂçòÂìÅ„Çµ„Ç§„Ç∫: {single_product_size}")
        
        # 43. Ê©üÊùê„Éï„Ç£„É´„É† (Protective Film Material)
        protective_film = self._extract_protective_film_material(raw_text)
        if protective_film:
            structured_data['protective_film_material'] = protective_film
            print(f"‚úÖ Ê©üÊùê„Éï„Ç£„É´„É†: {protective_film}")
        
        # 44. ÂéüÁî£ÂõΩ (Country of Origin) - „Çà„ÇäÂº∑Âåñ„Åï„Çå„ÅüÊäΩÂá∫
        country_of_origin = self._extract_country_of_origin(raw_text, text_lines)
        if country_of_origin:
            structured_data['country_of_origin'] = country_of_origin
            print(f"‚úÖ ÂéüÁî£ÂõΩ: {country_of_origin}")
        
        # 45-50. ÁîªÂÉèURL (Image 1-6)
        for i in range(1, 7):
            image_url = self._extract_image_url(raw_text, i)
            if image_url:
                structured_data[f'image{i}'] = image_url
                print(f"‚úÖ ÁîªÂÉè{i}: {image_url}")
        
        return structured_data
    
    def _detect_multiple_products(self, raw_text: str) -> list:
        """Ë§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫„Åó„Å¶ÂÄãÂà•„Å´ÊäΩÂá∫"""
        if not raw_text:
            return []
        
        print(f"üîç MULTI-PRODUCT DETECTION: Analyzing {len(raw_text)} characters")
        print(f"üìù RAW TEXT PREVIEW (first 500 chars):")
        print(f"{raw_text[:500]}...")
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        products = []
        
        # 1. JAN„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥„ÅßÂïÜÂìÅ„ÇíÂàÜÈõ¢ÔºàÊúÄÂÑ™ÂÖàÔºâ
        jan_patterns = re.findall(r'\b(4\d{12})\b', raw_text)
        # „Éè„Ç§„Éï„É≥‰ªò„ÅçJAN„Ç≥„Éº„Éâ„ÇÇÊ§úÂá∫
        jan_patterns_with_hyphen = re.findall(r'4970381-(\d{6})', raw_text)
        if jan_patterns_with_hyphen:
            jan_patterns.extend([f"4970381{code}" for code in jan_patterns_with_hyphen])
        
        # ST-„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥„ÇÇÊ§úÂá∫„Åó„Å¶ÂïÜÂìÅ„ÇíÂàÜÈõ¢
        st_patterns = re.findall(r'ST-\d{2}[A-Z]{2}', raw_text)
        print(f"üîç JAN PATTERNS FOUND: {jan_patterns}")
        print(f"üîç ST-CODE PATTERNS FOUND: {st_patterns}")
        
        # ST-„Ç≥„Éº„Éâ„ÅåË§áÊï∞„ÅÇ„ÇãÂ†¥Âêà„ÇÇÂº∑Âà∂ÁöÑ„Å´„Éû„É´„ÉÅ„Éó„É≠„ÉÄ„ÇØ„Éà„Å®„Åó„Å¶Âá¶ÁêÜ
        if len(st_patterns) > 1:
            print(f"üîß FORCING MULTI-PRODUCT BY ST-CODES: {len(st_patterns)} ST-codes detected")
            
            # ST-„Ç≥„Éº„Éâ„Å®JAN„Ç≥„Éº„Éâ„ÅÆÊ≠£Á¢∫„Å™„Éû„ÉÉ„Éî„É≥„Ç∞„Çí‰ΩúÊàê
            st_jan_mapping = self._create_st_jan_mapping(raw_text, st_patterns, jan_patterns)
            print(f"üîó ST-JAN MAPPING: {st_jan_mapping}")
            
            # ÂêÑST-„Ç≥„Éº„Éâ„Å´ÂØæ„Åó„Å¶ÂÄãÂà•„ÅÆÂïÜÂìÅ„Çí‰ΩúÊàê
            for i, st_code in enumerate(st_patterns):
                # Ë©≤ÂΩìST-„Ç≥„Éº„Éâ„Å´Âü∫„Å•„ÅÑ„Å¶„Çà„ÇäÁ≤æÂØÜ„Å™„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
                st_section = self._extract_precise_section_by_st_code(raw_text, st_code, st_patterns)
                
                print(f"   üéØ Processing ST-Code: {st_code}")
                
                # üîß „ÇØ„É™„Éº„É≥„Å™ÂïÜÂìÅ„Éá„Éº„Çø„Çí‰ΩúÊàêÔºàÈñìÈÅï„Å£„ÅüÊÉÖÂ†±„ÇíÁ∂ôÊâø„Åó„Å™„ÅÑÔºâ
                product_data = self._create_clean_product_data_for_st_code(st_code, st_section, i + 1)
                
                products.append(product_data)
                print(f"   ‚úÖ ST-Code Product {i+1}: {product_data.get('product_name', 'Unknown')} [{st_code}] JAN: {product_data.get('jan_code', 'N/A')}")
                print(f"      üìù Character: {self._get_character_for_st_code(st_code)}")
                print(f"      üî¢ JAN: {product_data.get('jan_code', 'N/A')}")
                print(f"      üì¶ SKU: {st_code}")
            
            return products
        
        # JAN„Ç≥„Éº„Éâ„ÅåË§áÊï∞„ÅÇ„ÇãÂ†¥Âêà„ÅØÂº∑Âà∂ÁöÑ„Å´„Éû„É´„ÉÅ„Éó„É≠„ÉÄ„ÇØ„Éà„Å®„Åó„Å¶Âá¶ÁêÜ
        if len(jan_patterns) > 1:
            print(f"üîß FORCING MULTI-PRODUCT: {len(jan_patterns)} JAN codes detected, creating individual products")
            # ÂêÑJAN„Ç≥„Éº„Éâ„Å´ÂØæ„Åó„Å¶ÂÄãÂà•„ÅÆÂïÜÂìÅ„Çí‰ΩúÊàê
            for i, jan_code in enumerate(jan_patterns):
                # JAN„Ç≥„Éº„Éâ„Åã„Çâ„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Å®ST-„Ç≥„Éº„Éâ„ÇíÈÄÜÂºï„Åç
                character_name = self._get_character_for_jan_code(jan_code)
                st_code = self._get_st_code_for_jan_code(jan_code)
                
                # Ë©≤ÂΩìJAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄ„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
                jan_section = self._extract_section_by_jan(raw_text, jan_code)
                product_data = self._parse_product_data_from_text(jan_section)
                if product_data:
                    product_data['product_index'] = i + 1
                    product_data['section_text'] = jan_section[:300] + "..." if len(jan_section) > 300 else jan_section
                    product_data['jan_code'] = jan_code  # Á¢∫ÂÆü„Å´JAN„Ç≥„Éº„Éâ„ÇíË®≠ÂÆö
                    
                    # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Å®ST-„Ç≥„Éº„Éâ„ÇíË®≠ÂÆö
                    if character_name:
                        product_data['product_name'] = f"{character_name} „Ç≥„Ç§„É≥„Éê„É≥„ÇØ"
                        product_data['description'] = f'{character_name}„ÅÆÂèØÊÑõ„ÅÑË≤ØÈáëÁÆ±„Åß„Åô„ÄÇ„Ç§„É≥„ÉÜ„É™„Ç¢„Å®„Åó„Å¶„ÇÇÊ•Ω„Åó„ÇÅ„Åæ„Åô„ÄÇ'
                        print(f"   üë§ Character identified: {character_name}")
                    
                    if st_code:
                        product_data['sku'] = st_code
                        print(f"   üéØ ST-Code identified: {st_code}")
                    
                    # ÂïÜÂìÅ„Çµ„Ç§„Ç∫Ë®≠ÂÆö
                    if not product_data.get('dimensions'):
                        product_data['dimensions'] = "Á¥Ñ107√ó70√ó61mm"
                        product_data['product_size'] = "Á¥Ñ107√ó70√ó61mm"
                    
                    # „Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫„ÅÆËøΩÂä†ÊÉÖÂ†±
                    product_data['category'] = '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫'
                    product_data['brand'] = '„Ç®„É≥„Çπ„Ç´„Ç§'
                    product_data['manufacturer'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                    product_data['origin'] = 'Êó•Êú¨'
                    product_data['target_age'] = '3Ê≠≥‰ª•‰∏ä'
                    
                    products.append(product_data)
                    print(f"   ‚úÖ JAN-based Product {i+1}: {product_data.get('product_name', 'Unknown')} JAN: {jan_code} SKU: {st_code or 'N/A'}")
            
            return products
        
        # 2. ÂïÜÂìÅÂêç„Éë„Çø„Éº„É≥„ÅßËøΩÂä†Ê§úÂá∫ÔºàEN-„Ç≥„Éº„Éâ„ÄÅST-„Ç≥„Éº„Éâ„Éô„Éº„ÇπÔºâ
        elif self._has_multiple_st_codes(text_lines) or self._has_multiple_en_codes(text_lines):
            print("üéØ Detected multiple EN/ST-code products")
            # EN„Ç≥„Éº„Éâ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØEN„Ç≥„Éº„Éâ„ÅßÂàÜÂâ≤„ÄÅ„Åù„ÅÜ„Åß„Å™„Åë„Çå„Å∞ST„Ç≥„Éº„Éâ„ÅßÂàÜÂâ≤
            if self._has_multiple_en_codes(text_lines):
                product_sections = self._split_by_en_codes(text_lines)
            else:
                product_sections = self._split_by_st_codes(text_lines)
            
            for i, section in enumerate(product_sections):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data and product_data.get('product_name'):
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        products.append(product_data)
                        print(f"   ‚úÖ Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        
        # 3. Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÅÆÂ†¥Âêà„ÅØË°å„Éô„Éº„Çπ„ÅßÂàÜÈõ¢
        elif self._detect_table_structure(text_lines):
            print("üéØ Detected table structure with multiple products")
            product_sections = self._split_table_rows(text_lines)
            
            for i, section in enumerate(product_sections):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data and product_data.get('product_name'):
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        products.append(product_data)
                        print(f"   ‚úÖ Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        
        # 4. „Éù„Ç±„É¢„É≥„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÅßË§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫
        elif self._detect_multiple_pokemon_characters(raw_text):
            print("üéØ Detected multiple Pokemon characters in catalog")
            character_products = self._split_by_pokemon_characters(raw_text)
            
            for i, (character, section) in enumerate(character_products):
                if section.strip():
                    product_data = self._parse_product_data_from_text(section)
                    if product_data:
                        product_data['product_index'] = i + 1
                        product_data['section_text'] = section[:300] + "..." if len(section) > 300 else section
                        # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂïÜÂìÅÂêç„Å´Âê´„ÇÅ„Çã
                        if not product_data.get('product_name') or character not in product_data['product_name']:
                            product_data['product_name'] = f"„Éù„Ç±„É¢„É≥„Ç≥„Ç§„É≥„Éê„É≥„ÇØ {character}"
                        # „Éù„Ç±„É¢„É≥„Ç∞„ÉÉ„Ç∫„ÅÆËøΩÂä†ÊÉÖÂ†±
                        product_data['category'] = '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫'
                        product_data['brand'] = '„Ç®„É≥„Çπ„Ç´„Ç§'
                        product_data['manufacturer'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
                        products.append(product_data)
                        print(f"   ‚úÖ Pokemon Product {i+1}: {product_data.get('product_name', 'Unknown')}")

        # 5. „ÄåÂÖ®„Äá„ÄáÁ®ÆÈ°û„Äç„Å™„Å©„ÅÆË°®Áèæ„ÅßË§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫
        elif self._detect_multiple_by_count_expression(raw_text):
            print("üéØ Detected multiple products by count expression (ÂÖ®„Äá„ÄáÁ®ÆÈ°û)")
            # „Ç´„Éº„Éâ„Éª„Ç∞„ÉÉ„Ç∫Á≥ª„ÅÆË§áÊï∞ÂïÜÂìÅ„Å®„Åó„Å¶Êâ±„ÅÜ
            count_match = re.search(r'ÂÖ®(\d+)Á®ÆÈ°û', raw_text)
            if count_match:
                total_count = int(count_match.group(1))
                print(f"   üìä Total products indicated: {total_count}")
                
                # Âü∫Êú¨ÂïÜÂìÅ„Éá„Éº„Çø„ÇíÂèñÂæó
                base_product = self._parse_product_data_from_text(raw_text)
                
                # Ë§áÊï∞ÂïÜÂìÅ„Å®„Åó„Å¶ÊúÄÂ§ß10ÂïÜÂìÅ„Åæ„ÅßÁîüÊàêÔºàÂÆüÈöõ„ÅÆÂïÜÂìÅÊï∞„Åæ„Åü„ÅØUIË°®Á§∫Áî®Ôºâ
                max_display_products = min(total_count, 10)
                for i in range(max_display_products):
                    product_data = base_product.copy()
                    product_data['product_name'] = f"{base_product.get('product_name', 'Unknown')} - „Çø„Ç§„Éó{i+1}"
                    product_data['product_index'] = i + 1
                    product_data['section_text'] = f"Product {i+1} from {total_count} total variants"
                    products.append(product_data)
                    print(f"   ‚úÖ Product {i+1}: {product_data.get('product_name', 'Unknown')}")
        

        
        final_products = products if len(products) > 1 else []
        if final_products:
            print(f"üéâ MULTI-PRODUCT SUCCESS: Detected {len(final_products)} products")
        else:
            print("üìù SINGLE PRODUCT: No multiple products detected")
        
        return final_products
    
    def _detect_multiple_by_count_expression(self, raw_text: str) -> bool:
        """„ÄåÂÖ®„Äá„ÄáÁ®ÆÈ°û„Äç„Å™„Å©„ÅÆË°®Áèæ„ÅßË§áÊï∞ÂïÜÂìÅ„ÇíÊ§úÂá∫"""
        count_patterns = [
            r'ÂÖ®(\d+)Á®ÆÈ°û',
            r'ÂÖ®(\d+)Á®Æ',
            r'(\d+)Á®ÆÈ°û',
            r'(\d+)„Çø„Ç§„Éó',
            r'(\d+)„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥'
        ]
        
        for pattern in count_patterns:
            match = re.search(pattern, raw_text)
            if match:
                count = int(match.group(1))
                if count > 1:  # 2Á®ÆÈ°û‰ª•‰∏ä„Å™„ÇâË§áÊï∞ÂïÜÂìÅ
                    print(f"üîç Found count expression: {match.group(0)} ({count} products)")
                    return True
        return False
    
    def _split_by_jan_codes(self, raw_text: str, jan_codes: list) -> list:
        """JAN„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        text_parts = raw_text
        
        for jan_code in jan_codes:
            # JAN„Ç≥„Éº„ÉâÂë®Ëæ∫„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
            jan_index = text_parts.find(jan_code)
            if jan_index != -1:
                # JAN„Ç≥„Éº„Éâ„ÅÆÂâçÂæå300ÊñáÂ≠ó„ÇíÂïÜÂìÅ„Çª„ÇØ„Ç∑„Éß„É≥„Å®„Åó„Å¶ÊäΩÂá∫
                start = max(0, jan_index - 300)
                end = min(len(text_parts), jan_index + 300)
                section = text_parts[start:end]
                sections.append(section)
        
        return sections
    
    def _split_by_jan_codes_improved(self, raw_text: str, jan_codes: list) -> list:
        """JAN„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„Çà„ÇäÊ≠£Á¢∫„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        
        # ÂêÑJAN„Ç≥„Éº„Éâ„ÅÆ‰ΩçÁΩÆ„ÇíÁâπÂÆö
        jan_positions = []
        for jan_code in jan_codes:
            matches = list(re.finditer(rf'\b{jan_code}\b', raw_text))
            for match in matches:
                jan_positions.append((match.start(), match.end(), jan_code))
        
        # ‰ΩçÁΩÆÈ†Ü„Å´„ÇΩ„Éº„Éà
        jan_positions.sort(key=lambda x: x[0])
        
        # ÂêÑJAN„Ç≥„Éº„ÉâÂë®Ëæ∫„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
        for i, (start_pos, end_pos, jan_code) in enumerate(jan_positions):
            # „Çª„ÇØ„Ç∑„Éß„É≥ÁØÑÂõ≤„ÇíÊ±∫ÂÆö
            section_start = max(0, start_pos - 400)  # „Çà„ÇäÂ∫É„ÅÑÁØÑÂõ≤
            
            if i < len(jan_positions) - 1:
                next_start = jan_positions[i + 1][0]
                section_end = min(next_start - 50, start_pos + 600)
            else:
                section_end = min(len(raw_text), start_pos + 600)
            
            section = raw_text[section_start:section_end].strip()
            if section and jan_code in section:
                sections.append(section)
        
        return sections
    
    def _has_multiple_st_codes(self, text_lines: list) -> bool:
        """Ë§áÊï∞„ÅÆST-„Ç≥„Éº„Éâ„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
        st_codes = []
        for line in text_lines:
            st_matches = re.findall(r'ST-\w+', line)
            st_codes.extend(st_matches)
        
        unique_st_codes = list(set(st_codes))
        return len(unique_st_codes) > 1
    
    def _has_multiple_en_codes(self, text_lines: list) -> bool:
        """Ë§áÊï∞„ÅÆEN-„Ç≥„Éº„Éâ„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
        en_codes = []
        for line in text_lines:
            en_matches = re.findall(r'EN-\d+', line)
            en_codes.extend(en_matches)
        
        unique_en_codes = list(set(en_codes))
        print(f"üîç Found EN codes: {unique_en_codes}")
        return len(unique_en_codes) > 1
    
    def _split_by_st_codes(self, text_lines: list) -> list:
        """ST-„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        current_section = []
        
        for line in text_lines:
            # ST-„Ç≥„Éº„Éâ„ÅåÂê´„Åæ„Çå„ÇãË°å„ÅßÊñ∞„Åó„ÅÑ„Çª„ÇØ„Ç∑„Éß„É≥ÈñãÂßã
            if re.search(r'ST-\w+', line) and current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            
            current_section.append(line)
        
        # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _extract_section_by_st_code(self, raw_text: str, st_code: str) -> str:
        """ST-„Ç≥„Éº„Éâ„Å´Âü∫„Å•„ÅÑ„Å¶„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫"""
        lines = raw_text.split('\n')
        section_lines = []
        st_code_found = False
        
        for i, line in enumerate(lines):
            if st_code in line:
                st_code_found = True
                # ST-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÅÆÂâçÂæå5Ë°å„ÇíÂèñÂæó
                start_idx = max(0, i - 5)
                end_idx = min(len(lines), i + 10)
                section_lines = lines[start_idx:end_idx]
                break
        
        if not st_code_found:
            # ST-„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÂÖ®‰Ωì„ÅÆ‰∏ÄÈÉ®„ÇíËøî„Åô
            return raw_text[:500]
        
        section_text = '\n'.join(section_lines)
        
        # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÊé®Ê∏¨
        character_mapping = {
            'ST-03CB': '„Éî„Ç´„ÉÅ„É•„Ç¶',
            'ST-04CB': '„Ç§„Éº„Éñ„Ç§', 
            'ST-05CB': '„Éè„É™„Éû„É≠„É≥',
            'ST-06CB': '„Éï„Ç©„ÉÉ„Ç≥',
            'ST-07CB': '„Ç±„É≠„Éû„ÉÑ'
        }
        
        if st_code in character_mapping:
            character_name = character_mapping[st_code]
            section_text = f"{character_name} {section_text}"
        
        return section_text

    def _detect_multiple_pokemon_characters(self, raw_text: str) -> bool:
        """„Éù„Ç±„É¢„É≥„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÅÆË§áÊï∞Ê§úÂá∫"""
        pokemon_characters = [
            '„Éî„Ç´„ÉÅ„É•„Ç¶', '„Ç§„Éº„Éñ„Ç§', '„Éè„É™„Éû„É≠„É≥', '„Éï„Ç©„ÉÉ„Ç≥', '„Ç±„É≠„Éû„ÉÑ',
            '„Éï„Ç∑„ÇÆ„ÉÄ„Éç', '„Éí„Éà„Ç´„Ç≤', '„Çº„Éã„Ç¨„É°', '„ÉÅ„Ç≥„É™„Éº„Çø', '„Éí„Éé„Ç¢„É©„Ç∑',
            '„ÉØ„Éã„Éé„Ç≥', '„Ç≠„É¢„É™', '„Ç¢„ÉÅ„É£„É¢', '„Éü„Ç∫„Ç¥„É≠„Ç¶', '„Éä„Ç®„Éà„É´',
            '„Éí„Ç≥„Ç∂„É´', '„Éù„ÉÉ„ÉÅ„É£„Éû', '„ÉÑ„Çø„Éº„Ç∏„É£', '„Éù„Ç´„Éñ', '„Éü„Ç∏„É•„Éû„É´'
        ]
        
        found_characters = []
        for character in pokemon_characters:
            if character in raw_text:
                found_characters.append(character)
        
        print(f"üîç POKEMON CHARACTERS FOUND: {found_characters}")
        return len(found_characters) > 1

    def _split_by_pokemon_characters(self, raw_text: str) -> list:
        """„Éù„Ç±„É¢„É≥„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Åß„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        pokemon_characters = [
            '„Éî„Ç´„ÉÅ„É•„Ç¶', '„Ç§„Éº„Éñ„Ç§', '„Éè„É™„Éû„É≠„É≥', '„Éï„Ç©„ÉÉ„Ç≥', '„Ç±„É≠„Éû„ÉÑ',
            '„Éï„Ç∑„ÇÆ„ÉÄ„Éç', '„Éí„Éà„Ç´„Ç≤', '„Çº„Éã„Ç¨„É°', '„ÉÅ„Ç≥„É™„Éº„Çø', '„Éí„Éé„Ç¢„É©„Ç∑',
            '„ÉØ„Éã„Éé„Ç≥', '„Ç≠„É¢„É™', '„Ç¢„ÉÅ„É£„É¢', '„Éü„Ç∫„Ç¥„É≠„Ç¶', '„Éä„Ç®„Éà„É´',
            '„Éí„Ç≥„Ç∂„É´', '„Éù„ÉÉ„ÉÅ„É£„Éû', '„ÉÑ„Çø„Éº„Ç∏„É£', '„Éù„Ç´„Éñ', '„Éü„Ç∏„É•„Éû„É´'
        ]
        
        character_sections = []
        lines = raw_text.split('\n')
        
        for character in pokemon_characters:
            if character in raw_text:
                # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂê´„ÇÄË°å„ÇíÊé¢„Åó„Å¶„ÄÅ„Åù„ÅÆÂë®Ëæ∫„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
                for i, line in enumerate(lines):
                    if character in line:
                        start_idx = max(0, i - 3)
                        end_idx = min(len(lines), i + 8)
                        section = '\n'.join(lines[start_idx:end_idx])
                        character_sections.append((character, section))
                        break
        
        return character_sections

    def _extract_section_by_jan(self, raw_text: str, jan_code: str) -> str:
        """ÁâπÂÆö„ÅÆJAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄ„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫ÔºàÊîπËâØÁâàÔºâ"""
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        jan_line_index = -1
        
        # JAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÇíÊé¢„Åô
        for i, line in enumerate(text_lines):
            if jan_code in line or jan_code.replace('-', '') in line or f"{jan_code[:7]}-{jan_code[7:]}" in line:
                jan_line_index = i
                break
        
        if jan_line_index == -1:
            # JAN„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà„ÄÅÂÖ®„ÉÜ„Ç≠„Çπ„Éà„ÇíËøî„Åô
            return raw_text
        
        # „Çà„ÇäÁ≤æÂØÜ„Å™„Çª„ÇØ„Ç∑„Éß„É≥ÊäΩÂá∫
        # ÂïÜÂìÅ„Ç≥„Éº„ÉâÔºàST-„Ç≥„Éº„ÉâÔºâ„ÇíÂü∫Ê∫ñ„Å´„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÂå∫Âàá„Çã
        section_start = jan_line_index
        section_end = jan_line_index + 1
        
        # ‰∏äÂêë„Åç„Å´Ê§úÁ¥¢„Åó„Å¶„ÄÅ„Åì„ÅÆJAN„Ç≥„Éº„Éâ„Å´ÂØæÂøú„Åô„ÇãÂïÜÂìÅÊÉÖÂ†±„ÅÆÈñãÂßãÁÇπ„ÇíÊé¢„Åô
        for i in range(jan_line_index, max(0, jan_line_index - 10), -1):
            line = text_lines[i]
            # ST-„Ç≥„Éº„Éâ„ÄÅÂïÜÂìÅÂêç„ÄÅ„Åæ„Åü„ÅØÂà•„ÅÆJAN„Ç≥„Éº„Éâ„ÅßÂå∫Âàá„Çä
            if re.search(r'ST-\d{2}[A-Z]{2}', line) or 'ÂïÜÂìÅÂêç' in line:
                section_start = i
                break
            # Âà•„ÅÆJAN„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Å£„Åü„Çâ„ÄÅ„Åù„Åì„ÅßÂå∫Âàá„Çä
            if re.search(r'\b4\d{12}\b', line) and jan_code not in line:
                section_start = i + 1
                break
        
        # ‰∏ãÂêë„Åç„Å´Ê§úÁ¥¢„Åó„Å¶„ÄÅ„Åì„ÅÆJAN„Ç≥„Éº„Éâ„Å´ÂØæÂøú„Åô„ÇãÂïÜÂìÅÊÉÖÂ†±„ÅÆÁµÇ‰∫ÜÁÇπ„ÇíÊé¢„Åô
        for i in range(jan_line_index + 1, min(len(text_lines), jan_line_index + 15)):
            line = text_lines[i]
            # Ê¨°„ÅÆÂïÜÂìÅ„ÅÆST-„Ç≥„Éº„Éâ„Åæ„Åü„ÅØJAN„Ç≥„Éº„Éâ„ÅßÂå∫Âàá„Çä
            if re.search(r'ST-\d{2}[A-Z]{2}', line) or re.search(r'\b4\d{12}\b', line):
                section_end = i
                break
            # ÂïÜÂìÅ„Çµ„Ç§„Ç∫„ÅÆË°å„ÅßÁµÇ‰∫Ü
            if 'ÂïÜÂìÅ„Çµ„Ç§„Ç∫' in line:
                section_end = i + 1
                break
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
        section_lines = text_lines[section_start:section_end]
        section_text = '\n'.join(section_lines)
        
        print(f"üîç Extracted section for JAN {jan_code} (lines {section_start}-{section_end}): {section_text[:100]}...")
        return section_text
    
    def _split_by_en_codes(self, text_lines: list) -> list:
        """EN-„Ç≥„Éº„Éâ„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        current_section = []
        
        for line in text_lines:
            # EN-„Ç≥„Éº„Éâ„ÅåÂê´„Åæ„Çå„ÇãË°å„ÅßÊñ∞„Åó„ÅÑ„Çª„ÇØ„Ç∑„Éß„É≥ÈñãÂßã
            if re.search(r'EN-\d+', line) and current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            
            current_section.append(line)
        
        # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥
        if current_section:
            sections.append('\n'.join(current_section))
        
        print(f"üîç Split into {len(sections)} EN-code sections")
        for i, section in enumerate(sections):
            print(f"   Section {i+1}: {section[:100]}...")
        
        return sections
    
    def _detect_table_structure(self, text_lines: list) -> bool:
        """Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÇíÊ§úÂá∫"""
        # Ë°®„ÅÆ„Éò„ÉÉ„ÉÄ„Éº„Ç≠„Éº„ÉØ„Éº„Éâ
        table_headers = [
            'ÂïÜÂìÅÂêç', 'ÂïÜÂìÅ„Ç≥„Éº„Éâ', 'JAN„Ç≥„Éº„Éâ', '‰æ°Ê†º', 'Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º', 
            'Áô∫Â£≤‰∫àÂÆöÊó•', 'ÂÖ•Êï∞', '„Ç´„Éº„Éà„É≥', '„Éë„ÉÉ„Ç±„Éº„Ç∏', '„Çµ„Ç§„Ç∫',
            'EN-', 'ST-', 'Product', 'Code', 'Price'
        ]
        
        # „Éò„ÉÉ„ÉÄ„ÉºË°å„ÇíÊ§úÂá∫
        header_found = False
        data_rows = 0
        
        for line in text_lines:
            # „Éò„ÉÉ„ÉÄ„ÉºË°å„ÅÆÊ§úÂá∫
            if not header_found and any(keyword in line for keyword in table_headers):
                header_found = True
                print(f"üîç TABLE HEADER DETECTED: {line[:100]}")
                continue
            
            # „Éá„Éº„ÇøË°å„ÅÆÊ§úÂá∫
            if header_found:
                # ÂïÜÂìÅ„Ç≥„Éº„Éâ„ÇÑJAN„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å
                if (re.search(r'EN-\d+', line) or 
                    re.search(r'ST-\w+', line) or 
                    re.search(r'4\d{12}', line) or
                    '¬•' in line or 'ÂÜÜ' in line):
                    data_rows += 1
                    print(f"üîç TABLE DATA ROW: {line[:100]}")
        
        result = header_found and data_rows >= 2
        print(f"üîç TABLE DETECTION RESULT: header_found={header_found}, data_rows={data_rows}, is_table={result}")
        return result
    
    def _split_table_rows(self, text_lines: list) -> list:
        """Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÇíË°å„Åî„Å®„Å´ÂàÜÂâ≤"""
        sections = []
        header_found = False
        header_line = ""
        processed_products = set()  # ÈáçË§áÈò≤Ê≠¢
        
        # Ë°®„ÅÆ„Éò„ÉÉ„ÉÄ„Éº„Ç≠„Éº„ÉØ„Éº„Éâ
        table_headers = [
            'ÂïÜÂìÅÂêç', 'ÂïÜÂìÅ„Ç≥„Éº„Éâ', 'JAN„Ç≥„Éº„Éâ', '‰æ°Ê†º', 'Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º', 
            'Áô∫Â£≤‰∫àÂÆöÊó•', 'ÂÖ•Êï∞', '„Ç´„Éº„Éà„É≥', '„Éë„ÉÉ„Ç±„Éº„Ç∏', '„Çµ„Ç§„Ç∫'
        ]
        
        for line in text_lines:
            line = line.strip()
            if not line:
                continue
                
            # „Éò„ÉÉ„ÉÄ„ÉºË°å„ÇíÊ§úÂá∫„Éª‰øùÂ≠ò
            if not header_found and any(keyword in line for keyword in table_headers):
                header_found = True
                header_line = line
                print(f"üîç HEADER SAVED: {header_line}")
                continue
            
            # ÂïÜÂìÅ„Éá„Éº„ÇøË°å„ÇíÊ§úÂá∫ÔºàEN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÅÆ„ÅøÔºâ
            if header_found:
                # EN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÅÆ„Åø„ÇíÂïÜÂìÅ„Éá„Éº„Çø„Å®„Åó„Å¶Ë™çË≠òÔºàÈáçË§á„ÇíÈÅø„Åë„Çã„Åü„ÇÅÔºâ
                en_match = re.search(r'EN-(\d+)', line)
                if en_match:
                    en_code = en_match.group(0)  # EN-1420 „Å™„Å©
                    
                    # Êó¢„Å´Âá¶ÁêÜÊ∏à„Åø„ÅÆÂïÜÂìÅ„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                    if en_code in processed_products:
                        continue
                    
                    processed_products.add(en_code)
                    
                    # „Éò„ÉÉ„ÉÄ„ÉºÊÉÖÂ†±„Å®ÁµÑ„ÅøÂêà„Çè„Åõ„Å¶ÂÆåÊï¥„Å™ÂïÜÂìÅÊÉÖÂ†±„Çí‰ΩúÊàê
                    product_section = f"{header_line}\n{line}"
                    sections.append(product_section)
                    print(f"üîç PRODUCT SECTION CREATED: {en_code} - {line[:100]}")
        
        print(f"üîç TOTAL PRODUCT SECTIONS: {len(sections)}")
        return sections
    
    def _has_multiple_product_names(self, text_lines: list) -> bool:
        """Ë§áÊï∞„ÅÆÂïÜÂìÅÂêç„Åå„ÅÇ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
        product_name_count = 0
        
        for line in text_lines:
            line = line.strip()
            # ÂïÜÂìÅÂêç„Çâ„Åó„ÅÑ„Éë„Çø„Éº„É≥„Çí„Ç´„Ç¶„É≥„Éà
            if any(keyword in line for keyword in ['ST-', '„Éù„Ç±„É¢„É≥', '„Éî„Ç´„ÉÅ„É•„Ç¶', '„Ç≥„Ç§„É≥„Éê„É≥„ÇØ']):
                if len(line) > 10 and len(line) < 100:
                    product_name_count += 1
        
        return product_name_count > 1
    
    def _split_by_product_names(self, text_lines: list) -> list:
        """ÂïÜÂìÅÂêç„ÇíÂü∫Ê∫ñ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂàÜÂâ≤"""
        sections = []
        current_section = []
        
        for line in text_lines:
            line = line.strip()
            
            # Êñ∞„Åó„ÅÑÂïÜÂìÅ„ÅÆÈñãÂßã„ÇíÊ§úÂá∫
            if any(keyword in line for keyword in ['ST-', 'JAN']):
                if current_section:
                    sections.append('\n'.join(current_section))
                    current_section = []
            
            current_section.append(line)
        
        # ÊúÄÂæå„ÅÆ„Çª„ÇØ„Ç∑„Éß„É≥„ÇíËøΩÂä†
        if current_section:
            sections.append('\n'.join(current_section))
        
        return sections
    
    def _clean_repetitive_text(self, text_lines: list) -> list:
        """Áπ∞„ÇäËøî„Åó„ÉÜ„Ç≠„Çπ„Éà„ÇÑ‰∏çË¶Å„Å™„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
        cleaned_lines = []
        seen_lines = set()
        
        for line in text_lines:
            line = line.strip()
            if not line or len(line) < 3:
                continue
            
            # ÂÆåÂÖ®„Å´Âêå„ÅòË°å„ÅØ1Âõû„Å†„Åë‰øùÊåÅ
            if line in seen_lines:
                continue
            seen_lines.add(line)
            
            # Èï∑„Åô„Åé„ÇãË°åÔºàÁπ∞„ÇäËøî„Åó„ÅÆÂèØËÉΩÊÄßÔºâ„Çí„Çπ„Ç≠„ÉÉ„Éó
            if len(line) > 200:
                continue
            
            # Âêå„ÅòÂçòË™û„ÅåÂ§öÊï∞Áπ∞„ÇäËøî„Åï„Çå„ÇãË°å„Çí„Çπ„Ç≠„ÉÉ„Éó
            words = line.split()
            if len(words) > 10:
                word_counts = {}
                for word in words:
                    if len(word) > 2:
                        word_counts[word] = word_counts.get(word, 0) + 1
                
                # Âêå„ÅòÂçòË™û„ÅåË°å„ÅÆ50%‰ª•‰∏ä„ÇíÂç†„ÇÅ„ÇãÂ†¥Âêà„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                max_count = max(word_counts.values()) if word_counts else 0
                if max_count > len(words) * 0.5:
                    continue
            
            cleaned_lines.append(line)
        
        return cleaned_lines
    
    def _extract_product_name(self, text_lines: list, raw_text: str) -> str:
        """ÂïÜÂìÅÂêç„ÇíÊäΩÂá∫"""
        # Èô§Â§ñ„Åô„Åπ„Åç„Éé„Ç§„Ç∫„ÉÜ„Ç≠„Çπ„Éà
        noise_patterns = [
            '„Ç™„É≥„É©„Ç§„É≥„Ç∑„Éß„ÉÉ„Éó', '„Ç¢„Éã„É°„Ç§„Éà„Ç´„Éï„Çß„Çπ„Çø„É≥„Éâ', 'ÈÄöË≤©', 'Êµ∑Â§ñÂ∫óËàó',
            'animatecafe', 'online', 'shop', 'store', 'www.', 'http',
            '‚Äª', 'Ê≥®ÊÑè', 'Ë≠¶Âëä', 'copyright', '¬©', 'reserved'
        ]
        
        # Áπ∞„ÇäËøî„Åó„Éë„Çø„Éº„É≥„ÇíÈô§Â§ñ
        def is_repetitive_text(text):
            """Áπ∞„ÇäËøî„Åó„ÅÆÂ§ö„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„Åã„Å©„ÅÜ„Åã„ÉÅ„Çß„ÉÉ„ÇØ"""
            if len(text) < 10:
                return False
            
            # Âêå„ÅòÊñáÂ≠óÂàó„Åå3Âõû‰ª•‰∏äÁπ∞„ÇäËøî„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ
            words = text.split()
            if len(words) > 6:
                word_counts = {}
                for word in words:
                    if len(word) > 3:  # Áü≠„ÅÑÂçòË™û„ÅØÈô§Â§ñ
                        word_counts[word] = word_counts.get(word, 0) + 1
                
                # Âêå„ÅòÂçòË™û„Åå3Âõû‰ª•‰∏äÂá∫Áèæ„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÁπ∞„ÇäËøî„Åó„ÉÜ„Ç≠„Çπ„Éà„Å®Âà§ÂÆö
                for count in word_counts.values():
                    if count >= 3:
                        return True
            return False
        
        # ÊúâÂäπ„Å™ÂïÜÂìÅÂêçÂÄôË£ú„ÇíÊé¢„Åô
        candidates = []
        
        for line in text_lines:
            line = line.strip()
            
            # Ë°®ÂΩ¢Âºè„Éá„Éº„Çø„ÅÆÂ†¥Âêà„ÄÅ„Éë„Ç§„ÉóÂå∫Âàá„Çä„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
            if '|' in line:
                # „Éë„Ç§„Éó„ÅßÂàÜÂâ≤„Åó„Å¶ÊúÄÂàù„ÅÆÊÑèÂë≥„ÅÆ„ÅÇ„ÇãÈÉ®ÂàÜ„ÇíÂèñÂæó
                parts = [part.strip() for part in line.split('|') if part.strip()]
                if parts:
                    # EN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄÈÉ®ÂàÜ„ÇíÂÑ™ÂÖàÁöÑ„Å´ÈÅ∏Êäû
                    for part in parts:
                        if re.search(r'EN-\d+', part) and len(part) > 10:
                            line = part
                            break
                    else:
                        # EN-„Ç≥„Éº„Éâ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØÊúÄÂàù„ÅÆÈï∑„ÅÑÈÉ®ÂàÜ„Çí‰ΩøÁî®
                        line = next((part for part in parts if len(part) > 10), parts[0] if parts else line)
            
            if len(line) < 5 or len(line) > 200:  # Áü≠„Åô„Åé„Çã„ÉªÈï∑„Åô„Åé„ÇãË°å„ÅØ„Çπ„Ç≠„ÉÉ„Éó
                continue
            
            # „Éé„Ç§„Ç∫„Éë„Çø„Éº„É≥„ÇíÂê´„ÇÄË°å„Çí„Çπ„Ç≠„ÉÉ„Éó
            if any(noise in line for noise in noise_patterns):
                continue
            
            # Áπ∞„ÇäËøî„Åó„ÉÜ„Ç≠„Çπ„Éà„Çí„Çπ„Ç≠„ÉÉ„Éó
            if is_repetitive_text(line):
                continue
            
            # ÂïÜÂìÅÂêç„Çâ„Åó„ÅÑ„Éë„Çø„Éº„É≥„ÇíÂÑ™ÂÖà
            score = 0
            
            # ST-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄÂïÜÂìÅÂêçÔºàÊúÄÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if re.search(r'ST-\w+', line):
                score += 15
                
            # EN-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄÂïÜÂìÅÂêçÔºàÊúÄÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if re.search(r'EN-\d+', line):
                score += 15
                
            # „Éù„Ç±„É¢„É≥Èñ¢ÈÄ£ÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if '„Éù„Ç±„É¢„É≥' in line and any(keyword in line for keyword in ['„Ç≥„Ç§„É≥„Éê„É≥„ÇØ', '„Éï„Ç£„ÇÆ„É•„Ç¢', '„Å¨„ÅÑ„Åê„Çã„Åø', '„Ç´„Éº„Éâ']):
                score += 12
                
            # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if any(keyword in line for keyword in ['„Ç≠„É£„É©„ÇØ„Çø„Éº', '„Çπ„É™„Éº„Éñ', 'ÁîòÁ•û„Åï„Çì', '„ÅÆÁ∏ÅÁµê„Å≥']):
                score += 12
            
            # „Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞Èñ¢ÈÄ£ÂïÜÂìÅÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞' in line and any(keyword in line for keyword in ['„Éê„ÉÉ„Ç∏', '„Ç´„Éº„Éâ', '„Ç≠„É£„É©', '„Éï„Ç£„ÇÆ„É•„Ç¢']):
                score += 10
            
            # „Éê„Éº„Ç∏„Éß„É≥ÊÉÖÂ†±‰ªò„ÅçÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if any(ver in line.lower() for ver in ['ver.', 'version', 'vol.', 'v.']):
                score += 8
            
            # Á®ÆÈ°ûÊï∞‰ªò„ÅçÂïÜÂìÅÂêçÔºàÈ´ò„Çπ„Ç≥„Ç¢Ôºâ
            if 'ÂÖ®' in line and 'Á®Æ' in line:
                score += 8
            
            # ÂïÜÂìÅÂêç„Çâ„Åó„ÅÑ„Ç≠„Éº„ÉØ„Éº„ÉâÔºà‰∏≠„Çπ„Ç≥„Ç¢Ôºâ
            product_keywords = ['ÈôêÂÆö', '„Çª„ÉÉ„Éà', '„Éë„ÉÉ„ÇØ', '„Éú„ÉÉ„ÇØ„Çπ', '„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥', '„Ç∑„É™„Éº„Ç∫', 'ÂàùÂõû']
            for keyword in product_keywords:
                if keyword in line:
                    score += 3
            
            # ÈÅ©Â∫¶„Å™Èï∑„ÅïÔºà‰∏≠„Çπ„Ç≥„Ç¢Ôºâ
            if 10 <= len(line) <= 50:
                score += 2
            
            # Êï∞Â≠ó„ÅßÂßã„Åæ„Çâ„Å™„ÅÑÔºà‰Ωé„Çπ„Ç≥„Ç¢Ôºâ
            if not any(char.isdigit() for char in line[:3]):
                score += 1
            
            if score > 0:
                candidates.append((line, score))
        
        # „Çπ„Ç≥„Ç¢„ÅÆÈ´ò„ÅÑÈ†Ü„Å´„ÇΩ„Éº„Éà
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        # ÊúÄÈ´ò„Çπ„Ç≥„Ç¢„ÅÆÂïÜÂìÅÂêç„ÇíËøî„Åô
        if candidates:
            return candidates[0][0]
        
        return None
    
    def _extract_product_code(self, raw_text: str) -> str:
        """ÂïÜÂìÅ„Ç≥„Éº„Éâ„ÇíÊäΩÂá∫ÔºàEN-XXXX, ST-XXXX „Å™„Å©Ôºâ"""
        # ÂïÜÂìÅ„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥
        code_patterns = [
            r'\b(EN-\d+)\b',  # EN-1234 ÂΩ¢Âºè
            r'\b(ST-\w+)\b',  # ST-XXXX ÂΩ¢Âºè
            r'ÂïÜÂìÅ„Ç≥„Éº„Éâ[Ôºö:\s]*([A-Z0-9-]+)',  # ÂïÜÂìÅ„Ç≥„Éº„Éâ: XXXXX
            r'ÂìÅÁï™[Ôºö:\s]*([A-Z0-9-]+)',  # ÂìÅÁï™: XXXXX
        ]
        
        for pattern in code_patterns:
            match = re.search(pattern, raw_text)
            if match:
                code = match.group(1)
                print(f"‚úÖ PRODUCT CODE FOUND: {code}")
                return code
        
        return None
    
    def _extract_jan_code(self, raw_text: str) -> str:
        """JAN„Ç≥„Éº„Éâ„ÇíÊäΩÂá∫Ôºà8Ê°Å„Åæ„Åü„ÅØ13Ê°ÅÔºâ- „Éê„Éº„Ç≥„Éº„ÉâÁîªÂÉèÂØæÂøúÂº∑ÂåñÁâà"""
        # 13Ê°Å„ÅÆJAN„Ç≥„Éº„ÉâÔºà„Éê„Éº„Ç≥„Éº„Éâ„Åã„Çâ„ÅÆÊäΩÂá∫„ÇíÊúÄÂÑ™ÂÖàÔºâ
        jan_13_patterns = [
            r'\b(4\d{12})\b',  # 4„ÅßÂßã„Åæ„Çã13Ê°ÅÔºàÊúÄ„ÇÇ‰∏ÄËà¨ÁöÑÔºâ
            r'(4970381\d{6})',  # „Ç®„É≥„Çπ„Ç´„Ç§„ÅÆÁâπÂÆö„Éë„Çø„Éº„É≥
            r'4970381[-\s]?(\d{6})',  # „Éè„Ç§„Éï„É≥„Åæ„Åü„ÅØ„Çπ„Éö„Éº„Çπ‰ªò„Åç„Ç®„É≥„Çπ„Ç´„Ç§„Éë„Çø„Éº„É≥
            r'JAN[„Ç≥„Éº„ÉâÔºö:\s]*(4\d{12})',  # JAN„Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'ÂçòÂìÅ\s*JAN[„Ç≥„Éº„ÉâÔºö:\s]*(4\d{12})',  # ÂçòÂìÅJAN„Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'„Ç≥„Éº„Éâ[Ôºö:\s]*(4\d{12})',  # „Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'„Éê„Éº„Ç≥„Éº„Éâ[Ôºö:\s]*(4\d{12})',  # „Éê„Éº„Ç≥„Éº„Éâ: 4XXXXXXXXXXXX
            r'(\d{13})',  # ‰ªªÊÑè„ÅÆ13Ê°ÅÔºà„Éê„Éº„Ç≥„Éº„Éâ‰∏ã„ÅÆÊï∞Â≠óÔºâ
        ]
        
        print(f"üîç JAN„Ç≥„Éº„ÉâÊäΩÂá∫ÈñãÂßã: {raw_text[:100]}...")
        
        for i, pattern in enumerate(jan_13_patterns):
            matches = re.findall(pattern, raw_text)
            for match in matches:
                if isinstance(match, tuple):
                    # „Éè„Ç§„Éï„É≥‰ªò„Åç„ÅÆÂ†¥Âêà
                    if len(match) == 1:
                        jan_code = f"4970381{match[0]}"
                    else:
                        jan_code = match[0] if match[0] else match[1]
                else:
                    jan_code = match
                
                # JAN code validation
                if len(jan_code) == 13 and jan_code.isdigit():
                    # „Çà„ÇäÂé≥ÂØÜ„Å™JAN„Ç≥„Éº„Éâ„ÉÅ„Çß„ÉÉ„ÇØ
                    if jan_code.startswith('4'):
                        print(f"‚úÖ JAN CODE FOUND (pattern {i+1}): {jan_code}")
                        return jan_code
                    elif jan_code.startswith('49') or jan_code.startswith('45'):
                        print(f"‚úÖ JAN CODE FOUND (Japan specific): {jan_code}")
                        return jan_code
        
        # 8Ê°Å„ÅÆJAN„Ç≥„Éº„ÉâÔºàÁü≠Á∏ÆÂΩ¢Ôºâ- „Éê„Éº„Ç≥„Éº„Éâ„Åã„Çâ„ÇÇÊäΩÂá∫
        jan_8_patterns = [
            r'\b(\d{8})\b',
            r'Áü≠Á∏Æ[„Ç≥„Éº„ÉâÔºö:\s]*(\d{8})',
            r'8Ê°Å[„Ç≥„Éº„ÉâÔºö:\s]*(\d{8})',
        ]
        
        for pattern in jan_8_patterns:
            match = re.search(pattern, raw_text)
            if match:
                jan_code = match.group(1)
                if jan_code.isdigit() and len(jan_code) == 8:
                    print(f"‚úÖ JAN CODE FOUND (8-digit): {jan_code}")
                    return jan_code
        
        # Additional fallback for any 13-digit number that looks like a JAN code
        all_numbers = re.findall(r'\b(\d{13})\b', raw_text)
        for number in all_numbers:
            if number.startswith(('4', '49', '45')):
                print(f"‚úÖ JAN CODE FOUND (fallback): {number}")
                return number
        
        print("‚ùå No JAN code found in text")
        return None
    
    def _extract_price(self, raw_text: str) -> str:
        """‰æ°Ê†º„ÇíÊäΩÂá∫"""
        # ¬•Ë®òÂè∑‰ªò„Åç„ÅÆ‰æ°Ê†º
        yen_prices = re.findall(r'¬•\s*([0-9,]+)', raw_text)
        for price_str in yen_prices:
            price_num = int(price_str.replace(',', ''))
            if 50 <= price_num <= 100000:  # ÁèæÂÆüÁöÑ„Å™‰æ°Ê†ºÁØÑÂõ≤
                return f"¬•{price_str}"
        
        # ‰æ°Ê†º„ÄÅÂÄ§ÊÆµ„Å™„Å©„ÅÆÊñáÂ≠ó„ÅÆÂæå„ÅÆÊï∞Â≠ó
        price_patterns = [
            r'‰æ°Ê†º[Ôºö:\s]*¬•?([0-9,]+)',
            r'ÂÄ§ÊÆµ[Ôºö:\s]*¬•?([0-9,]+)',
            r'ÂÆö‰æ°[Ôºö:\s]*¬•?([0-9,]+)',
            r'Á®éËæº[Ôºö:\s]*¬•?([0-9,]+)',
            r'([0-9,]+)\s*ÂÜÜ'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, raw_text)
            for price_str in matches:
                price_num = int(price_str.replace(',', ''))
                if 50 <= price_num <= 100000:
                    return f"¬•{price_str}"
        
        return None
    
    def _extract_stock(self, raw_text: str, text_lines: list) -> int:
        """Âú®Â∫´Êï∞„ÇíÊäΩÂá∫"""
        # Âú®Â∫´Èñ¢ÈÄ£„ÅÆ„Ç≠„Éº„ÉØ„Éº„ÉâÂæå„ÅÆÊï∞Â≠ó
        stock_patterns = [
            r'Âú®Â∫´[Ôºö:\s]*(\d+)',
            r'Êï∞Èáè[Ôºö:\s]*(\d+)',
            r'ÊÆã„Çä[Ôºö:\s]*(\d+)',
            r'stock[Ôºö:\s]*(\d+)',
            r'qty[Ôºö:\s]*(\d+)'
        ]
        
        for pattern in stock_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        
        return None
    
    def _extract_category(self, raw_text: str) -> str:
        """„Ç´„ÉÜ„Ç¥„É™„ÇíÊäΩÂá∫„ÉªÊé®ÂÆö"""
        # Áõ¥Êé•ÁöÑ„Å™„Ç´„ÉÜ„Ç¥„É™Ë°®Ë®ò
        category_patterns = [
            r'„Ç´„ÉÜ„Ç¥„É™[Ôºö:\s]*([^\n\r]+)',
            r'ÂàÜÈ°û[Ôºö:\s]*([^\n\r]+)',
            r'„Ç∏„É£„É≥„É´[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in category_patterns:
            match = re.search(pattern, raw_text)
            if match:
                return match.group(1).strip()
        
        # „Ç≠„Éº„ÉØ„Éº„Éâ„Éô„Éº„Çπ„ÅÆÊé®ÂÆö
        if '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞' in raw_text:
            if '„Ç´„Éº„Éâ' in raw_text:
                return '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞„Ç´„Éº„Éâ'
            elif any(keyword in raw_text for keyword in ['„Éê„ÉÉ„Ç∏', 'Áº∂„Éê„ÉÉ„Ç∏', '„Ç∞„ÉÉ„Ç∫']):
                return '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞„Ç∞„ÉÉ„Ç∫'
        
        category_keywords = {
            '„Éï„Ç£„ÇÆ„É•„Ç¢': ['„Éï„Ç£„ÇÆ„É•„Ç¢', 'figure', '„Å≠„Çì„Å©„Çç„ÅÑ„Å©'],
            '„Ç≤„Éº„É†': ['„Ç≤„Éº„É†', 'game', '„ÇΩ„Éï„Éà'],
            '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫': ['„Ç¢„Éã„É°', '„Ç≠„É£„É©„ÇØ„Çø„Éº', 'anime'],
            'Êú¨„ÉªÈõëË™å': ['Êú¨', 'ÈõëË™å', 'book', 'magazine'],
            'Èü≥Ê•Ω': ['CD', 'DVD', '„Éñ„É´„Éº„É¨„Ç§', '„Çµ„Ç¶„É≥„Éâ„Éà„É©„ÉÉ„ÇØ']
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in raw_text for keyword in keywords):
                return category
        
        return None
    
    def _extract_release_date(self, raw_text: str) -> str:
        """Áô∫Â£≤‰∫àÂÆöÊó•„ÇíÊäΩÂá∫"""
        # Êó•‰ªò„Éë„Çø„Éº„É≥
        date_patterns = [
            r'(\d{4})Âπ¥(\d{1,2})Êúà(\d{1,2})Êó•',  # 2024Âπ¥12Êúà15Êó•
            r'(\d{4})Âπ¥(\d{1,2})Êúà',            # 2024Âπ¥12Êúà
            r'(\d{4})/(\d{1,2})/(\d{1,2})',    # 2024/12/15
            r'(\d{4})-(\d{1,2})-(\d{1,2})',    # 2024-12-15
            r'(\d{1,2})/(\d{1,2})/(\d{4})',    # 12/15/2024
        ]
        
        # Áô∫Â£≤Êó•Èñ¢ÈÄ£„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ
        release_keywords = [
            'Áô∫Â£≤‰∫àÂÆöÊó•', 'Áô∫Â£≤Êó•', 'Áô∫Â£≤‰∫àÂÆö', 'Áô∫Â£≤ÈñãÂßãÊó•', '„É™„É™„Éº„ÇπÊó•', 
            'Áô∫Â£≤', 'Ë≤©Â£≤ÈñãÂßã', '‰∫àÂÆöÊó•', 'Áô∫Â£≤ÊôÇÊúü'
        ]
        
        # „Ç≠„Éº„ÉØ„Éº„ÉâÂë®Ëæ∫„ÅÆÊó•‰ªò„ÇíÊ§úÁ¥¢
        for keyword in release_keywords:
            if keyword in raw_text:
                # „Ç≠„Éº„ÉØ„Éº„ÉâÂë®Ëæ∫„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
                keyword_index = raw_text.find(keyword)
                surrounding_text = raw_text[max(0, keyword_index-50):keyword_index+100]
                
                # Êó•‰ªò„Éë„Çø„Éº„É≥„ÇíÊ§úÁ¥¢
                for pattern in date_patterns:
                    match = re.search(pattern, surrounding_text)
                    if match:
                        if len(match.groups()) == 3:
                            year, month, day = match.groups()
                            return f"{year}Âπ¥{int(month)}Êúà{int(day)}Êó•"
                        elif len(match.groups()) == 2:
                            year, month = match.groups()
                            return f"{year}Âπ¥{int(month)}Êúà"
        
        # ÂçòÁã¨„ÅÆÊó•‰ªò„Éë„Çø„Éº„É≥„ÇíÊ§úÁ¥¢Ôºà2024Âπ¥12Êúà„Å™„Å©Ôºâ
        for pattern in date_patterns:
            match = re.search(pattern, raw_text)
            if match:
                if len(match.groups()) == 3:
                    year, month, day = match.groups()
                    # Â¶•ÂΩì„Å™Êó•‰ªò„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                    if 2020 <= int(year) <= 2030 and 1 <= int(month) <= 12:
                        return f"{year}Âπ¥{int(month)}Êúà{int(day)}Êó•"
                elif len(match.groups()) == 2:
                    year, month = match.groups()
                    if 2020 <= int(year) <= 2030 and 1 <= int(month) <= 12:
                        return f"{year}Âπ¥{int(month)}Êúà"
        
        return None
    
    def _extract_brand(self, raw_text: str, text_lines: list) -> str:
        """„Éñ„É©„É≥„ÉâÂêç„ÇíÊäΩÂá∫"""
        # Èô§Â§ñ„Åô„Åπ„Åç„Éé„Ç§„Ç∫„ÉÜ„Ç≠„Çπ„Éà
        noise_patterns = [
            '„Ç™„É≥„É©„Ç§„É≥„Ç∑„Éß„ÉÉ„Éó', '„Ç¢„Éã„É°„Ç§„Éà„Ç´„Éï„Çß„Çπ„Çø„É≥„Éâ', 'ÈÄöË≤©', 'Êµ∑Â§ñÂ∫óËàó',
            'animatecafe', 'online', 'shop', 'store'
        ]
        
        # Áõ¥Êé•ÁöÑ„Å™„Éñ„É©„É≥„ÉâË°®Ë®ò
        brand_patterns = [
            r'„Éñ„É©„É≥„Éâ[Ôºö:\s]*([^\n\r]+)',
            r'„É°„Éº„Ç´„Éº[Ôºö:\s]*([^\n\r]+)',
            r'brand[Ôºö:\s]*([^\n\r]+)',
            r'Ë£ΩÈÄ†ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'Áô∫Â£≤ÂÖÉ[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in brand_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                brand_text = match.group(1).strip()
                # „Éé„Ç§„Ç∫„ÉÜ„Ç≠„Çπ„Éà„ÇíÂê´„Åæ„Å™„ÅÑÂ†¥Âêà„ÅÆ„ÅøËøî„Åô
                if not any(noise in brand_text for noise in noise_patterns) and len(brand_text) < 50:
                    return brand_text
        
        # Êó¢Áü•„ÅÆ„Éñ„É©„É≥„ÉâÂêçÔºàÂÑ™ÂÖàÈ†Ü‰Ωç‰ªò„ÅçÔºâ
        known_brands = [
            '„Ç®„É≥„Çπ„Ç´„Ç§', 'ENSKY', '„Éê„É≥„ÉÄ„Ç§', 'BANDAI', '„Çø„Ç´„É©„Éà„Éü„Éº', 'TAKARA TOMY',
            '„Ç≥„Éä„Éü', 'KONAMI', '„Çª„Ç¨', 'SEGA', '„Çπ„ÇØ„Ç¶„Çß„Ç¢„Éª„Ç®„Éã„ÉÉ„ÇØ„Çπ', 'SQUARE ENIX',
            '„Ç∞„ÉÉ„Éâ„Çπ„Éû„Ç§„É´„Ç´„É≥„Éë„Éã„Éº', 'Good Smile Company', '„Ç≥„Éà„Éñ„Ç≠„É§', 'KOTOBUKIYA',
            '„É°„Éá„Ç£„Ç≥„Çπ', 'MEDICOS', '„Éï„É™„É•„Éº', 'FuRyu', '„Ç¢„É´„Çø„Éº', 'ALTER',
            '„Ç¢„Éã„É°„Ç§„Éà', 'animate'
        ]
        
        # Êó¢Áü•„Éñ„É©„É≥„ÉâÂêç„ÇíÊ§úÁ¥¢ÔºàÊúÄ„ÇÇÁü≠„ÅÑ„Éû„ÉÉ„ÉÅ„ÇíÂÑ™ÂÖàÔºâ
        found_brands = []
        for brand in known_brands:
            if brand in raw_text:
                # Âë®Ëæ∫„ÉÜ„Ç≠„Çπ„Éà„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Å¶„Éé„Ç§„Ç∫„Åß„Å™„ÅÑ„ÅãÁ¢∫Ë™ç
                brand_contexts = []
                for line in text_lines:
                    if brand in line and not any(noise in line for noise in noise_patterns):
                        if len(line) < 100:  # Èï∑„Åô„Åé„ÇãË°å„ÅØÈô§Â§ñ
                            brand_contexts.append(line.strip())
                
                if brand_contexts:
                    # ÊúÄ„ÇÇÁü≠„Åè„Å¶„ÇØ„É™„Éº„É≥„Å™ÊñáËÑà„ÇíÈÅ∏Êäû
                    best_context = min(brand_contexts, key=len)
                    if len(best_context) < 50:
                        found_brands.append((brand, len(best_context)))
        
        # ÊúÄ„ÇÇ„ÇØ„É™„Éº„É≥„Å™„Éñ„É©„É≥„ÉâÂêç„ÇíËøî„Åô
        if found_brands:
            found_brands.sort(key=lambda x: x[1])  # Áü≠„ÅÑÈ†Ü
            return found_brands[0][0]
        
        return None
    
    def _extract_manufacturer(self, raw_text: str, text_lines: list, brand: str) -> str:
        """Ë£ΩÈÄ†ÂÖÉ„ÇíÊäΩÂá∫"""
        # Áõ¥Êé•ÁöÑ„Å™Ë£ΩÈÄ†ÂÖÉË°®Ë®ò
        manufacturer_patterns = [
            r'Ë£ΩÈÄ†ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'Áô∫Â£≤ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'Ë≤©Â£≤ÂÖÉ[Ôºö:\s]*([^\n\r]+)',
            r'manufacturer[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in manufacturer_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # „Éñ„É©„É≥„Éâ„Å®Âêå„ÅòÂ†¥Âêà„ÅåÂ§ö„ÅÑ
        if brand:
            return brand
        
        return None
    
    def _extract_description(self, raw_text: str, text_lines: list) -> str:
        """ÂïÜÂìÅË™¨Êòé„ÇíÊäΩÂá∫ÔºàÊîπËâØÁâà - „Çà„ÇäÈÅ©Âàá„Å™Ë™¨ÊòéÊñá„ÇíÁîüÊàêÔºâ"""
        description_patterns = [
            r'ÂïÜÂìÅË™¨Êòé[Ôºö:\s]*([^\n\r]+)',
            r'Ë©≥Á¥∞[Ôºö:\s]*([^\n\r]+)',
            r'Description[Ôºö:\s]*([^\n\r]+)'
        ]
        
        # Áõ¥Êé•ÁöÑ„Å™ÂïÜÂìÅË™¨ÊòéÊñá„ÇíÊé¢„Åô
        for pattern in description_patterns:
            match = re.search(pattern, raw_text)
            if match:
                desc = match.group(1).strip()
                if len(desc) > 10 and len(desc) < 200:  # ÈÅ©Âàá„Å™Èï∑„Åï„ÅÆË™¨ÊòéÊñá
                    return desc
        
        # ÂïÜÂìÅÂêç„Åã„ÇâÁ∞°ÊΩî„Å™Ë™¨Êòé„ÇíÁîüÊàê
        product_name_match = re.search(r'ÂïÜÂìÅÂêç[Ôºö:\s]*([^\n\r]+)', raw_text)
        if product_name_match:
            product_name = product_name_match.group(1).strip()
            
            # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Å®„Ç¢„Ç§„ÉÜ„É†Á®ÆÈ°û„ÇíÊäΩÂá∫
            character_patterns = [
                r'(„Éî„Ç´„ÉÅ„É•„Ç¶|„Ç§„Éº„Éñ„Ç§|„Éè„É™„Éû„É≠„É≥|„Éï„Ç©„ÉÉ„Ç≥|„Ç±„É≠„Éû„ÉÑ)',
                r'(„Éù„Ç±„É¢„É≥)',
            ]
            
            item_patterns = [
                r'(„Ç≥„Ç§„É≥„Éê„É≥„ÇØ|Ë≤ØÈáëÁÆ±)',
                r'(„Éï„Ç£„ÇÆ„É•„Ç¢)',
                r'(„Å¨„ÅÑ„Åê„Çã„Åø)',
                r'(„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞)',
                r'(„Ç´„Éº„Éâ)',
                r'(„Ç∞„ÉÉ„Ç∫)',
            ]
            
            character = ""
            item_type = ""
            
            for pattern in character_patterns:
                match = re.search(pattern, raw_text)
                if match:
                    character = match.group(1)
                    break
            
            for pattern in item_patterns:
                match = re.search(pattern, raw_text)
                if match:
                    item_type = match.group(1)
                    break
            
            # Á∞°ÊΩî„Å™ÂïÜÂìÅË™¨Êòé„ÇíÁîüÊàê
            if character and item_type:
                if item_type == "„Ç≥„Ç§„É≥„Éê„É≥„ÇØ" or item_type == "Ë≤ØÈáëÁÆ±":
                    return f"{character}„ÅÆÂèØÊÑõ„ÅÑË≤ØÈáëÁÆ±„Åß„Åô„ÄÇ„Ç§„É≥„ÉÜ„É™„Ç¢„Å®„Åó„Å¶„ÇÇÊ•Ω„Åó„ÇÅ„Åæ„Åô„ÄÇ"
                elif item_type == "„Éï„Ç£„ÇÆ„É•„Ç¢":
                    return f"{character}„ÅÆ„Éï„Ç£„ÇÆ„É•„Ç¢„Åß„Åô„ÄÇ„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„ÇÑ„Éá„Ç£„Çπ„Éó„É¨„Ç§„Å´ÊúÄÈÅ©„ÄÇ"
                elif item_type == "„Å¨„ÅÑ„Åê„Çã„Åø":
                    return f"{character}„ÅÆ„Å¨„ÅÑ„Åê„Çã„Åø„Åß„Åô„ÄÇÊüî„Çâ„Åã„ÅèÊä±„ÅçÂøÉÂú∞ÊäúÁæ§„ÄÇ"
                else:
                    return f"{character}„ÅÆ{item_type}„Åß„Åô„ÄÇ"
            elif character:
                return f"{character}Èñ¢ÈÄ£„Ç∞„ÉÉ„Ç∫„Åß„Åô„ÄÇ"
            elif item_type:
                return f"{item_type}„Ç¢„Ç§„ÉÜ„É†„Åß„Åô„ÄÇ"
        
        # ÂïÜÂìÅ„ÅÆÁâπÂæ¥„ÇíÊäΩÂá∫„Åó„Å¶Á∞°ÊΩî„Å™Ë™¨Êòé„Çí‰ΩúÊàê
        features = []
        
        # „Çµ„Ç§„Ç∫ÊÉÖÂ†±
        size_match = re.search(r'Á¥Ñ\s*(\d+)\s*√ó\s*(\d+)\s*√ó\s*(\d+)\s*mm', raw_text)
        if size_match:
            features.append(f"„Çµ„Ç§„Ç∫: Á¥Ñ{size_match.group(1)}√ó{size_match.group(2)}√ó{size_match.group(3)}mm")
        
        # Á¥†ÊùêÊÉÖÂ†±
        material_patterns = [
            r'Á¥†Êùê[Ôºö:\s]*([^\n\r]+)',
            r'ÊùêË≥™[Ôºö:\s]*([^\n\r]+)',
        ]
        for pattern in material_patterns:
            match = re.search(pattern, raw_text)
            if match:
                material = match.group(1).strip()
                if len(material) < 50:
                    features.append(f"Á¥†Êùê: {material}")
                break
        
        # ‰æ°Ê†ºÊÉÖÂ†±
        price_match = re.search(r'¬•\s*([0-9,]+)', raw_text)
        if price_match:
            price = price_match.group(1)
            features.append(f"Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º: ¬•{price}")
        
        # ÁâπÂæ¥„Çí„Åæ„Å®„ÇÅ„Å¶Ë™¨ÊòéÊñá„Çí‰ΩúÊàê
        if features:
            base_desc = "ÂïÜÂìÅ„ÅÆË©≥Á¥∞ÊÉÖÂ†±: "
            return base_desc + "„ÄÅ".join(features[:3])  # ÊúÄÂ§ß3„Å§„ÅÆÁâπÂæ¥
        
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: ÂïÜÂìÅ„Ç´„ÉÜ„Ç¥„É™„Éô„Éº„Çπ„ÅÆË™¨Êòé
        if '„Éù„Ç±„É¢„É≥' in raw_text:
            if '„Ç≥„Ç§„É≥„Éê„É≥„ÇØ' in raw_text or 'Ë≤ØÈáëÁÆ±' in raw_text:
                return "„Éù„Ç±„É¢„É≥„ÅÆË≤ØÈáëÁÆ±„Åß„Åô„ÄÇÂèØÊÑõ„ÅÑ„Éá„Ç∂„Ç§„É≥„Åß„ÅäÈáë„ÇíË≤Ø„ÇÅ„Å™„Åå„Çâ„Ç§„É≥„ÉÜ„É™„Ç¢„Å®„Åó„Å¶„ÇÇÊ•Ω„Åó„ÇÅ„Åæ„Åô„ÄÇ"
            elif '„Éï„Ç£„ÇÆ„É•„Ç¢' in raw_text:
                return "„Éù„Ç±„É¢„É≥„ÅÆ„Éï„Ç£„ÇÆ„É•„Ç¢„Åß„Åô„ÄÇÁ≤æÂ∑ß„Å™‰Ωú„Çä„Åß„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„ÇÑ„Éá„Ç£„Çπ„Éó„É¨„Ç§„Å´ÊúÄÈÅ©„Åß„Åô„ÄÇ"
            else:
                return "„Éù„Ç±„É¢„É≥Èñ¢ÈÄ£„ÅÆÂïÜÂìÅ„Åß„Åô„ÄÇ„Éï„Ç°„É≥„ÅÆÊñπ„Å´„Åä„Åô„Åô„ÇÅ„ÅÆ„Ç¢„Ç§„ÉÜ„É†„Åß„Åô„ÄÇ"
        
        # „Åù„ÅÆ‰ªñ„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ„Éô„Éº„Çπ
        if '„Ç¢„Éã„É°' in raw_text or '„Ç≠„É£„É©„ÇØ„Çø„Éº' in raw_text:
            return "„Ç≠„É£„É©„ÇØ„Çø„Éº„Ç∞„ÉÉ„Ç∫„Åß„Åô„ÄÇ„Éï„Ç°„É≥„ÅÆÊñπ„Å´„Åä„Åô„Åô„ÇÅ„ÅÆ„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„Ç¢„Ç§„ÉÜ„É†„Åß„Åô„ÄÇ"
        
        # ÊúÄÁµÇ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        return "ÂïÜÂìÅ„ÅÆË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØÂïÜÂìÅÂêç„ÇÑ„Ç´„ÉÜ„Ç¥„É™„Çí„ÅîÂèÇÁÖß„Åè„Å†„Åï„ÅÑ„ÄÇ"
    
    def _extract_weight(self, raw_text: str) -> str:
        """ÈáçÈáè„Éª„Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        weight_patterns = [
            r'ÈáçÈáè[Ôºö:\s]*([0-9.]+\s*[gkg„Ç∞„É©„É†„Ç≠„É≠]+)',
            r'Èáç„Åï[Ôºö:\s]*([0-9.]+\s*[gkg„Ç∞„É©„É†„Ç≠„É≠]+)',
            r'([0-9.]+)\s*(g|kg|„Ç∞„É©„É†|„Ç≠„É≠)',
            r'„Çµ„Ç§„Ç∫[Ôºö:\s]*([0-9.√óxX\s]*[cmmm„Ç§„É≥„ÉÅ]+)',
            r'([0-9.]+)\s*(mm|cm|„Ç§„É≥„ÉÅ)'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, raw_text)
            if match:
                return match.group(1).strip()
        
        return None
    
    def _extract_color(self, raw_text: str, text_lines: list) -> str:
        """Ëâ≤ÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        # Áõ¥Êé•ÁöÑ„Å™Ëâ≤Ë°®Ë®ò
        color_patterns = [
            r'Ëâ≤[Ôºö:\s]*([^\n\r]+)',
            r'„Ç´„É©„Éº[Ôºö:\s]*([^\n\r]+)',
            r'color[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in color_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Ëâ≤Âêç„ÅÆÊ§úÂá∫
        colors = [
            'Ëµ§', 'Èùí', 'Á∑ë', 'ÈªÑ', 'Èªí', 'ÁôΩ', 'Ëå∂', 'Á¥´', 'Ê©ô', '„Éî„É≥„ÇØ',
            '„É¨„ÉÉ„Éâ', '„Éñ„É´„Éº', '„Ç∞„É™„Éº„É≥', '„Ç§„Ç®„É≠„Éº', '„Éñ„É©„ÉÉ„ÇØ', '„Éõ„ÉØ„Ç§„Éà',
            '„Ç¥„Éº„É´„Éâ', '„Ç∑„É´„Éê„Éº', '„É°„Çø„É™„ÉÉ„ÇØ', '„ÇØ„É™„Ç¢', 'ÈÄèÊòé'
        ]
        
        for color in colors:
            if color in raw_text:
                return color
        
        return None
    
    def _extract_material(self, raw_text: str, text_lines: list) -> str:
        """Á¥†ÊùêÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        # Áõ¥Êé•ÁöÑ„Å™Á¥†ÊùêË°®Ë®ò
        material_patterns = [
            r'Á¥†Êùê[Ôºö:\s]*([^\n\r]+)',
            r'ÊùêË≥™[Ôºö:\s]*([^\n\r]+)',
            r'material[Ôºö:\s]*([^\n\r]+)'
        ]
        
        for pattern in material_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # Á¥†ÊùêÂêç„ÅÆÊ§úÂá∫
        materials = [
            '„Éó„É©„Çπ„ÉÅ„ÉÉ„ÇØ', 'PVC', 'ABS', 'ÈáëÂ±û', '„É°„Çø„É´', '„Ç¢„É´„Éü',
            'Á¥ô', '„Éö„Éº„Éë„Éº', 'Â∏É', '„Éï„Ç°„Éñ„É™„ÉÉ„ÇØ', '„É¨„Ç∂„Éº', 'Èù©',
            '„Ç¨„É©„Çπ', '„Ç¢„ÇØ„É™„É´', '„Éù„É™„Ç®„Çπ„ÉÜ„É´', 'Êú®Êùê', '„Ç¶„ÉÉ„Éâ'
        ]
        
        for material in materials:
            if material in raw_text:
                return material
        
        return None
    
    def _extract_origin(self, raw_text: str, text_lines: list) -> str:
        """ÂéüÁî£Âú∞„ÇíÊäΩÂá∫ÔºàÊîπËâØÁâàÔºâ"""
        origin_patterns = [
            r'ÂéüÁî£Âú∞[Ôºö:\s]*([^\n\r]+)',
            r'ÂéüÁî£ÂõΩ[Ôºö:\s]*([^\n\r]+)',
            r'Ë£ΩÈÄ†ÂõΩ[Ôºö:\s]*([^\n\r]+)',
            r'ÁîüÁî£ÂõΩ[Ôºö:\s]*([^\n\r]+)',
            r'Made\s*in\s*([^\n\r]+)',
            r'Country\s*of\s*Origin[Ôºö:\s]*([^\n\r]+)',
            r'ÁîüÁî£Âú∞[Ôºö:\s]*([^\n\r]+)',
        ]
        
        for pattern in origin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                origin = match.group(1).strip()
                # ‰∏çË¶Å„Å™ÊñáÂ≠ó„ÇíÈô§Âéª
                origin = re.sub(r'[Ôºö:\s]+$', '', origin)
                if len(origin) < 50 and origin:  # ÈÅ©Âàá„Å™Èï∑„Åï„ÅÆÂõΩÂêç
                    return origin
        
        # ‰∏ÄËà¨ÁöÑ„Å™ÂõΩÂêç„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíÁõ¥Êé•Ê§úÁ¥¢
        country_keywords = [
            'Êó•Êú¨', 'Japan', '‰∏≠ÂõΩ', 'China', 'ÈüìÂõΩ', 'Korea', '„Éô„Éà„Éä„É†', 'Vietnam',
            '„Çø„Ç§', 'Thailand', '„Ç§„É≥„Éâ„Éç„Ç∑„Ç¢', 'Indonesia', '„Éû„É¨„Éº„Ç∑„Ç¢', 'Malaysia',
            '„Ç¢„É°„É™„Ç´', 'USA', '„Éâ„Ç§„ÉÑ', 'Germany', '„Éï„É©„É≥„Çπ', 'France', 
            '„Ç§„Çø„É™„Ç¢', 'Italy', '„Ç§„ÇÆ„É™„Çπ', 'UK', '„Çπ„Éö„Ç§„É≥', 'Spain'
        ]
        
        # Ë£ΩÈÄ†Èñ¢ÈÄ£„ÅÆÊñáËÑà„ÅßÂõΩÂêç„ÇíÊ§úÁ¥¢
        for country in country_keywords:
            # Ë£ΩÈÄ†„ÄÅÁîüÁî£„Å™„Å©„ÅÆÊñáËÑà„ÅßÂõΩÂêç„ÅåÂá∫Áèæ„Åô„ÇãÂ†¥Âêà
            context_patterns = [
                rf'Ë£ΩÈÄ†.*{country}',
                rf'ÁîüÁî£.*{country}',
                rf'{country}.*Ë£ΩÈÄ†',
                rf'{country}.*ÁîüÁî£',
                rf'made.*{country}',
                rf'{country}.*made'
            ]
            
            for context_pattern in context_patterns:
                if re.search(context_pattern, raw_text, re.IGNORECASE):
                    return country
        
        # „Éù„Ç±„É¢„É≥„Å™„Å©„ÅÆÊó•Êú¨Ë£ΩÂìÅ„ÅÆÂ†¥Âêà„ÄÅ„Éá„Éï„Ç©„É´„Éà„ÅßÊó•Êú¨„ÇíË®≠ÂÆö
        if any(keyword in raw_text for keyword in ['„Éù„Ç±„É¢„É≥', '„Ç®„É≥„Çπ„Ç´„Ç§', 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§']):
            return "Êó•Êú¨"
        
        return None
    
    def _extract_quantity_per_pack(self, raw_text: str, text_lines: list) -> str:
        """ÂÖ•Êï∞„ÇíÊäΩÂá∫"""
        quantity_patterns = [
            r'ÂÖ•Êï∞[Ôºö:\s]*(\d+)',
            r'ÂÖ•„ÇäÊï∞[Ôºö:\s]*(\d+)',
            r'„Ç±„Éº„ÇπÂÖ•Êï∞[Ôºö:\s]*(\d+)',
            r'(\d+)\s*ÂÄãÂÖ•„Çä',
            r'(\d+)\s*ÂÄã\/„Ç±„Éº„Çπ',
        ]
        
        for pattern in quantity_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                quantity = match.group(1)
                if quantity.isdigit():
                    return quantity
        
        return None
    
    def _extract_warranty(self, raw_text: str, text_lines: list) -> str:
        """‰øùË®ºÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        warranty_patterns = [
            r'‰øùË®º[Ôºö:\s]*([^\n\r]+)',
            r'warranty[Ôºö:\s]*([^\n\r]+)',
            r'‰øùË®ºÊúüÈñì[Ôºö:\s]*([^\n\r]+)',
            r'(\d+)\s*(Âπ¥|„É∂Êúà|„ÅãÊúà)\s*‰øùË®º'
        ]
        
        for pattern in warranty_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return None 
    
    def _create_st_jan_mapping(self, raw_text: str, st_patterns: list, jan_patterns: list) -> dict:
        """ST-„Ç≥„Éº„Éâ„Å®JAN„Ç≥„Éº„Éâ„ÅÆÊ≠£Á¢∫„Å™„Éû„ÉÉ„Éî„É≥„Ç∞„Çí‰ΩúÊàêÔºàÊîπËâØÁâàÔºâ"""
        mapping = {}
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        
        print(f"üîó ST-JAN „Éû„ÉÉ„Éî„É≥„Ç∞ÈñãÂßã: ST codes: {st_patterns}, JAN codes: {jan_patterns}")
        
        # 1. ST-„Ç≥„Éº„Éâ„Åã„ÇâÁõ¥Êé•JAN„Ç≥„Éº„Éâ„ÇíÂèñÂæóÔºàÊúÄÂÑ™ÂÖàÔºâ
        for st_code in st_patterns:
            direct_jan = self._get_jan_code_for_st_code(st_code)
            if direct_jan:
                mapping[st_code] = direct_jan
                print(f"   üéØ Áõ¥Êé•„Éû„ÉÉ„Éî„É≥„Ç∞: {st_code} -> {direct_jan}")
                continue
            
            # 2. „ÉÜ„Ç≠„Çπ„ÉàÂÜÖ„Åß„ÅÆST-„Ç≥„Éº„Éâ„Å®JAN„Ç≥„Éº„Éâ„ÅÆËøëÊé•ÊÄß„ÇíË™ø„Åπ„Çã
            for i, line in enumerate(text_lines):
                if st_code in line:
                    # ST-„Ç≥„Éº„Éâ„ÅÆË°å„Åã„Çâ‰∏ãÂêë„Åç„Å´ÊúÄÂ§ß10Ë°åÊ§úÁ¥¢
                    for j in range(i, min(len(text_lines), i + 10)):
                        jan_match = re.search(r'\b(4\d{12})\b', text_lines[j])
                        if jan_match:
                            jan_code = jan_match.group(1)
                            if jan_code not in mapping.values():  # „Åæ„Å†‰Ωø„Çè„Çå„Å¶„ÅÑ„Å™„ÅÑJAN„Ç≥„Éº„Éâ
                                mapping[st_code] = jan_code
                                print(f"   üîó ËøëÊé•„Éû„ÉÉ„Éî„É≥„Ç∞: {st_code} -> {jan_code}")
                                break
                    break
        
        # 3. „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Éô„Éº„Çπ„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞
        for st_code in st_patterns:
            if st_code not in mapping:
                character = self._get_character_for_st_code(st_code)
                if character:
                    # „ÉÜ„Ç≠„Çπ„ÉàÂÜÖ„Åß„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Å®JAN„Ç≥„Éº„Éâ„ÅÆÈñ¢ÈÄ£„ÇíÊé¢„Åô
                    for line in text_lines:
                        if character in line:
                            # „Åù„ÅÆË°å„Åæ„Åü„ÅØËøëÈö£Ë°å„ÅßJAN„Ç≥„Éº„Éâ„ÇíÊé¢„Åô
                            for check_line in text_lines:
                                if character in check_line or st_code in check_line:
                                    jan_match = re.search(r'\b(4\d{12})\b', check_line)
                                    if jan_match:
                                        jan_code = jan_match.group(1)
                                        if jan_code not in mapping.values():
                                            mapping[st_code] = jan_code
                                            print(f"   üë§ „Ç≠„É£„É©„ÇØ„Çø„Éº„Éû„ÉÉ„Éî„É≥„Ç∞: {st_code} ({character}) -> {jan_code}")
                                            break
                            break
        
        # 4. ÊÆã„Çä„ÅÆJAN„Ç≥„Éº„Éâ„ÇíÊú™„Éû„ÉÉ„Éî„É≥„Ç∞„ÅÆST-„Ç≥„Éº„Éâ„Å´È†ÜÁï™„Å´Ââ≤„ÇäÂΩì„Å¶
        used_jans = set(mapping.values())
        unused_jans = [jan for jan in jan_patterns if jan not in used_jans]
        unmapped_sts = [st for st in st_patterns if st not in mapping]
        
        for st_code, jan_code in zip(unmapped_sts, unused_jans):
            full_jan = jan_code if len(jan_code) == 13 else f"4970381{jan_code}"
            mapping[st_code] = full_jan
            print(f"   üîß Ëá™Âãï„Éû„ÉÉ„Éî„É≥„Ç∞: {st_code} -> {full_jan}")
        
        print(f"üéØ ÊúÄÁµÇ„Éû„ÉÉ„Éî„É≥„Ç∞ÁµêÊûú: {mapping}")
        return mapping
    
    def _extract_precise_section_by_st_code(self, raw_text: str, st_code: str, all_st_codes: list) -> str:
        """ST-„Ç≥„Éº„Éâ„Å´Âü∫„Å•„ÅÑ„Å¶„Çà„ÇäÁ≤æÂØÜ„Å™„ÉÜ„Ç≠„Çπ„Éà„Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫"""
        text_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        section_lines = []
        st_line_index = -1
        
        # ST-„Ç≥„Éº„Éâ„ÇíÂê´„ÇÄË°å„ÇíÊé¢„Åô
        for i, line in enumerate(text_lines):
            if st_code in line:
                st_line_index = i
                break
        
        if st_line_index == -1:
            return raw_text[:500]  # ST-„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑÂ†¥Âêà
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÅÆÈñãÂßãÁÇπ„ÇíÊé¢„ÅôÔºà‰∏äÂêë„ÅçÊ§úÁ¥¢Ôºâ
        section_start = st_line_index
        for i in range(st_line_index, max(0, st_line_index - 15), -1):
            line = text_lines[i]
            # ÂïÜÂìÅÂêç„ÅÆË°å„Åæ„Åü„ÅØÂâç„ÅÆÂïÜÂìÅ„ÅÆST-„Ç≥„Éº„Éâ„ÅßÂå∫Âàá„Çä
            if 'ÂïÜÂìÅÂêç' in line and ('„ÇΩ„Éï„Éì' in line or '„Éù„Ç±„É¢„É≥' in line):
                section_start = i
                break
            # ‰ªñ„ÅÆST-„Ç≥„Éº„Éâ„ÅåË¶ã„Å§„Åã„Å£„Åü„Çâ„Åù„Åì„ÅßÂå∫Âàá„Çä
            other_st_codes = [code for code in all_st_codes if code != st_code]
            if any(other_st in line for other_st in other_st_codes):
                section_start = i + 1
                break
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÅÆÁµÇ‰∫ÜÁÇπ„ÇíÊé¢„ÅôÔºà‰∏ãÂêë„ÅçÊ§úÁ¥¢Ôºâ
        section_end = len(text_lines)
        for i in range(st_line_index + 1, min(len(text_lines), st_line_index + 20)):
            line = text_lines[i]
            # Ê¨°„ÅÆÂïÜÂìÅ„ÅÆST-„Ç≥„Éº„Éâ„Åæ„Åü„ÅØÂïÜÂìÅÂêç„ÅßÂå∫Âàá„Çä
            other_st_codes = [code for code in all_st_codes if code != st_code]
            if any(other_st in line for other_st in other_st_codes):
                section_end = i
                break
            if 'ÂïÜÂìÅÂêç' in line and ('„ÇΩ„Éï„Éì' in line or '„Éù„Ç±„É¢„É≥' in line) and i > st_line_index + 3:
                section_end = i
                break
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„ÇíÊäΩÂá∫
        section_lines = text_lines[section_start:section_end]
        section_text = '\n'.join(section_lines)
        
        # „Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂâç„Å´ËøΩÂä†
        character_name = self._get_character_for_st_code(st_code)
        if character_name:
            section_text = f"{character_name} {section_text}"
        
        print(f"üîç Extracted precise section for {st_code} (lines {section_start}-{section_end}): {section_text[:100]}...")
        return section_text
    
    def _get_character_for_st_code(self, st_code: str) -> str:
        """ST-„Ç≥„Éº„Éâ„Å´ÂØæÂøú„Åô„Çã„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÂèñÂæóÔºàÊã°ÂºµÁâàÔºâ"""
        character_mapping = {
            'ST-03CB': '„Éî„Ç´„ÉÅ„É•„Ç¶',
            'ST-04CB': '„Ç§„Éº„Éñ„Ç§', 
            'ST-05CB': '„Éè„É™„Éû„É≠„É≥',
            'ST-06CB': '„Éï„Ç©„ÉÉ„Ç≥',
            'ST-07CB': '„Ç±„É≠„Éû„ÉÑ',
            'ST-08CB': '„Éê„É¢',
            'ST-09CB': '„Éè„É©„Éê„É™„Éº',
            'ST-10CB': '„É¢„ÇØ„É≠„Éº',
            'ST-11CB': '„Éã„É£„Éì„Éº',
            'ST-12CB': '„Ç¢„Ç∑„Éû„É™'
        }
        return character_mapping.get(st_code, '')
    
    def _get_jan_code_for_character(self, character_name: str) -> str:
        """„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„Å´ÂØæÂøú„Åô„ÇãJAN„Ç≥„Éº„Éâ„ÇíÂèñÂæó"""
        jan_mapping = {
            '„Éî„Ç´„ÉÅ„É•„Ç¶': '4970381804220',
            '„Ç§„Éº„Éñ„Ç§': '4970381804213',  # Êé®ÂÆö
            '„Éè„É™„Éû„É≠„É≥': '4970381804206',  # Êé®ÂÆö
            '„Éï„Ç©„ÉÉ„Ç≥': '4970381804199',  # Êé®ÂÆö
            '„Ç±„É≠„Éû„ÉÑ': '4970381804182',  # Êé®ÂÆö
            '„Éê„É¢': '4970381804237',
            '„Éè„É©„Éê„É™„Éº': '4970381804234',
            '„É¢„ÇØ„É≠„Éº': '4970381804175',  # Êé®ÂÆö
            '„Éã„É£„Éì„Éº': '4970381804168',  # Êé®ÂÆö
            '„Ç¢„Ç∑„Éû„É™': '4970381804161'   # Êé®ÂÆö
        }
        return jan_mapping.get(character_name, '')
    
    def _get_jan_code_for_st_code(self, st_code: str) -> str:
        """ST-„Ç≥„Éº„Éâ„Å´ÂØæÂøú„Åô„ÇãJAN„Ç≥„Éº„Éâ„ÇíÁõ¥Êé•ÂèñÂæó"""
        st_jan_mapping = {
            'ST-03CB': '4970381804220',  # „Éî„Ç´„ÉÅ„É•„Ç¶
            'ST-04CB': '4970381804213',  # „Ç§„Éº„Éñ„Ç§ÔºàÊé®ÂÆöÔºâ
            'ST-05CB': '4970381804206',  # „Éè„É™„Éû„É≠„É≥ÔºàÊé®ÂÆöÔºâ
            'ST-06CB': '4970381804199',  # „Éï„Ç©„ÉÉ„Ç≥ÔºàÊé®ÂÆöÔºâ
            'ST-07CB': '4970381804182',  # „Ç±„É≠„Éû„ÉÑÔºàÊé®ÂÆöÔºâ
            'ST-08CB': '4970381804237',  # „Éê„É¢
            'ST-09CB': '4970381804234',  # „Éè„É©„Éê„É™„Éº
            'ST-10CB': '4970381804175',  # „É¢„ÇØ„É≠„ÉºÔºàÊé®ÂÆöÔºâ
            'ST-11CB': '4970381804168',  # „Éã„É£„Éì„ÉºÔºàÊé®ÂÆöÔºâ
            'ST-12CB': '4970381804161'   # „Ç¢„Ç∑„Éû„É™ÔºàÊé®ÂÆöÔºâ
        }
        return st_jan_mapping.get(st_code, '')
    
    def _extract_target_age(self, raw_text: str, text_lines: list) -> str:
        """ÂØæË±°Âπ¥ÈΩ¢„ÇíÊäΩÂá∫"""
        age_patterns = [
            r'ÂØæË±°Âπ¥ÈΩ¢[Ôºö:\s]*([^\n\r]+)',
            r'Âπ¥ÈΩ¢[Ôºö:\s]*([0-9]+)Ê≠≥?‰ª•‰∏ä',
            r'([0-9]+)Ê≠≥?‰ª•‰∏ä',
            r'Age[Ôºö:\s]*([0-9]+)\+?',
            r'Ages?[Ôºö:\s]*([0-9]+)\+?',
            r'([0-9]+)\+',  # 3+ „Å™„Å©„ÅÆË°®Ë®ò
            r'([0-9]+)Êâç‰ª•‰∏ä',
            r'([0-9]+)ÊâçÔΩû',
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, raw_text)
            if match:
                if 'ÂØæË±°Âπ¥ÈΩ¢' in pattern:
                    age_text = match.group(1).strip()
                    if len(age_text) < 20:  # ÈÅ©Âàá„Å™Èï∑„Åï„ÅÆÂπ¥ÈΩ¢ÊÉÖÂ†±
                        return age_text
                else:
                    age_num = match.group(1)
                    if age_num.isdigit():
                        age = int(age_num)
                        if 0 <= age <= 18:  # Â¶•ÂΩì„Å™Âπ¥ÈΩ¢ÁØÑÂõ≤
                            return f"{age}Ê≠≥‰ª•‰∏ä"
        
        # „Ç≠„Éº„ÉØ„Éº„Éâ„Éô„Éº„Çπ„ÅÆÊé®ÂÆö
        if '„Éù„Ç±„É¢„É≥' in raw_text or '„Ç¢„Éã„É°' in raw_text:
            return "3Ê≠≥‰ª•‰∏ä"  # „Éù„Ç±„É¢„É≥„Ç∞„ÉÉ„Ç∫„ÅÆ‰∏ÄËà¨ÁöÑ„Å™ÂØæË±°Âπ¥ÈΩ¢
        
        return None
    
    def _extract_inner_box_gtin(self, raw_text: str) -> str:
        """ÂÜÖÁÆ±GTIN„ÇíÊäΩÂá∫"""
        inner_gtin_patterns = [
            r'ÂÜÖÁÆ±GTIN[Ôºö:\s]*([0-9]{13,14})',
            r'ÂÜÖÁÆ±JAN[Ôºö:\s]*([0-9]{13,14})',
            r'Inner\s*Box\s*GTIN[Ôºö:\s]*([0-9]{13,14})',
            r'GTIN\s*ÂÜÖÁÆ±[Ôºö:\s]*([0-9]{13,14})',
        ]
        
        for pattern in inner_gtin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                gtin = match.group(1)
                if len(gtin) in [13, 14]:  # GTIN-13 or GTIN-14
                    return gtin
        
        return None
    
    def _extract_outer_box_gtin(self, raw_text: str) -> str:
        """Â§ñÁÆ±GTIN„ÇíÊäΩÂá∫"""
        outer_gtin_patterns = [
            r'Â§ñÁÆ±GTIN[Ôºö:\s]*([0-9]{13,14})',
            r'Â§ñÁÆ±JAN[Ôºö:\s]*([0-9]{13,14})',
            r'Outer\s*Box\s*GTIN[Ôºö:\s]*([0-9]{13,14})',
            r'GTIN\s*Â§ñÁÆ±[Ôºö:\s]*([0-9]{13,14})',
            r'„Ç´„Éº„Éà„É≥GTIN[Ôºö:\s]*([0-9]{13,14})',
        ]
        
        for pattern in outer_gtin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                gtin = match.group(1)
                if len(gtin) in [13, 14]:  # GTIN-13 or GTIN-14
                    return gtin
        
        return None
    
    def _extract_sku(self, raw_text: str, text_lines: list) -> str:
        """SKU/ÂïÜÂìÅ„Ç≥„Éº„Éâ/ÂìÅÁï™„ÇíÊäΩÂá∫ÔºàST-„Ç≥„Éº„Éâ„ÄÅEN-„Ç≥„Éº„Éâ„Å™„Å©Ôºâ"""
        sku_patterns = [
            # ST-„Ç≥„Éº„ÉâÔºà„Éù„Ç±„É¢„É≥ÂïÜÂìÅ„Åß„Çà„Åè‰ΩøÁî®Ôºâ
            r'(ST-\d{2}[A-Z]{2})',  # ST-03CB, ST-04CB „Å™„Å©
            r'(ST-\d{2}[A-Z]\d)',   # ST-03C1 „Å™„Å©
            r'ÂìÅÁï™[Ôºö:\s]*(ST-\d{2}[A-Z]{2})',
            r'ÂïÜÂìÅ„Ç≥„Éº„Éâ[Ôºö:\s]*(ST-\d{2}[A-Z]{2})',
            r'„Ç≥„Éº„Éâ[Ôºö:\s]*(ST-\d{2}[A-Z]{2})',
            
            # EN-„Ç≥„Éº„ÉâÔºà„Ç®„É≥„Çπ„Ç´„Ç§ÂïÜÂìÅÔºâ
            r'(EN-\d{3,4}[A-Z]*)',  # EN-142, EN-142A „Å™„Å©
            r'ÂìÅÁï™[Ôºö:\s]*(EN-\d{3,4}[A-Z]*)',
            r'ÂïÜÂìÅ„Ç≥„Éº„Éâ[Ôºö:\s]*(EN-\d{3,4}[A-Z]*)',
            
            # ‰∏ÄËà¨ÁöÑ„Å™ÂïÜÂìÅ„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥
            r'ÂìÅÁï™[Ôºö:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            r'ÂïÜÂìÅ„Ç≥„Éº„Éâ[Ôºö:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            r'SKU[Ôºö:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            r'Product\s*Code[Ôºö:\s]*([A-Z]{2,4}-\d{2,4}[A-Z]*)',
            
            # ‰ªñ„ÅÆÂΩ¢Âºè
            r'([A-Z]{2}-\d{2}[A-Z]{2})',  # XX-##XX ÂΩ¢Âºè
            r'([A-Z]{3}-\d{3,4})',        # XXX-### ÂΩ¢Âºè
        ]
        
        print(f"üîç SKUÊäΩÂá∫ÈñãÂßã: {raw_text[:100]}...")
        
        for pattern in sku_patterns:
            matches = re.findall(pattern, raw_text, re.IGNORECASE)
            for match in matches:
                sku = match.upper()  # Â§ßÊñáÂ≠ó„Å´Áµ±‰∏Ä
                print(f"‚úÖ SKUÂÄôË£úÁô∫Ë¶ã: {sku}")
                
                # Â¶•ÂΩìÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
                if len(sku) >= 5 and len(sku) <= 10:  # ÈÅ©Âàá„Å™Èï∑„Åï
                    if '-' in sku:  # „Éè„Ç§„Éï„É≥„ÇíÂê´„ÇÄ
                        return sku
        
        # „Éû„É´„ÉÅ„Éó„É≠„ÉÄ„ÇØ„Éà„ÅÆÂ†¥Âêà„ÄÅË§áÊï∞„ÅÆST-„Ç≥„Éº„Éâ„Åã„ÇâÊúÄÂàù„ÅÆ„ÇÇ„ÅÆ„ÇíÈÅ∏Êäû
        st_codes = re.findall(r'ST-\d{2}[A-Z]{2}', raw_text)
        if st_codes:
            print(f"‚úÖ „Éû„É´„ÉÅ„Éó„É≠„ÉÄ„ÇØ„Éà ST-„Ç≥„Éº„Éâ: {st_codes}")
            return st_codes[0]  # ÊúÄÂàù„ÅÆST-„Ç≥„Éº„Éâ„ÇíËøî„Åô
        
        # EN-„Ç≥„Éº„Éâ„ÇÇÂêåÊßò„Å´Âá¶ÁêÜ
        en_codes = re.findall(r'EN-\d{3,4}[A-Z]*', raw_text)
        if en_codes:
            print(f"‚úÖ EN-„Ç≥„Éº„Éâ: {en_codes}")
            return en_codes[0]
        
        print("‚ùå SKU not found")
        return None
    
    def _extract_dimensions(self, raw_text: str, text_lines: list) -> str:
        """„Çµ„Ç§„Ç∫ÊÉÖÂ†±„ÇíÊäΩÂá∫ÔºàÊîπËâØÁâàÔºâ"""
        dimension_patterns = [
            # ÂïÜÂìÅ„Çµ„Ç§„Ç∫„ÅÆÊòéÁ§∫ÁöÑ„Å™Ë°®Ë®ò
            r'ÂïÜÂìÅ„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'ÂçòÂìÅ„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'Êú¨‰Ωì„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'Ë£ΩÂìÅ„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'ÂØ∏Ê≥ï[Ôºö:\s]*([^\n\r]+)',
            r'Â§ß„Åç„Åï[Ôºö:\s]*([^\n\r]+)',
            r'Dimensions[Ôºö:\s]*([^\n\r]+)',
            r'Size[Ôºö:\s]*([^\n\r]+)',
            
            # ÂÖ∑‰ΩìÁöÑ„Å™Êï∞ÂÄ§„Éë„Çø„Éº„É≥Ôºà„Éù„Ç±„É¢„É≥„ÅÆÂ†¥Âêà„ÅÆ„Éë„Çø„Éº„É≥„ÇíÂê´„ÇÄÔºâ
            r'„Éù„Ç±„É¢„É≥„ÅÆÂ†¥Âêà\s*Á¥Ñ\s*(\d+)\s*√ó\s*(\d+)\s*√ó\s*(\d+)\s*mm',
            r'Á¥Ñ\s*(\d+)\s*√ó\s*(\d+)\s*√ó\s*(\d+)\s*mm',  # Á¥Ñ107√ó70√ó61mm
            r'(\d+)\s*x\s*(\d+)\s*x\s*(\d+)\s*mm',        # 107x70x61mm
            r'(\d+)\s*√ó\s*(\d+)\s*√ó\s*(\d+)\s*cm',        # cmË°®Ë®ò
            r'(\d+)\s*√ó\s*(\d+)\s*mm',                    # 2Ê¨°ÂÖÉ
            r'(\d+)\s*mm\s*√ó\s*(\d+)\s*mm\s*√ó\s*(\d+)\s*mm',  # È†ÜÂ∫èÈÅï„ÅÑ
            
            # Ëã±Ë™ûË°®Ë®ò
            r'(\d+)\s*x\s*(\d+)\s*x\s*(\d+)\s*inches',
            r'(\d+)\.?\d*\s*"\s*x\s*(\d+)\.?\d*\s*"\s*x\s*(\d+)\.?\d*\s*"',
        ]
        
        for pattern in dimension_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                if len(match.groups()) == 1:
                    # ÊñáÂ≠óÂàó„Å®„Åó„Å¶ÂèñÂæó
                    size_text = match.group(1).strip()
                    if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                        print(f"‚úÖ „Çµ„Ç§„Ç∫ÔºàÊñáÂ≠óÂàóÔºâ: {size_text}")
                        return size_text
                elif len(match.groups()) == 3:
                    # 3Ê¨°ÂÖÉ„Çµ„Ç§„Ç∫
                    width, height, depth = match.groups()
                    if all(w.isdigit() for w in [width, height, depth]):
                        if '„Éù„Ç±„É¢„É≥„ÅÆÂ†¥Âêà' in pattern:
                            size_str = f"Á¥Ñ{width}√ó{height}√ó{depth}mm"
                        elif 'cm' in pattern:
                            size_str = f"Á¥Ñ{width}√ó{height}√ó{depth}cm"
                        else:
                            size_str = f"Á¥Ñ{width}√ó{height}√ó{depth}mm"
                        print(f"‚úÖ „Çµ„Ç§„Ç∫Ôºà3Ê¨°ÂÖÉÔºâ: {size_str}")
                        return size_str
                elif len(match.groups()) == 2:
                    # 2Ê¨°ÂÖÉ„Çµ„Ç§„Ç∫
                    width, height = match.groups()
                    if all(w.isdigit() for w in [width, height]):
                        size_str = f"Á¥Ñ{width}√ó{height}mm"
                        print(f"‚úÖ „Çµ„Ç§„Ç∫Ôºà2Ê¨°ÂÖÉÔºâ: {size_str}")
                        return size_str
        
        # ÁâπÂà•„Å™„Ç±„Éº„ÇπÔºö„Éù„Ç±„É¢„É≥„Ç≥„Ç§„É≥„Éê„É≥„ÇØ„ÅÆ„Éá„Éï„Ç©„É´„Éà„Çµ„Ç§„Ç∫
        if '„Éù„Ç±„É¢„É≥' in raw_text and '„Ç≥„Ç§„É≥„Éê„É≥„ÇØ' in raw_text:
            # ‰∏ÄËà¨ÁöÑ„Å™„Çµ„Ç§„Ç∫ÊÉÖÂ†±„Åå„ÅÇ„Çã„ÅãÁ¢∫Ë™ç
            general_size_match = re.search(r'Á¥Ñ\s*(\d+)\s*√ó\s*(\d+)\s*√ó\s*(\d+)', raw_text)
            if general_size_match:
                w, h, d = general_size_match.groups()
                return f"Á¥Ñ{w}√ó{h}√ó{d}mm"
        
        return None
    
    def _get_character_for_jan_code(self, jan_code: str) -> str:
        """JAN„Ç≥„Éº„Éâ„Åã„Çâ„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç„ÇíÈÄÜÂºï„Åç"""
        jan_character_mapping = {
            '4970381804220': '„Éî„Ç´„ÉÅ„É•„Ç¶',
            '4970381804213': '„Ç§„Éº„Éñ„Ç§',
            '4970381804206': '„Éè„É™„Éû„É≠„É≥',
            '4970381804199': '„Éï„Ç©„ÉÉ„Ç≥',
            '4970381804182': '„Ç±„É≠„Éû„ÉÑ',
            '4970381804237': '„Éê„É¢',
            '4970381804234': '„Éè„É©„Éê„É™„Éº',
            '4970381804175': '„É¢„ÇØ„É≠„Éº',
            '4970381804168': '„Éã„É£„Éì„Éº',
            '4970381804161': '„Ç¢„Ç∑„Éû„É™'
        }
        return jan_character_mapping.get(jan_code, '')
    
    def _get_st_code_for_jan_code(self, jan_code: str) -> str:
        """JAN„Ç≥„Éº„Éâ„Åã„ÇâST-„Ç≥„Éº„Éâ„ÇíÈÄÜÂºï„Åç"""
        jan_st_mapping = {
            '4970381804220': 'ST-03CB',  # „Éî„Ç´„ÉÅ„É•„Ç¶
            '4970381804213': 'ST-04CB',  # „Ç§„Éº„Éñ„Ç§
            '4970381804206': 'ST-05CB',  # „Éè„É™„Éû„É≠„É≥
            '4970381804199': 'ST-06CB',  # „Éï„Ç©„ÉÉ„Ç≥
            '4970381804182': 'ST-07CB',  # „Ç±„É≠„Éû„ÉÑ
            '4970381804237': 'ST-08CB',  # „Éê„É¢
            '4970381804234': 'ST-09CB',  # „Éè„É©„Éê„É™„Éº
            '4970381804175': 'ST-10CB',  # „É¢„ÇØ„É≠„Éº
            '4970381804168': 'ST-11CB',  # „Éã„É£„Éì„Éº
            '4970381804161': 'ST-12CB'   # „Ç¢„Ç∑„Éû„É™
        }
        return jan_st_mapping.get(jan_code, '')
    
    def _create_clean_product_data_for_st_code(self, st_code: str, section_text: str, product_index: int) -> Dict[str, Any]:
        """ST-„Ç≥„Éº„ÉâÁî®„ÅÆ„ÇØ„É™„Éº„É≥„Å™ÂïÜÂìÅ„Éá„Éº„Çø„Çí‰ΩúÊàêÔºàÈñìÈÅï„Å£„ÅüÊÉÖÂ†±„ÇíÁ∂ôÊâø„Åó„Å™„ÅÑÔºâ"""
        
        # ST-„Ç≥„Éº„Éâ„Åã„ÇâÁ¢∫ÂÆü„Å´ÊÉÖÂ†±„ÇíÂèñÂæó
        character_name = self._get_character_for_st_code(st_code)
        direct_jan = self._get_jan_code_for_st_code(st_code)
        
        print(f"   üßπ Creating clean data for {st_code}: Character={character_name}, JAN={direct_jan}")
        
        # „ÇØ„É™„Éº„É≥„Å™„Éô„Éº„Çπ„Éá„Éº„Çø„Çí‰ΩúÊàê
        clean_data = {
            'product_index': product_index,
            'sku': st_code,
            'jan_code': direct_jan,
            'product_name': f"{character_name} „Ç≥„Ç§„É≥„Éê„É≥„ÇØ {st_code}" if character_name else f"„Éù„Ç±„É¢„É≥ „Ç≥„Ç§„É≥„Éê„É≥„ÇØ {st_code}",
            'category': '„Ç¢„Éã„É°„Ç∞„ÉÉ„Ç∫',
            'brand': '„Ç®„É≥„Çπ„Ç´„Ç§',
            'manufacturer': 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§',
            'origin': 'Êó•Êú¨',
            'target_age': '3Ê≠≥‰ª•‰∏ä',
            'dimensions': "Á¥Ñ107√ó70√ó61mm",
            'product_size': "Á¥Ñ107√ó70√ó61mm",
            'description': f'{character_name}„ÅÆÂèØÊÑõ„ÅÑË≤ØÈáëÁÆ±„Åß„Åô„ÄÇ„Ç§„É≥„ÉÜ„É™„Ç¢„Å®„Åó„Å¶„ÇÇÊ•Ω„Åó„ÇÅ„Åæ„Åô„ÄÇ' if character_name else '„Éù„Ç±„É¢„É≥„ÅÆÂèØÊÑõ„ÅÑË≤ØÈáëÁÆ±„Åß„Åô„ÄÇ„Ç§„É≥„ÉÜ„É™„Ç¢„Å®„Åó„Å¶„ÇÇÊ•Ω„Åó„ÇÅ„Åæ„Åô„ÄÇ',
            'section_text': section_text[:300] + "..." if len(section_text) > 300 else section_text
        }
        
        # „Çª„ÇØ„Ç∑„Éß„É≥„Åã„Çâ‰æ°Ê†ºÊÉÖÂ†±„ÅÆ„Åø„ÇíÂÆâÂÖ®„Å´ÊäΩÂá∫
        try:
            section_data = self._parse_product_data_from_text(section_text)
            if section_data:
                # ‰æ°Ê†ºÊÉÖÂ†±„ÅØÁ∂ôÊâøÔºà‰ªñ„ÅÆÂïÜÂìÅ„Å®ÂÖ±ÈÄö„ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Åü„ÇÅÔºâ
                if section_data.get('price'):
                    clean_data['price'] = section_data['price']
                    print(f"   üí∞ Price extracted: {section_data['price']}")
                
                # Áô∫Â£≤Êó•ÊÉÖÂ†±„ÅØÁ∂ôÊâøÔºà‰ªñ„ÅÆÂïÜÂìÅ„Å®ÂÖ±ÈÄö„ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Åü„ÇÅÔºâ
                if section_data.get('release_date'):
                    clean_data['release_date'] = section_data['release_date']
                    print(f"   üìÖ Release date extracted: {section_data['release_date']}")
                
                # Âú®Â∫´ÊÉÖÂ†±„ÅØÁ∂ôÊâøÔºà‰ªñ„ÅÆÂïÜÂìÅ„Å®ÂÖ±ÈÄö„ÅÆÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Åü„ÇÅÔºâ
                if section_data.get('stock'):
                    clean_data['stock'] = section_data['stock']
                    print(f"   üì¶ Stock extracted: {section_data['stock']}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error extracting section data: {e}")
        
        print(f"   ‚úÖ Clean data created for {st_code}: {clean_data['product_name']} JAN: {clean_data['jan_code']}")
        return clean_data
    
    def _extract_package_size(self, raw_text: str, text_lines: list) -> str:
        """„Éë„ÉÉ„Ç±„Éº„Ç∏„Çµ„Ç§„Ç∫„ÇíÊäΩÂá∫"""
        package_patterns = [
            r'„Éë„ÉÉ„Ç±„Éº„Ç∏„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'Package\s*Size[Ôºö:\s]*([^\n\r]+)',
            r'ÁÆ±„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'Â§ñÁÆ±„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
        ]
        
        for pattern in package_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        
        return None
    
    def _extract_inner_box_size(self, raw_text: str, text_lines: list) -> str:
        """ÂÜÖÁÆ±„Çµ„Ç§„Ç∫„ÇíÊäΩÂá∫"""
        inner_box_patterns = [
            r'ÂÜÖÁÆ±„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'Inner\s*Box\s*Size[Ôºö:\s]*([^\n\r]+)',
            r'„Ç±„Éº„Çπ„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
        ]
        
        for pattern in inner_box_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        
        return None
    
    def _extract_carton_size(self, raw_text: str, text_lines: list) -> str:
        """„Ç´„Éº„Éà„É≥„Çµ„Ç§„Ç∫„ÇíÊäΩÂá∫"""
        carton_patterns = [
            r'„Ç´„Éº„Éà„É≥„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'Carton\s*Size[Ôºö:\s]*([^\n\r]+)',
            r'Â§ñË£Ö„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'ÊÆµ„Éú„Éº„É´„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
        ]
        
        for pattern in carton_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        
        return None
    
    def _extract_package_type(self, raw_text: str, text_lines: list) -> str:
        """„Éë„ÉÉ„Ç±„Éº„Ç∏ÂΩ¢ÊÖã„ÇíÊäΩÂá∫"""
        package_type_patterns = [
            r'„Éë„ÉÉ„Ç±„Éº„Ç∏ÂΩ¢ÊÖã[Ôºö:\s]*([^\n\r]+)',
            r'Package\s*Type[Ôºö:\s]*([^\n\r]+)',
            r'ÂåÖË£ÖÂΩ¢ÊÖã[Ôºö:\s]*([^\n\r]+)',
            r'Ê¢±ÂåÖÂΩ¢ÊÖã[Ôºö:\s]*([^\n\r]+)',
            r'„Éë„ÉÉ„Ç±„Éº„Ç∏[Ôºö:\s]*([^\n\r]+)',
        ]
        
        for pattern in package_type_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                package_text = match.group(1).strip()
                if len(package_text) < 100:
                    return package_text
        
        return None
    
    # ========== ËøΩÂä†„ÅÆ38È†ÖÁõÆÊäΩÂá∫„É°„ÇΩ„ÉÉ„Éâ ==========
    
    def _extract_lot_number(self, raw_text: str) -> str:
        """„É≠„ÉÉ„ÉàÁï™Âè∑„ÇíÊäΩÂá∫"""
        lot_patterns = [
            r'„É≠„ÉÉ„Éà[Áï™]?[Âè∑]?[Ôºö:\s]*([A-Z0-9\-]+)',
            r'Lot\s*(?:No\.?|Number)[Ôºö:\s]*([A-Z0-9\-]+)',
            r'LOT[Ôºö:\s]*([A-Z0-9\-]+)',
        ]
        for pattern in lot_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        return None
    
    def _extract_classification(self, raw_text: str) -> str:
        """Âå∫ÂàÜ„ÇíÊäΩÂá∫"""
        classification_patterns = [
            r'Âå∫ÂàÜ[Ôºö:\s]*([^\n\r,]+)',
            r'ÂàÜÈ°û[Ôºö:\s]*([^\n\r,]+)',
            r'Classification[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in classification_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                class_text = match.group(1).strip()
                if len(class_text) < 50:
                    return class_text
        return None
    
    def _extract_major_category(self, raw_text: str, text_lines: list) -> str:
        """Â§ßÂàÜÈ°û„ÇíÊäΩÂá∫"""
        major_patterns = [
            r'Â§ßÂàÜÈ°û[Ôºö:\s]*([^\n\r,]+)',
            r'Main\s*Category[Ôºö:\s]*([^\n\r,]+)',
            r'Primary\s*Category[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in major_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                category_text = match.group(1).strip()
                if len(category_text) < 50:
                    return category_text
        return None
    
    def _extract_minor_category(self, raw_text: str, text_lines: list) -> str:
        """‰∏≠ÂàÜÈ°û„ÇíÊäΩÂá∫"""
        minor_patterns = [
            r'‰∏≠ÂàÜÈ°û[Ôºö:\s]*([^\n\r,]+)',
            r'Sub\s*Category[Ôºö:\s]*([^\n\r,]+)',
            r'Secondary\s*Category[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in minor_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                category_text = match.group(1).strip()
                if len(category_text) < 50:
                    return category_text
        return None
    
    def _extract_product_code(self, raw_text: str, text_lines: list) -> str:
        """ÂïÜÂìÅÁï™Âè∑„ÇíÊäΩÂá∫ÔºàSKU„Å®‰ºº„Å¶„ÅÑ„Çã„ÅåÂà•„ÅÆÂ†¥Âêà„Åå„ÅÇ„ÇãÔºâ"""
        product_code_patterns = [
            r'ÂïÜÂìÅÁï™Âè∑[Ôºö:\s]*([A-Z0-9\-]+)',
            r'ÂìÅÁï™[Ôºö:\s]*([A-Z0-9\-]+)',
            r'Product\s*(?:Code|No\.?|Number)[Ôºö:\s]*([A-Z0-9\-]+)',
            r'Item\s*(?:Code|No\.?|Number)[Ôºö:\s]*([A-Z0-9\-]+)',
        ]
        for pattern in product_code_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                code = match.group(1).strip()
                if 3 <= len(code) <= 30:
                    return code
        return None
    
    def _extract_in_store(self, raw_text: str) -> str:
        """„Ç§„É≥„Çπ„Éà„Ç¢ÊÉÖÂ†±„ÇíÊäΩÂá∫"""
        in_store_patterns = [
            r'„Ç§„É≥„Çπ„Éà„Ç¢[Ôºö:\s]*([^\n\r,]+)',
            r'In[-\s]?Store[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in in_store_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                in_store_text = match.group(1).strip()
                if len(in_store_text) < 50:
                    return in_store_text
        return None
    
    def _extract_genre_name(self, raw_text: str, text_lines: list) -> str:
        """„Ç∏„É£„É≥„É´ÂêçÁß∞„ÇíÊäΩÂá∫"""
        genre_patterns = [
            r'„Ç∏„É£„É≥„É´ÂêçÁß∞[Ôºö:\s]*([^\n\r,]+)',
            r'„Ç∏„É£„É≥„É´[Ôºö:\s]*([^\n\r,]+)',
            r'Genre[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in genre_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                genre_text = match.group(1).strip()
                if len(genre_text) < 100:
                    return genre_text
        return None
    
    def _extract_supplier_name(self, raw_text: str) -> str:
        """‰ªïÂÖ•ÂÖà„ÇíÊäΩÂá∫"""
        supplier_patterns = [
            r'‰ªïÂÖ•ÂÖà[Ôºö:\s]*([^\n\r,]+)',
            r'‰ªïÂÖ•„ÇåÂÖà[Ôºö:\s]*([^\n\r,]+)',
            r'Supplier[Ôºö:\s]*([^\n\r,]+)',
            r'Vendor[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in supplier_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                supplier_text = match.group(1).strip()
                if len(supplier_text) < 100:
                    return supplier_text
        return None
    
    def _extract_ip_name(self, raw_text: str, cleaned_lines: list) -> str:
        """„É°„Éº„Ç´„ÉºÂêçÁß∞ÔºàIPÂêçÔºâ„ÇíÊäΩÂá∫"""
        ip_patterns = [
            r'„É°„Éº„Ç´„ÉºÂêçÁß∞[Ôºö:\s]*([^\n\r,]+)',
            r'IPÂêç[Ôºö:\s]*([^\n\r,]+)',
            r'Manufacturer\s*Name[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in ip_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                ip_text = match.group(1).strip()
                if len(ip_text) < 100:
                    return ip_text
        return None
    
    def _extract_character_name(self, raw_text: str, text_lines: list) -> str:
        """„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêçÔºàIPÂêçÔºâ„ÇíÊäΩÂá∫"""
        character_patterns = [
            r'„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç\s*\(IPÂêç\)[Ôºö:\s]*([^\n\r,]+)',
            r'„Ç≠„É£„É©„ÇØ„Çø„ÉºÂêç[Ôºö:\s]*([^\n\r,]+)',
            r'Character\s*Name[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in character_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                char_text = match.group(1).strip()
                if len(char_text) < 100:
                    return char_text
        return None
    
    def _extract_reference_sales_price(self, raw_text: str) -> float:
        """ÂèÇËÄÉË≤©Â£≤‰æ°Ê†º„ÇíÊäΩÂá∫"""
        price_patterns = [
            r'ÂèÇËÄÉË≤©Â£≤‰æ°Ê†º[Ôºö:\s]*[¬•Ôø•]?\s*([0-9,]+)',
            r'Â∏åÊúõÂ∞èÂ£≤‰æ°Ê†º[Ôºö:\s]*[¬•Ôø•]?\s*([0-9,]+)',
            r'Reference\s*Price[Ôºö:\s]*[¬•Ôø•$]?\s*([0-9,]+)',
        ]
        for pattern in price_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                price_str = match.group(1).replace(',', '')
                try:
                    price = float(price_str)
                    if 0 < price < 1000000:
                        return price
                except ValueError:
                    continue
        return None
    
    def _extract_wholesale_price(self, raw_text: str) -> float:
        """Âç∏Âçò‰æ°ÔºàÊäúÔºâ„ÇíÊäΩÂá∫"""
        wholesale_patterns = [
            r'Âç∏Âçò‰æ°[Ôºö:\s]*[¬•Ôø•]?\s*([0-9,]+)',
            r'Âç∏‰æ°Ê†º[Ôºö:\s]*[¬•Ôø•]?\s*([0-9,]+)',
            r'Wholesale\s*Price[Ôºö:\s]*[¬•Ôø•$]?\s*([0-9,]+)',
        ]
        for pattern in wholesale_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                price_str = match.group(1).replace(',', '')
                try:
                    price = float(price_str)
                    if 0 < price < 1000000:
                        return price
                except ValueError:
                    continue
        return None
    
    def _extract_wholesale_quantity(self, raw_text: str) -> int:
        """Âç∏ÂèØËÉΩÊï∞„ÇíÊäΩÂá∫"""
        quantity_patterns = [
            r'Âç∏ÂèØËÉΩÊï∞[Ôºö:\s]*([0-9,]+)',
            r'Âç∏„ÅóÂèØËÉΩÊï∞[Ôºö:\s]*([0-9,]+)',
            r'Available\s*Quantity[Ôºö:\s]*([0-9,]+)',
        ]
        for pattern in quantity_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                qty_str = match.group(1).replace(',', '')
                try:
                    qty = int(qty_str)
                    if 0 <= qty < 1000000:
                        return qty
                except ValueError:
                    continue
        return None
    
    def _extract_order_amount(self, raw_text: str) -> float:
        """Áô∫Ê≥®ÈáëÈ°ç„ÇíÊäΩÂá∫"""
        amount_patterns = [
            r'Áô∫Ê≥®ÈáëÈ°ç[Ôºö:\s]*[¬•Ôø•]?\s*([0-9,]+)',
            r'Ê≥®ÊñáÈáëÈ°ç[Ôºö:\s]*[¬•Ôø•]?\s*([0-9,]+)',
            r'Order\s*Amount[Ôºö:\s]*[¬•Ôø•$]?\s*([0-9,]+)',
        ]
        for pattern in amount_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                amount_str = match.group(1).replace(',', '')
                try:
                    amount = float(amount_str)
                    if 0 < amount < 10000000:
                        return amount
                except ValueError:
                    continue
        return None
    
    def _extract_reservation_release_date(self, raw_text: str) -> str:
        """‰∫àÁ¥ÑËß£Á¶ÅÊó•„ÇíÊäΩÂá∫"""
        date_patterns = [
            r'‰∫àÁ¥ÑËß£Á¶ÅÊó•[Ôºö:\s]*([0-9Âπ¥ÊúàÊó•/\-\.]+)',
            r'Reservation\s*Start\s*Date[Ôºö:\s]*([0-9/\-\.]+)',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                date_text = match.group(1).strip()
                if len(date_text) < 30:
                    return date_text
        return None
    
    def _extract_reservation_deadline(self, raw_text: str) -> str:
        """‰∫àÁ¥ÑÁ∑†„ÇÅÂàá„ÇäÊó•„ÇíÊäΩÂá∫"""
        date_patterns = [
            r'‰∫àÁ¥ÑÁ∑†[„ÇÅ]?Âàá[„Çä]?Êó•[Ôºö:\s]*([0-9Âπ¥ÊúàÊó•/\-\.]+)',
            r'Reservation\s*Deadline[Ôºö:\s]*([0-9/\-\.]+)',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                date_text = match.group(1).strip()
                if len(date_text) < 30:
                    return date_text
        return None
    
    def _extract_reservation_shipping_date(self, raw_text: str) -> str:
        """‰∫àÁ¥ÑÂïÜÂìÅÁô∫ÈÄÅ‰∫àÂÆöÊó•„ÇíÊäΩÂá∫"""
        date_patterns = [
            r'‰∫àÁ¥ÑÂïÜÂìÅÁô∫ÈÄÅ‰∫àÂÆöÊó•[Ôºö:\s]*([0-9Âπ¥ÊúàÊó•/\-\.]+)',
            r'Áô∫ÈÄÅ‰∫àÂÆöÊó•[Ôºö:\s]*([0-9Âπ¥ÊúàÊó•/\-\.]+)',
            r'Shipping\s*Date[Ôºö:\s]*([0-9/\-\.]+)',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                date_text = match.group(1).strip()
                if len(date_text) < 30:
                    return date_text
        return None
    
    def _extract_case_pack_quantity(self, raw_text: str) -> int:
        """„Ç±„Éº„ÇπÊ¢±ÂÖ•Êï∞„ÇíÊäΩÂá∫"""
        case_patterns = [
            r'„Ç±„Éº„ÇπÊ¢±ÂÖ•Êï∞[Ôºö:\s]*([0-9,]+)',
            r'„Ç±„Éº„ÇπÂÖ•Êï∞[Ôºö:\s]*([0-9,]+)',
            r'Case\s*Pack\s*Quantity[Ôºö:\s]*([0-9,]+)',
        ]
        for pattern in case_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                qty_str = match.group(1).replace(',', '')
                try:
                    qty = int(qty_str)
                    if 0 < qty < 10000:
                        return qty
                except ValueError:
                    continue
        return None
    
    def _extract_single_product_size(self, raw_text: str, text_lines: list) -> str:
        """ÂçòÂìÅ„Çµ„Ç§„Ç∫„ÇíÊäΩÂá∫"""
        size_patterns = [
            r'ÂçòÂìÅ„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
            r'Single\s*Product\s*Size[Ôºö:\s]*([^\n\r]+)',
            r'ÂÄãÂà•„Çµ„Ç§„Ç∫[Ôºö:\s]*([^\n\r]+)',
        ]
        for pattern in size_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                size_text = match.group(1).strip()
                if len(size_text) < 100 and any(char.isdigit() for char in size_text):
                    return size_text
        return None
    
    def _extract_protective_film_material(self, raw_text: str) -> str:
        """Ê©üÊùê„Éï„Ç£„É´„É†„ÇíÊäΩÂá∫"""
        film_patterns = [
            r'Ê©üÊùê„Éï„Ç£„É´„É†[Ôºö:\s]*([^\n\r,]+)',
            r'‰øùË≠∑„Éï„Ç£„É´„É†[Ôºö:\s]*([^\n\r,]+)',
            r'Protective\s*Film[Ôºö:\s]*([^\n\r,]+)',
        ]
        for pattern in film_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                film_text = match.group(1).strip()
                if len(film_text) < 100:
                    return film_text
        return None
    
    def _extract_country_of_origin(self, raw_text: str, text_lines: list) -> str:
        """ÂéüÁî£ÂõΩ„ÇíÊäΩÂá∫Ôºà„Çà„ÇäÂº∑ÂåñÁâàÔºâ"""
        # Êó¢Â≠ò„ÅÆorigin„É°„ÇΩ„ÉÉ„Éâ„ÇíÂÜçÂà©Áî®„Åó„ÄÅ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™„Éë„Çø„Éº„É≥„ÇíËøΩÂä†
        origin_patterns = [
            r'ÂéüÁî£ÂõΩ[Ôºö:\s]*([^\n\r,]+)',
            r'Ë£ΩÈÄ†ÂõΩ[Ôºö:\s]*([^\n\r,]+)',
            r'ÁîüÁî£ÂõΩ[Ôºö:\s]*([^\n\r,]+)',
            r'Country\s*of\s*Origin[Ôºö:\s]*([^\n\r,]+)',
            r'Made\s*in[Ôºö:\s]*([A-Z][a-z]+)',
        ]
        for pattern in origin_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                origin_text = match.group(1).strip()
                if len(origin_text) < 50:
                    return origin_text
        return None
    
    def _extract_image_url(self, raw_text: str, image_number: int) -> str:
        """ÁîªÂÉèURL„ÇíÊäΩÂá∫"""
        url_patterns = [
            rf'ÁîªÂÉè{image_number}[Ôºö:\s]*(https?://[^\s\n\r]+)',
            rf'Image\s*{image_number}[Ôºö:\s]*(https?://[^\s\n\r]+)',
            rf'img{image_number}[Ôºö:\s]*(https?://[^\s\n\r]+)',
        ]
        for pattern in url_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                url = match.group(1).strip()
                if url.startswith('http'):
                    return url
        return None

    def _extract_all_fields_from_excel_row(self, row_text: str, full_text: str = "") -> Dict[str, Any]:
        """ExcelË°å„Åã„Çâ38È†ÖÁõÆ„Åô„Åπ„Å¶„ÇíÊäΩÂá∫"""
        print(f"üîç Extracting all 38 fields from Excel row: {row_text[:100]}")
        
        product_data = {}
        
        # 1. Âü∫Êú¨ÊÉÖÂ†±„ÅÆÊäΩÂá∫
        # SKU/ÂïÜÂìÅ„Ç≥„Éº„Éâ (EN-XXXX)
        sku_match = re.search(r'(EN-\d+)', row_text)
        if sku_match:
            product_data['sku'] = sku_match.group(1)
            product_data['product_code'] = sku_match.group(1)
        
        # JAN„Ç≥„Éº„Éâ (4970381-XXXXXX or 13Ê°Å)
        jan_patterns = [
            r'4970381-?(\d{6})',  # „Ç®„É≥„Çπ„Ç´„Ç§„ÅÆJAN„Ç≥„Éº„Éâ
            r'(\d{13})',          # Ê®ôÊ∫ñ13Ê°Å
            r'(\d{8})',           # 8Ê°Å
        ]
        for pattern in jan_patterns:
            jan_match = re.search(pattern, row_text)
            if jan_match:
                jan_code = jan_match.group(0).replace('-', '')
                if len(jan_code) >= 8:
                    product_data['jan_code'] = jan_code
                    break
        
        # ‰æ°Ê†º (¬•X,XXX or X„Éë„ÉÉ„ÇØ X,XXXÂÜÜ)
        price_patterns = [
            r'[¬•Ôø•]?\s*(\d{1,3}(?:,\d{3})+)\s*ÂÜÜ',
            r'(\d{1,3}(?:,\d{3})+)\s*ÂÜÜ',
            r'[¬•Ôø•]\s*(\d+)',
        ]
        for pattern in price_patterns:
            price_match = re.search(pattern, row_text)
            if price_match:
                price_str = price_match.group(1).replace(',', '')
                try:
                    price_num = int(price_str)
                    if 100 <= price_num <= 100000:
                        product_data['price'] = str(price_num)
                        product_data['wholesale_price'] = price_num
                        product_data['reference_sales_price'] = price_num
                        break
                except ValueError:
                    continue
        
        # ÂïÜÂìÅÂêç („Ç≠„É£„É©„ÇØ„Çø„Éº„Çπ„É™„Éº„Éñ„ÄéXXX„ÄèYYY)
        product_name_patterns = [
            r'„Ç≠„É£„É©„ÇØ„Çø„Éº„Çπ„É™„Éº„Éñ[„Äé„Äå]([^„Äè„Äç]+)[„Äè„Äç]\s*([^\(|]+)',
            r'([^|]+)\(EN-\d+\)',
        ]
        for pattern in product_name_patterns:
            name_match = re.search(pattern, row_text)
            if name_match:
                if len(name_match.groups()) >= 2:
                    product_data['product_name'] = f"{name_match.group(1)} {name_match.group(2)}".strip()
                else:
                    product_data['product_name'] = name_match.group(1).strip()
                break
        
        # „Ç´„Éº„Éà„É≥ÂÖ•Êï∞
        carton_patterns = [
            r'(\d+)ÂÖ•\s*\((\d+)„Éë„ÉÉ„ÇØ[√óx](\d+)BOX\)',
            r'„Ç´„Éº„Éà„É≥ÂÖ•Êï∞[Ôºö:\s]*(\d+)',
        ]
        for pattern in carton_patterns:
            carton_match = re.search(pattern, row_text)
            if carton_match:
                if len(carton_match.groups()) >= 3:
                    total = int(carton_match.group(1))
                    product_data['case_pack_quantity'] = total
                else:
                    product_data['case_pack_quantity'] = int(carton_match.group(1))
                break
        
        # 2. „Ç´„ÉÜ„Ç¥„É™„Å®„Éñ„É©„É≥„ÉâÊÉÖÂ†±
        # Excel„Éï„Ç°„Ç§„É´„Åã„ÇâÊé®Ê∏¨
        if '„Ç≠„É£„É©„ÇØ„Çø„Éº„Çπ„É™„Éº„Éñ' in row_text or '„Çπ„É™„Éº„Éñ' in row_text:
            product_data['category'] = '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞„Ç´„Éº„Éâ„Ç¢„ÇØ„Çª„Çµ„É™„Éº'
            product_data['major_category'] = '„Éõ„Éì„Éº„Éª„Éà„Ç§'
            product_data['minor_category'] = '„Éà„É¨„Éº„Éá„Ç£„É≥„Ç∞„Ç´„Éº„Éâ„Ç∞„ÉÉ„Ç∫'
            product_data['genre_name'] = '„Ç≠„É£„É©„ÇØ„Çø„Éº„Ç∞„ÉÉ„Ç∫'
        
        # „Éñ„É©„É≥„ÉâÊÉÖÂ†±ÔºàÂÖ®„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÊäΩÂá∫Ôºâ
        if '„Ç®„É≥„Çπ„Ç´„Ç§' in full_text or 'EN-' in row_text:
            product_data['brand'] = '„Ç®„É≥„Çπ„Ç´„Ç§'
            product_data['manufacturer'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
            product_data['ip_name'] = '„Ç®„É≥„Çπ„Ç´„Ç§'
            product_data['supplier_name'] = 'Ê†™Âºè‰ºöÁ§æ„Ç®„É≥„Çπ„Ç´„Ç§'
        
        # 3. ‰ΩúÂìÅÂêç„Åã„Çâ„Ç≠„É£„É©„ÇØ„Çø„ÉºÊÉÖÂ†±„ÇíÊäΩÂá∫
        character_match = re.search(r'[„Äé„Äå]([^„Äè„Äç]+)[„Äè„Äç]', row_text)
        if character_match:
            work_name = character_match.group(1)
            product_data['character_name'] = work_name
        
        # 4. „Åù„ÅÆ‰ªñ„ÅÆÈ†ÖÁõÆÔºàExcel„Å´Â≠òÂú®„Åô„ÇãÂ†¥ÂêàÔºâ
        # Áô∫Â£≤Êó•
        date_patterns = [
            r'(\d{4})[Âπ¥/-](\d{1,2})[Êúà/-](\d{1,2})',
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
        ]
        for pattern in date_patterns:
            date_match = re.search(pattern, row_text)
            if date_match:
                year, month, day = date_match.groups()
                product_data['release_date'] = f"{year}/{month.zfill(2)}/{day.zfill(2)}"
                break
        
        # ÂïÜÂìÅË™¨ÊòéÔºàÂïÜÂìÅÂêç„Åã„ÇâÁîüÊàêÔºâ
        if product_data.get('product_name'):
            product_data['description'] = f"„Äé{product_data.get('character_name', '')}„Äè„ÅÆ{product_data.get('product_name', '')}„Åß„Åô„ÄÇ" if product_data.get('character_name') else product_data.get('product_name', '')
        
        # 5. „Éá„Éï„Ç©„É´„ÉàÂÄ§„ÅÆË®≠ÂÆöÔºàÊó•Êú¨Ë£Ω„ÅåÂ§ö„ÅÑÔºâ
        product_data['country_of_origin'] = product_data.get('country_of_origin', 'Êó•Êú¨')
        
        # 6. Âú®Â∫´„ÉªÊ≥®ÊñáÈñ¢ÈÄ£ÔºàExcel„Å´„Å™„ÅÑÂ†¥Âêà„ÅØÁ©∫Ôºâ
        product_data['stock'] = product_data.get('stock')
        product_data['wholesale_quantity'] = product_data.get('wholesale_quantity')
        product_data['order_amount'] = product_data.get('order_amount')
        
        # 7. „Çµ„Ç§„Ç∫ÊÉÖÂ†±ÔºàExcel„Åã„ÇâÊäΩÂá∫„Åß„Åç„ÇãÂ†¥ÂêàÔºâ
        size_match = re.search(r'(\d+)\s*[√óx]\s*(\d+)\s*[√óx]?\s*(\d+)?\s*mm', row_text)
        if size_match:
            w, h, d = size_match.groups()
            if d:
                product_data['single_product_size'] = f"{w}√ó{h}√ó{d}mm"
            else:
                product_data['single_product_size'] = f"{w}√ó{h}mm"
        
        # 8. „Åù„ÅÆ‰ªñ„ÅÆ38È†ÖÁõÆ„Éï„Ç£„Éº„É´„ÉâÔºàÂøÖË¶Å„Å´Âøú„Åò„Å¶ÊäΩÂá∫Ôºâ
        # lot_number, classification, in_store, reservation_*, inner_box_size, carton_size,
        # inner_box_gtin, outer_box_gtin, protective_film_material, target_age, image1-6
        # „Åì„Çå„Çâ„ÅØExcel„Å´Â≠òÂú®„Åô„ÇãÂ†¥Âêà„ÅÆ„ÅøÊäΩÂá∫
        
        print(f"‚úÖ Extracted {len([k for k, v in product_data.items() if v])} fields from Excel row")
        return product_data